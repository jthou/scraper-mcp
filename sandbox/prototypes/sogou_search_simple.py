#!/usr/bin/env python3
"""
æœç‹—å¾®ä¿¡æœç´¢ç®€åŒ–ç‰ˆ

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æœç‹—å¾®ä¿¡æœç´¢å®ç°ï¼Œç”¨äºå¿«é€ŸéªŒè¯å’Œæµ‹è¯•
"""

import asyncio
import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from playwright.async_api import async_playwright


class SimpleSogouSearch:
    """ç®€åŒ–çš„æœç‹—å¾®ä¿¡æœç´¢ç±»"""
    
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.page = None
    
    async def setup(self, headless: bool = True):
        """è®¾ç½®æµè§ˆå™¨ç¯å¢ƒ"""
        try:
            self.playwright = await async_playwright().start()
            
            self.browser = await self.playwright.chromium.launch(
                channel="chrome",
                headless=headless,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--no-first-run",
                    "--disable-sync"
                ]
            )
            
            self.page = await self.browser.new_page()
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            
            return True
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def search(self, query: str, max_pages: int = 3) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢ï¼Œæ”¯æŒå¤šé¡µç¿»é¡µ"""
        try:
            print(f"ğŸ” æœç´¢: {query} (æœ€å¤š{max_pages}é¡µ)")
            
            # æ„å»ºæœç´¢URL
            search_url = f"https://weixin.sogou.com/weixin?type=2&query={query}"
            
            # è®¿é—®æœç´¢é¡µé¢
            await self.page.goto(search_url)
            await self.page.wait_for_load_state("networkidle")
            
            # ç­‰å¾…ç»“æœåŠ è½½
            await self.page.wait_for_timeout(3000)
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await self.page.query_selector(".captcha, .verify-code, .slider")
            if captcha:
                return {
                    "status": "error",
                    "message": "éœ€è¦éªŒè¯ç ï¼Œæ— æ³•è‡ªåŠ¨æœç´¢",
                    "url": search_url
                }
            
            # æ”¶é›†æ‰€æœ‰é¡µé¢çš„ç»“æœ
            all_results = []
            all_links = []
            
            for page_num in range(1, max_pages + 1):
                print(f"  ğŸ“„ æ­£åœ¨æœç´¢ç¬¬ {page_num} é¡µ...")
                
                # æå–å½“å‰é¡µç»“æœ
                page_results = await self._extract_page_results()
                all_results.extend(page_results)
                
                # è®°å½•å½“å‰é¡µçš„é“¾æ¥
                page_links = [result['link'] for result in page_results if result.get('link')]
                all_links.extend(page_links)
                
                print(f"  âœ… ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(page_results)} ä¸ªç»“æœ")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œå°è¯•ç¿»é¡µ
                if page_num < max_pages:
                    next_page_success = await self._go_to_next_page(page_num + 1)
                    if not next_page_success:
                        print(f"  âš ï¸ æ— æ³•ç¿»é¡µåˆ°ç¬¬ {page_num + 1} é¡µï¼Œåœæ­¢æœç´¢")
                        break
                    
                    # é¡µé¢é—´ç­‰å¾…
                    await self.page.wait_for_timeout(2000)
            
            # å»é‡é“¾æ¥
            unique_links = list(set(all_links))
            
            print(f"ğŸ“Š æœç´¢å®Œæˆ: å…±æ‰¾åˆ° {len(all_results)} ä¸ªç»“æœï¼Œ{len(unique_links)} ä¸ªå”¯ä¸€é“¾æ¥")
            
            return {
                "status": "success",
                "message": f"æ‰¾åˆ° {len(all_results)} ä¸ªç»“æœï¼Œ{len(unique_links)} ä¸ªå”¯ä¸€é“¾æ¥",
                "query": query,
                "url": search_url,
                "total_results": len(all_results),
                "unique_links": len(unique_links),
                "pages_searched": min(page_num, max_pages),
                "all_links": unique_links,  # è®°å½•æ‰€æœ‰å”¯ä¸€é“¾æ¥
                "results": all_results
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"æœç´¢å¤±è´¥: {str(e)}",
                "query": query
            }
    
    async def _extract_page_results(self) -> List[Dict[str, Any]]:
        """æå–å½“å‰é¡µçš„æœç´¢ç»“æœ"""
        try:
            results = []
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await self.page.wait_for_load_state("networkidle")
            await self.page.wait_for_timeout(2000)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢å†…å®¹
            page_title = await self.page.title()
            print(f"  ğŸ” é¡µé¢æ ‡é¢˜: {page_title}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç 
            captcha_elements = await self.page.query_selector_all(".captcha, .verify-code, .slider, .geetest")
            if captcha_elements:
                print(f"  âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç å…ƒç´ : {len(captcha_elements)}ä¸ª")
                return []
            
            # ä½¿ç”¨è°ƒè¯•å‘ç°çš„æœ‰æ•ˆé€‰æ‹©å™¨
            result_items = await self.page.query_selector_all(".txt-box")
            print(f"  âœ… ä½¿ç”¨é€‰æ‹©å™¨ '.txt-box' æ‰¾åˆ° {len(result_items)} ä¸ªç»“æœ")
            
            if not result_items:
                print(f"  âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœå…ƒç´ ")
                # ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
                screenshot_path = Path(__file__).parent / f"debug_screenshot_{int(datetime.now().timestamp())}.png"
                await self.page.screenshot(path=str(screenshot_path))
                print(f"  ğŸ“¸ å·²ä¿å­˜è°ƒè¯•æˆªå›¾: {screenshot_path}")
                return []
            
            for item in result_items:
                try:
                    result = await self._extract_single_result(item)
                    if result:
                        results.append(result)
                except Exception as e:
                    continue
            
            return results
            
        except Exception as e:
            print(f"  âŒ æå–é¡µé¢ç»“æœå¤±è´¥: {e}")
            return []
    
    async def _extract_single_result(self, item) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªæœç´¢ç»“æœ"""
        try:
            # åœ¨ .txt-box å†…éƒ¨æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
            title_element = await item.query_selector("h3 a")
            if not title_element:
                return None
            
            title = await title_element.inner_text()
            link = await title_element.get_attribute("href")
            
            # å¤„ç†æœç‹—çš„é‡å®šå‘é“¾æ¥
            if link and link.startswith("/link?"):
                # è¿™æ˜¯æœç‹—çš„é‡å®šå‘é“¾æ¥ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†
                # æš‚æ—¶ä¿æŒåŸæ ·ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦è§£æé‡å®šå‘
                link = f"https://weixin.sogou.com{link}"
            
            # æå–æ‘˜è¦ - åœ¨ .txt-box å†…éƒ¨æŸ¥æ‰¾
            summary_element = await item.query_selector(".txt-info")
            summary = ""
            if summary_element:
                summary = await summary_element.inner_text()
            
            # æå–ä½œè€…ä¿¡æ¯
            author_element = await item.query_selector(".s-p .account")
            author = ""
            if author_element:
                author = await author_element.inner_text()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_element = await item.query_selector(".s-p .s2")
            publish_time = ""
            if time_element:
                publish_time = await time_element.inner_text()
            
            # æå–å…¬ä¼—å·åç§°
            account_element = await item.query_selector(".s-p .account")
            account_name = ""
            if account_element:
                account_name = await account_element.inner_text()
            
            # æå–é˜…è¯»æ•°
            read_element = await item.query_selector(".s-p .s3")
            read_count = ""
            if read_element:
                read_count = await read_element.inner_text()
            
            return {
                "title": title.strip(),
                "link": link,
                "summary": summary.strip()[:200] + "..." if len(summary.strip()) > 200 else summary.strip(),
                "author": author.strip(),
                "account_name": account_name.strip(),
                "publish_time": publish_time.strip(),
                "read_count": read_count.strip()
            }
            
        except Exception as e:
            return None
    
    async def _go_to_next_page(self, page_num: int) -> bool:
        """ç¿»é¡µåˆ°æŒ‡å®šé¡µé¢"""
        try:
            # å°è¯•å¤šç§ç¿»é¡µæ–¹å¼
            next_page_selectors = [
                f".pagination a[href*='page={page_num}']",
                f".pagination a:has-text('{page_num}')",
                ".pagination .next:not(.disabled)",
                ".pagination .next-page",
                ".pagination .page-next"
            ]
            
            for selector in next_page_selectors:
                try:
                    next_button = await self.page.query_selector(selector)
                    if next_button:
                        # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                        await next_button.scroll_into_view_if_needed()
                        await self.page.wait_for_timeout(1000)
                        
                        # ç‚¹å‡»æŒ‰é’®
                        await next_button.click()
                        
                        # ç­‰å¾…é¡µé¢åŠ è½½
                        await self.page.wait_for_load_state("networkidle")
                        await self.page.wait_for_timeout(2000)
                        
                        return True
                except Exception:
                    continue
            
            # å¦‚æœæ‰¾ä¸åˆ°ç¿»é¡µæŒ‰é’®ï¼Œå°è¯•ç›´æ¥è®¿é—®URL
            try:
                current_url = self.page.url
                if "page=" in current_url:
                    new_url = re.sub(r'page=\d+', f'page={page_num}', current_url)
                else:
                    new_url = f"{current_url}&page={page_num}"
                
                await self.page.goto(new_url)
                await self.page.wait_for_load_state("networkidle")
                await self.page.wait_for_timeout(2000)
                
                return True
                
            except Exception:
                return False
            
        except Exception as e:
            return False
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æœç‹—å¾®ä¿¡æœç´¢ç®€åŒ–ç‰ˆæµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºæœç´¢å®ä¾‹
    searcher = SimpleSogouSearch()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        if not await searcher.setup(headless=False):
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥")
            return
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "Pythonç¼–ç¨‹",
            "æœºå™¨å­¦ä¹ ",
            "äººå·¥æ™ºèƒ½"
        ]
        
        all_results = {
            "timestamp": datetime.now().isoformat(),
            "queries": {},
            "all_links_summary": {}
        }
        
        for query in test_queries:
            print(f"\nğŸ” æœç´¢: {query}")
            print("-" * 30)
            
            # æ‰§è¡Œæœç´¢ï¼ˆæ”¯æŒå¤šé¡µï¼‰
            result = await searcher.search(query, max_pages=2)
            all_results["queries"][query] = result
            
            # æ˜¾ç¤ºç»“æœ
            if result["status"] == "success":
                print(f"âœ… {result['message']}")
                print(f"ğŸ”— æœç´¢URL: {result['url']}")
                print(f"ğŸ“Š æ€»ç»“æœæ•°: {result['total_results']}")
                print(f"ğŸ”— å”¯ä¸€é“¾æ¥æ•°: {result['unique_links']}")
                print(f"ğŸ“„ æœç´¢é¡µæ•°: {result['pages_searched']}")
                
                # æ˜¾ç¤ºæ‰€æœ‰é“¾æ¥
                if result.get('all_links'):
                    print(f"\nğŸ“‹ æ‰€æœ‰æœç´¢åˆ°çš„é“¾æ¥ ({len(result['all_links'])}ä¸ª):")
                    for i, link in enumerate(result['all_links'], 1):
                        print(f"  {i}. {link}")
                
                # æ˜¾ç¤ºå‰3ä¸ªè¯¦ç»†ç»“æœ
                print(f"\nğŸ“„ å‰3ä¸ªè¯¦ç»†ç»“æœ:")
                for i, item in enumerate(result.get('results', [])[:3], 1):
                    print(f"\n{i}. {item['title']}")
                    print(f"   ä½œè€…: {item['author']}")
                    print(f"   æ—¶é—´: {item['publish_time']}")
                    print(f"   æ‘˜è¦: {item['summary']}")
                    print(f"   é“¾æ¥: {item['link']}")
                
                # è®°å½•é“¾æ¥æ‘˜è¦
                all_results["all_links_summary"][query] = {
                    "total_links": len(result['all_links']),
                    "links": result['all_links']
                }
            else:
                print(f"âŒ {result['message']}")
            
            # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
            input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€ä¸ªæœç´¢...")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        output_file = Path(__file__).parent / f"sogou_search_simple_results_{int(datetime.now().timestamp())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        await searcher.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
