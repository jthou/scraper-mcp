#!/usr/bin/env python3
"""
æœç‹—å¾®ä¿¡æœç´¢ä¸“ç”¨æ¨¡å—

æœç‹—å¾®ä¿¡æœç´¢çš„ä¼˜åŠ¿ï¼š
1. æ— éœ€ç™»å½•å¾®ä¿¡è´¦å·
2. æœç´¢ç»“æœç›¸å¯¹å®Œæ•´
3. æ”¯æŒå…³é”®è¯æœç´¢
4. ç›¸å¯¹ç¨³å®š

æŠ€æœ¯æŒ‘æˆ˜ï¼š
1. ç»å¸¸å‡ºç°éªŒè¯ç 
2. åçˆ¬è™«æœºåˆ¶ä¸¥æ ¼
3. æœç´¢ç»“æœå¯èƒ½ä¸å®Œæ•´
4. éœ€è¦å¤„ç†åŠ¨æ€åŠ è½½
"""

import asyncio
import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from playwright.async_api import async_playwright
from src.utils.logger import Logger


class SogouWeChatSearch:
    """æœç‹—å¾®ä¿¡æœç´¢ä¸“ç”¨ç±»"""
    
    def __init__(self):
        self.logger = Logger("SogouWeChatSearch")
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.base_url = "https://weixin.sogou.com"
    
    async def setup_browser(self, headless: bool = False) -> bool:
        """è®¾ç½®æµè§ˆå™¨ç¯å¢ƒï¼Œä¸“é—¨é’ˆå¯¹æœç‹—æœç´¢ä¼˜åŒ–"""
        try:
            self.playwright = await async_playwright().start()
            
            # é’ˆå¯¹æœç‹—æœç´¢ä¼˜åŒ–çš„æµè§ˆå™¨å‚æ•°
            self.browser = await self.playwright.chromium.launch(
                channel="chrome",
                headless=headless,
                args=[
                    "--start-maximized",
                    "--disable-blink-features=AutomationControlled",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-sync",
                    "--disable-extensions",
                    "--disable-default-apps",
                    "--disable-background-timer-throttling",
                    "--disable-background-networking",
                    "--disable-breakpad",
                    "--disable-component-extensions-with-background-pages",
                    "--disable-domain-reliability",
                    "--disable-features=TranslateUI",
                    "--disable-hang-monitor",
                    "--disable-prompt-on-repost",
                    "--disable-sync-preferences",
                    "--disable-web-resources",
                    "--enable-features=NetworkService,NetworkServiceLogging",
                    "--force-color-profile=srgb",
                    "--metrics-recording-only",
                    "--safebrowsing-disable-auto-update",
                    "--enable-automation",
                    "--password-store=basic",
                    "--use-mock-keychain",
                    "--disable-dev-shm-usage",
                    "--no-sandbox"
                ]
            )
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.context = await self.browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            
            self.page = await self.context.new_page()
            
            # è®¾ç½®é¢å¤–çš„HTTPå¤´ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
            await self.page.set_extra_http_headers({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            })
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´
            self.page.set_default_timeout(30000)
            
            self.logger.info("æœç‹—å¾®ä¿¡æœç´¢æµè§ˆå™¨ç¯å¢ƒè®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def search(self, query: str, max_pages: int = 3) -> Dict[str, Any]:
        """æœç´¢å¾®ä¿¡å…¬ä¼—å·å†…å®¹"""
        try:
            self.logger.info(f"å¼€å§‹æœç´¢: {query}")
            
            # æ„å»ºæœç´¢URL
            search_url = f"{self.base_url}/weixin?type=2&query={query}"
            
            # è®¿é—®æœç´¢é¡µé¢
            await self.page.goto(search_url)
            await self.page.wait_for_load_state("networkidle")
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸º - éšæœºç­‰å¾…
            await self.page.wait_for_timeout(random.randint(2000, 4000))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç 
            captcha_result = await self._check_captcha()
            if captcha_result["has_captcha"]:
                return {
                    "status": "error",
                    "message": "éœ€è¦éªŒè¯ç ï¼Œæ— æ³•è‡ªåŠ¨æœç´¢",
                    "captcha_type": captcha_result["type"],
                    "url": search_url,
                    "suggestion": "è¯·æ‰‹åŠ¨è®¿é—®é¡µé¢å®ŒæˆéªŒè¯ç åé‡è¯•"
                }
            
            # æå–æœç´¢ç»“æœ
            all_results = []
            
            for page_num in range(1, max_pages + 1):
                self.logger.info(f"æå–ç¬¬ {page_num} é¡µç»“æœ...")
                
                # æå–å½“å‰é¡µç»“æœ
                page_results = await self._extract_page_results()
                all_results.extend(page_results)
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œå°è¯•ç¿»é¡µ
                if page_num < max_pages:
                    next_page_success = await self._go_to_next_page(page_num + 1)
                    if not next_page_success:
                        self.logger.warning(f"æ— æ³•ç¿»é¡µåˆ°ç¬¬ {page_num + 1} é¡µï¼Œåœæ­¢æœç´¢")
                        break
                
                # é¡µé¢é—´ç­‰å¾…
                await self.page.wait_for_timeout(random.randint(3000, 5000))
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(all_results)
            
            return {
                "status": "success",
                "message": f"æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_results)} ä¸ªç»“æœ",
                "query": query,
                "total_results": len(unique_results),
                "pages_searched": min(page_num, max_pages),
                "url": search_url,
                "results": unique_results
            }
            
        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"æœç´¢å¤±è´¥: {str(e)}",
                "query": query,
                "error": str(e)
            }
    
    async def _check_captcha(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç """
        try:
            # æ£€æŸ¥å„ç§éªŒè¯ç ç±»å‹
            captcha_selectors = [
                (".captcha", "å›¾ç‰‡éªŒè¯ç "),
                (".verify-code", "éªŒè¯ç "),
                (".slider", "æ»‘å—éªŒè¯ç "),
                (".geetest", "æéªŒéªŒè¯ç "),
                (".nc-container", "é˜¿é‡Œäº‘éªŒè¯ç "),
                ("#captcha", "éªŒè¯ç "),
                (".captcha-container", "éªŒè¯ç å®¹å™¨")
            ]
            
            for selector, captcha_type in captcha_selectors:
                captcha_element = await self.page.query_selector(selector)
                if captcha_element:
                    return {
                        "has_captcha": True,
                        "type": captcha_type,
                        "selector": selector
                    }
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å«éªŒè¯ç ç›¸å…³æ–‡å­—
            title = await self.page.title()
            if any(keyword in title for keyword in ["éªŒè¯ç ", "captcha", "éªŒè¯", "å®‰å…¨éªŒè¯"]):
                return {
                    "has_captcha": True,
                    "type": "é¡µé¢æ ‡é¢˜æ£€æµ‹",
                    "title": title
                }
            
            return {"has_captcha": False}
            
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥éªŒè¯ç å¤±è´¥: {e}")
            return {"has_captcha": False}
    
    async def _extract_page_results(self) -> List[Dict[str, Any]]:
        """æå–å½“å‰é¡µçš„æœç´¢ç»“æœ"""
        try:
            results = []
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            await self.page.wait_for_selector(".results .result", timeout=10000)
            
            # è·å–æ‰€æœ‰ç»“æœé¡¹
            result_items = await self.page.query_selector_all(".results .result")
            
            for item in result_items:
                try:
                    result = await self._extract_single_result(item)
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.warning(f"æå–å•ä¸ªç»“æœå¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"æˆåŠŸæå– {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"æå–é¡µé¢ç»“æœå¤±è´¥: {e}")
            return []
    
    async def _extract_single_result(self, item) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªæœç´¢ç»“æœ"""
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_element = await item.query_selector(".txt-box h3 a")
            if not title_element:
                return None
            
            title = await title_element.inner_text()
            link = await title_element.get_attribute("href")
            
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if link and not link.startswith("http"):
                if link.startswith("//"):
                    link = f"https:{link}"
                elif link.startswith("/"):
                    link = f"{self.base_url}{link}"
                else:
                    link = f"{self.base_url}/{link}"
            
            # æå–æ‘˜è¦
            summary_element = await item.query_selector(".txt-box .txt-info")
            summary = ""
            if summary_element:
                summary = await summary_element.inner_text()
                # æ¸…ç†æ‘˜è¦æ–‡æœ¬
                summary = re.sub(r'\s+', ' ', summary).strip()
            
            # æå–ä½œè€…
            author_element = await item.query_selector(".txt-box .s-p .account")
            author = ""
            if author_element:
                author = await author_element.inner_text()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_element = await item.query_selector(".txt-box .s-p .s2")
            publish_time = ""
            if time_element:
                publish_time = await time_element.inner_text()
            
            # æå–é˜…è¯»æ•°
            read_count_element = await item.query_selector(".txt-box .s-p .s3")
            read_count = ""
            if read_count_element:
                read_count = await read_count_element.inner_text()
            
            # æå–å…¬ä¼—å·åç§°
            account_element = await item.query_selector(".txt-box .s-p .account")
            account_name = ""
            if account_element:
                account_name = await account_element.inner_text()
            
            return {
                "title": title.strip(),
                "link": link,
                "summary": summary,
                "author": author.strip(),
                "account_name": account_name.strip(),
                "publish_time": publish_time.strip(),
                "read_count": read_count.strip(),
                "source": "sogou_wechat",
                "extracted_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.warning(f"æå–å•ä¸ªç»“æœå¤±è´¥: {e}")
            return None
    
    async def _go_to_next_page(self, page_num: int) -> bool:
        """ç¿»é¡µåˆ°æŒ‡å®šé¡µé¢"""
        try:
            # å°è¯•å¤šç§ç¿»é¡µæ–¹å¼
            next_page_selectors = [
                f".pagination a[href*='page={page_num}']",
                f".pagination a:has-text('{page_num}')",
                ".pagination .next:not(.disabled)",
                ".pagination .next-page",
                ".pagination .page-next"
            ]
            
            for selector in next_page_selectors:
                try:
                    next_button = await self.page.query_selector(selector)
                    if next_button:
                        # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                        await next_button.scroll_into_view_if_needed()
                        await self.page.wait_for_timeout(1000)
                        
                        # ç‚¹å‡»æŒ‰é’®
                        await next_button.click()
                        
                        # ç­‰å¾…é¡µé¢åŠ è½½
                        await self.page.wait_for_load_state("networkidle")
                        await self.page.wait_for_timeout(2000)
                        
                        self.logger.info(f"æˆåŠŸç¿»é¡µåˆ°ç¬¬ {page_num} é¡µ")
                        return True
                except Exception as e:
                    continue
            
            # å¦‚æœæ‰¾ä¸åˆ°ç¿»é¡µæŒ‰é’®ï¼Œå°è¯•ç›´æ¥è®¿é—®URL
            try:
                current_url = self.page.url
                if "page=" in current_url:
                    new_url = re.sub(r'page=\d+', f'page={page_num}', current_url)
                else:
                    new_url = f"{current_url}&page={page_num}"
                
                await self.page.goto(new_url)
                await self.page.wait_for_load_state("networkidle")
                await self.page.wait_for_timeout(2000)
                
                self.logger.info(f"é€šè¿‡URLç›´æ¥è®¿é—®ç¬¬ {page_num} é¡µ")
                return True
                
            except Exception as e:
                self.logger.warning(f"ç›´æ¥è®¿é—®ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
                return False
            
        except Exception as e:
            self.logger.error(f"ç¿»é¡µå¤±è´¥: {e}")
            return False
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æœç´¢ç»“æœ"""
        try:
            seen_links = set()
            unique_results = []
            
            for result in results:
                link = result.get("link", "")
                if link and link not in seen_links:
                    seen_links.add(link)
                    unique_results.append(result)
            
            self.logger.info(f"å»é‡å‰: {len(results)} ä¸ªç»“æœï¼Œå»é‡å: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results
            
        except Exception as e:
            self.logger.error(f"å»é‡å¤±è´¥: {e}")
            return results
    
    async def search_with_retry(self, query: str, max_pages: int = 3, max_retries: int = 3) -> Dict[str, Any]:
        """å¸¦é‡è¯•çš„æœç´¢åŠŸèƒ½"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"æœç´¢å°è¯• {attempt + 1}/{max_retries}: {query}")
                
                result = await self.search(query, max_pages)
                
                if result["status"] == "success":
                    return result
                elif result["status"] == "error" and "éªŒè¯ç " in result.get("message", ""):
                    self.logger.warning(f"é‡åˆ°éªŒè¯ç ï¼Œç­‰å¾… {5 * (attempt + 1)} ç§’åé‡è¯•...")
                    await asyncio.sleep(5 * (attempt + 1))
                    continue
                else:
                    return result
                    
            except Exception as e:
                self.logger.error(f"æœç´¢å°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(3 * (attempt + 1))
                    continue
                else:
                    return {
                        "status": "error",
                        "message": f"æœç´¢å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡",
                        "error": str(e)
                    }
        
        return {
            "status": "error",
            "message": "æœç´¢å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æœç‹—å¾®ä¿¡æœç´¢"""
    print("ğŸ” æœç‹—å¾®ä¿¡æœç´¢æµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºæœç´¢å®ä¾‹
    searcher = SogouWeChatSearch()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        if not await searcher.setup_browser(headless=False):
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥")
            return
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "Pythonç¼–ç¨‹",
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "äººå·¥æ™ºèƒ½åº”ç”¨"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æœç´¢: {query}")
            print("-" * 30)
            
            # æ‰§è¡Œæœç´¢
            result = await searcher.search_with_retry(query, max_pages=2)
            
            if result["status"] == "success":
                print(f"âœ… {result['message']}")
                print(f"ğŸ“Š æ€»ç»“æœæ•°: {result['total_results']}")
                print(f"ğŸ“„ æœç´¢é¡µæ•°: {result['pages_searched']}")
                
                # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                for i, item in enumerate(result.get('results', [])[:3], 1):
                    print(f"\n{i}. {item['title']}")
                    print(f"   ä½œè€…: {item['author']}")
                    print(f"   å…¬ä¼—å·: {item['account_name']}")
                    print(f"   æ—¶é—´: {item['publish_time']}")
                    print(f"   é˜…è¯»: {item['read_count']}")
                    print(f"   æ‘˜è¦: {item['summary'][:100]}...")
                    print(f"   é“¾æ¥: {item['link']}")
            else:
                print(f"âŒ {result['message']}")
                if result.get('suggestion'):
                    print(f"ğŸ’¡ å»ºè®®: {result['suggestion']}")
            
            # ä¿å­˜ç»“æœ
            if result["status"] == "success":
                output_file = Path(__file__).parent / f"sogou_search_{query}_{int(datetime.now().timestamp())}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
            input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€ä¸ªæœç´¢...")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æœç´¢è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        await searcher.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
