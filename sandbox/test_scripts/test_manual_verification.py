#!/usr/bin/env python3
"""
äººå·¥éªŒè¯ç­‰å¾…æµ‹è¯•

æµ‹è¯•æœç‹—å¾®ä¿¡æœç´¢çš„äººå·¥éªŒè¯ç­‰å¾…åŠŸèƒ½
"""

import asyncio
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_manual_verification():
    """æµ‹è¯•äººå·¥éªŒè¯ç­‰å¾…åŠŸèƒ½"""
    print("ğŸ” äººå·¥éªŒè¯ç­‰å¾…æµ‹è¯•")
    print("=" * 40)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "manual_verification_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢é“¾æ¥
        test_url = "https://weixin.sogou.com/link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS-RU6ljlTvJpOQfAfK4l7_65z41o-wcdAVqXa8Fplpd9952kvnRUuUGmJJKhudQ35mGjx4ciicfy_t6uMaCgkDBcJat_Dw3Ktt9Z3CebSf58m4l0myOsAm_R-JrxCE85wcX0SQNdi8xJjoL3tZWFVb"
        
        print(f"\n3. æµ‹è¯•é“¾æ¥:")
        print(f"   {test_url}")
        
        # è®¿é—®é“¾æ¥
        print(f"\n4. è®¿é—®é“¾æ¥...")
        await scraper.page.goto(test_url)
        await scraper.page.wait_for_load_state("networkidle")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(3)
        
        # è·å–åˆå§‹é¡µé¢ä¿¡æ¯
        title = await scraper.page.title()
        current_url = scraper.page.url
        content = await scraper.page.content()
        
        print(f"   åˆå§‹é¡µé¢æ ‡é¢˜: {title}")
        print(f"   åˆå§‹URL: {current_url}")
        print(f"   å†…å®¹é•¿åº¦: {len(content)}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
        if "antispider" in current_url or "éªŒè¯ç " in title or "captcha" in content.lower():
            print(f"\n5. æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå¼€å§‹ç­‰å¾…äººå·¥éªŒè¯...")
            print(f"   âš ï¸ è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯ç éªŒè¯")
            print(f"   â³ ç¨‹åºå°†ä¸€ç›´ç­‰å¾…ç›´åˆ°éªŒè¯å®Œæˆ...")
            
            # ç­‰å¾…äººå·¥éªŒè¯å®Œæˆ
            verification_result = await scraper.wait_for_manual_verification(timeout=None)
            
            if verification_result["success"]:
                print(f"   âœ… äººå·¥éªŒè¯å®Œæˆ!")
                print(f"   ç­‰å¾…æ—¶é—´: {verification_result['wait_time']}ç§’")
                print(f"   æœ€ç»ˆURL: {verification_result['current_url']}")
                print(f"   æœ€ç»ˆæ ‡é¢˜: {verification_result['title']}")
                
                # éªŒè¯å®Œæˆåï¼Œå°è¯•ä¸‹è½½å†…å®¹
                print(f"\n6. éªŒè¯å®Œæˆï¼Œå°è¯•ä¸‹è½½å†…å®¹...")
                
                # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ°å¾®ä¿¡æ–‡ç« 
                if "mp.weixin.qq.com" in verification_result['current_url']:
                    print(f"   âœ… æˆåŠŸé‡å®šå‘åˆ°å¾®ä¿¡æ–‡ç« ")
                    
                    # å°è¯•ä¸‹è½½å†…å®¹
                    download_result = await scraper.download_and_save_content(
                        verification_result['current_url'],
                        output_dir,
                        "å¾®ä¿¡æ–‡ç« "
                    )
                    
                    if download_result["status"] == "success":
                        print(f"   âœ… å†…å®¹ä¸‹è½½æˆåŠŸ!")
                        print(f"   ğŸ“ PDF: {download_result['files']['pdf']}")
                        print(f"   ğŸ“„ Markdown: {download_result['files']['markdown']}")
                    else:
                        print(f"   âŒ å†…å®¹ä¸‹è½½å¤±è´¥: {download_result['message']}")
                else:
                    print(f"   âš ï¸ æœªé‡å®šå‘åˆ°å¾®ä¿¡æ–‡ç« ï¼Œå½“å‰URL: {verification_result['current_url']}")
                    
                    # å°è¯•ç”ŸæˆPDF
                    pdf_result = await scraper.print_page_to_pdf(verification_result['current_url'])
                    if pdf_result["status"] == "success":
                        print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
                        print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_result['pdf_path']}")
                        
                        # æ£€æŸ¥PDFæ–‡ä»¶
                        pdf_path = Path(pdf_result['pdf_path'])
                        if pdf_path.exists():
                            file_size = pdf_path.stat().st_size
                            print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                            
                            if file_size > 50000:
                                print(f"   âœ… PDFæ–‡ä»¶å¤§å°æ­£å¸¸ï¼Œå¯èƒ½åŒ…å«çœŸå®å†…å®¹")
                            else:
                                print(f"   âš ï¸ PDFæ–‡ä»¶è¾ƒå°ï¼Œå¯èƒ½æ˜¯éªŒè¯é¡µé¢")
                    else:
                        print(f"   âŒ PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}")
            else:
                print(f"   âŒ ç­‰å¾…äººå·¥éªŒè¯è¶…æ—¶")
                print(f"   ç­‰å¾…æ—¶é—´: {verification_result['wait_time']}ç§’")
                print(f"   æœ€ç»ˆURL: {verification_result['current_url']}")
                print(f"   æœ€ç»ˆæ ‡é¢˜: {verification_result['title']}")
        else:
            print(f"\n5. æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç›´æ¥å°è¯•ä¸‹è½½...")
            
            # ç›´æ¥å°è¯•ä¸‹è½½
            download_result = await scraper.download_and_save_content(
                test_url,
                output_dir,
                "å¾®ä¿¡æ–‡ç« "
            )
            
            if download_result["status"] == "success":
                print(f"   âœ… å†…å®¹ä¸‹è½½æˆåŠŸ!")
                print(f"   ğŸ“ PDF: {download_result['files']['pdf']}")
                print(f"   ğŸ“„ Markdown: {download_result['files']['markdown']}")
            else:
                print(f"   âŒ å†…å®¹ä¸‹è½½å¤±è´¥: {download_result['message']}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print(f"\n7. ç”Ÿæˆçš„æ–‡ä»¶:")
        if output_dir.exists():
            for item in output_dir.rglob("*"):
                if item.is_file():
                    print(f"   ğŸ“„ {item.relative_to(output_dir)}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n8. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ äººå·¥éªŒè¯ç­‰å¾…æµ‹è¯•å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_manual_verification()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
