#!/usr/bin/env python3
"""
å¾®ä¿¡é«˜çº§ç»•è¿‡æµ‹è¯•

ä½¿ç”¨å¤šç§ç­–ç•¥å°è¯•ç»•è¿‡å¾®ä¿¡çš„åçˆ¬è™«æœºåˆ¶
"""

import asyncio
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_wechat_advanced_bypass():
    """æµ‹è¯•å¾®ä¿¡é«˜çº§ç»•è¿‡ç­–ç•¥"""
    print("ğŸš€ å¾®ä¿¡é«˜çº§ç»•è¿‡æµ‹è¯•")
    print("=" * 50)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®é«˜çº§éšèº«æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "wechat_advanced_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # ç­–ç•¥1: ç›´æ¥æœç´¢å¹¶å°è¯•ä¸‹è½½
        print("\n3. ç­–ç•¥1: ç›´æ¥æœç´¢å¹¶å°è¯•ä¸‹è½½")
        search_result = await scraper.search_wechat("Pythonç¼–ç¨‹", max_pages=1)
        
        if search_result["status"] == "success":
            print(f"   âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {search_result['total_results']} ä¸ªç»“æœ")
            
            results = search_result.get('results', [])
            if results:
                # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ
                first_result = results[0]
                print(f"\n4. å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ...")
                print(f"   æ ‡é¢˜: {first_result['title']}")
                print(f"   é“¾æ¥: {first_result['link']}")
                
                # ä½¿ç”¨é«˜çº§ç­–ç•¥ä¸‹è½½
                download_result = await advanced_download(scraper, first_result['link'], output_dir, first_result['title'])
                
                if download_result["status"] == "success":
                    print(f"   âœ… ä¸‹è½½æˆåŠŸ!")
                    print(f"   ğŸ“ PDF: {download_result['files']['pdf']}")
                    print(f"   ğŸ“„ Markdown: {download_result['files']['markdown']}")
                else:
                    print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_result['message']}")
                    
                    # å°è¯•å…¶ä»–ç­–ç•¥
                    print(f"\n5. å°è¯•å…¶ä»–ç­–ç•¥...")
                    await try_alternative_strategies(scraper, first_result['link'], output_dir)
        else:
            print(f"   âŒ æœç´¢å¤±è´¥: {search_result['message']}")
            
            # å°è¯•ç›´æ¥è®¿é—®æœç‹—å¾®ä¿¡æœç´¢
            print(f"\n6. å°è¯•ç›´æ¥è®¿é—®æœç‹—å¾®ä¿¡æœç´¢...")
            await try_direct_sogou_access(scraper, output_dir)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n7. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ å¾®ä¿¡é«˜çº§ç»•è¿‡æµ‹è¯•å®Œæˆï¼")


async def advanced_download(scraper, url, output_dir, title):
    """é«˜çº§ä¸‹è½½ç­–ç•¥"""
    try:
        print(f"   ğŸ”„ ä½¿ç”¨é«˜çº§ç­–ç•¥ä¸‹è½½...")
        
        # ç­–ç•¥1: å¢åŠ æ›´é•¿çš„ç­‰å¾…æ—¶é—´
        print(f"   â³ ç­–ç•¥1: å¢åŠ ç­‰å¾…æ—¶é—´...")
        await asyncio.sleep(10)  # ç­‰å¾…10ç§’
        
        # ç­–ç•¥2: æ¨¡æ‹Ÿäººç±»è¡Œä¸º
        print(f"   ğŸ­ ç­–ç•¥2: æ¨¡æ‹Ÿäººç±»è¡Œä¸º...")
        await scraper.stealth.simulate_human_behavior(scraper.page, duration=10)
        
        # ç­–ç•¥3: å°è¯•è®¿é—®é¡µé¢
        print(f"   ğŸŒ ç­–ç•¥3: è®¿é—®é¡µé¢...")
        await scraper.page.goto(url)
        await scraper.page.wait_for_load_state("networkidle")
        
        # ç­–ç•¥4: ç­‰å¾…æ›´é•¿æ—¶é—´è®©å†…å®¹åŠ è½½
        print(f"   â° ç­–ç•¥4: ç­‰å¾…å†…å®¹åŠ è½½...")
        await asyncio.sleep(15)
        
        # ç­–ç•¥5: å†æ¬¡æ¨¡æ‹Ÿäººç±»è¡Œä¸º
        print(f"   ğŸ­ ç­–ç•¥5: å†æ¬¡æ¨¡æ‹Ÿäººç±»è¡Œä¸º...")
        await scraper.stealth.simulate_human_behavior(scraper.page, duration=8)
        
        # ç­–ç•¥6: æ£€æŸ¥é¡µé¢å†…å®¹
        print(f"   ğŸ” ç­–ç•¥6: æ£€æŸ¥é¡µé¢å†…å®¹...")
        content = await scraper.page.content()
        title_text = await scraper.page.title()
        
        print(f"   é¡µé¢æ ‡é¢˜: {title_text}")
        print(f"   å†…å®¹é•¿åº¦: {len(content)}")
        
        # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç 
        if "éªŒè¯ç " in content or "captcha" in content.lower():
            print(f"   âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå°è¯•ç»•è¿‡...")
            
            # å°è¯•åˆ·æ–°é¡µé¢
            await scraper.page.reload()
            await scraper.page.wait_for_load_state("networkidle")
            await asyncio.sleep(10)
            
            # å†æ¬¡æ£€æŸ¥
            content = await scraper.page.content()
            if "éªŒè¯ç " in content or "captcha" in content.lower():
                return {
                    "status": "error",
                    "message": "æ— æ³•ç»•è¿‡éªŒè¯ç ",
                    "content_preview": content[:200]
                }
        
        # ç­–ç•¥7: å°è¯•ç”ŸæˆPDF
        print(f"   ğŸ“„ ç­–ç•¥7: ç”ŸæˆPDF...")
        pdf_result = await scraper.print_page_to_pdf(url)
        
        if pdf_result["status"] == "success":
            print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
            
            # æ£€æŸ¥PDFæ–‡ä»¶
            pdf_path = Path(pdf_result['pdf_path'])
            if pdf_path.exists():
                file_size = pdf_path.stat().st_size
                print(f"   ğŸ“Š PDFæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                
                if file_size > 50000:  # å¤§äº50KBï¼Œå¯èƒ½æ˜¯çœŸå®å†…å®¹
                    print(f"   âœ… PDFæ–‡ä»¶å¤§å°æ­£å¸¸ï¼Œå¯èƒ½åŒ…å«çœŸå®å†…å®¹")
                    
                    # å°è¯•è½¬æ¢ä¸ºMarkdown
                    markdown_result = await scraper.pdf_to_markdown(str(pdf_path))
                    if markdown_result["status"] == "success":
                        print(f"   âœ… Markdownè½¬æ¢æˆåŠŸ")
                        
                        # ä¿å­˜æ–‡ä»¶
                        pdf_dir = output_dir / "pdfs"
                        markdown_dir = output_dir / "markdown"
                        pdf_dir.mkdir(exist_ok=True)
                        markdown_dir.mkdir(exist_ok=True)
                        
                        # å¤åˆ¶PDFæ–‡ä»¶
                        import shutil
                        target_pdf = pdf_dir / f"{title}.pdf"
                        shutil.copy2(pdf_path, target_pdf)
                        
                        # ä¿å­˜Markdownæ–‡ä»¶
                        markdown_content = f"""# {title}

**æ¥æº**: {url}
**ä¿å­˜æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**æ¥æºå¹³å°**: æœç‹—å¾®ä¿¡æœç´¢

---

{markdown_result['markdown_content']}
"""
                        
                        markdown_file = markdown_dir / f"{title}.md"
                        with open(markdown_file, 'w', encoding='utf-8') as f:
                            f.write(markdown_content)
                        
                        return {
                            "status": "success",
                            "message": "é«˜çº§ç­–ç•¥ä¸‹è½½æˆåŠŸ",
                            "files": {
                                "pdf": str(target_pdf),
                                "markdown": str(markdown_file)
                            }
                        }
                    else:
                        return {
                            "status": "error",
                            "message": f"Markdownè½¬æ¢å¤±è´¥: {markdown_result['message']}"
                        }
                else:
                    return {
                        "status": "error",
                        "message": f"PDFæ–‡ä»¶å¤ªå°({file_size}å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯éªŒè¯ç é¡µé¢"
                    }
            else:
                return {
                    "status": "error",
                    "message": "PDFæ–‡ä»¶ä¸å­˜åœ¨"
                }
        else:
            return {
                "status": "error",
                "message": f"PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}"
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"é«˜çº§ä¸‹è½½å¤±è´¥: {str(e)}"
        }


async def try_alternative_strategies(scraper, url, output_dir):
    """å°è¯•æ›¿ä»£ç­–ç•¥"""
    print(f"   ğŸ”„ å°è¯•æ›¿ä»£ç­–ç•¥...")
    
    # ç­–ç•¥1: ä½¿ç”¨ä¸åŒçš„ç”¨æˆ·ä»£ç†
    print(f"   ğŸ­ ç­–ç•¥1: æ›´æ¢ç”¨æˆ·ä»£ç†...")
    try:
        await scraper.page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.0(0x18000029) NetType/WIFI Language/zh_CN"
        })
        
        await scraper.page.goto(url)
        await scraper.page.wait_for_load_state("networkidle")
        await asyncio.sleep(10)
        
        content = await scraper.page.content()
        if "éªŒè¯ç " not in content and "captcha" not in content.lower():
            print(f"   âœ… ç”¨æˆ·ä»£ç†ç­–ç•¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ ç”¨æˆ·ä»£ç†ç­–ç•¥å¤±è´¥")
    except Exception as e:
        print(f"   âŒ ç”¨æˆ·ä»£ç†ç­–ç•¥å¤±è´¥: {e}")
    
    # ç­–ç•¥2: ä½¿ç”¨æ— å¤´æ¨¡å¼
    print(f"   ğŸ‘» ç­–ç•¥2: å°è¯•æ— å¤´æ¨¡å¼...")
    try:
        await scraper.cleanup()
        setup_result = await scraper.setup_browser(headless=True, persistent=False)
        if setup_result["status"] == "success":
            await scraper.page.goto(url)
            await scraper.page.wait_for_load_state("networkidle")
            await asyncio.sleep(10)
            
            content = await scraper.page.content()
            if "éªŒè¯ç " not in content and "captcha" not in content.lower():
                print(f"   âœ… æ— å¤´æ¨¡å¼ç­–ç•¥æˆåŠŸ")
                return True
            else:
                print(f"   âŒ æ— å¤´æ¨¡å¼ç­–ç•¥å¤±è´¥")
    except Exception as e:
        print(f"   âŒ æ— å¤´æ¨¡å¼ç­–ç•¥å¤±è´¥: {e}")
    
    return False


async def try_direct_sogou_access(scraper, output_dir):
    """å°è¯•ç›´æ¥è®¿é—®æœç‹—å¾®ä¿¡æœç´¢"""
    print(f"   ğŸ” å°è¯•ç›´æ¥è®¿é—®æœç‹—å¾®ä¿¡æœç´¢...")
    
    try:
        # ç›´æ¥è®¿é—®æœç‹—å¾®ä¿¡æœç´¢é¦–é¡µ
        await scraper.page.goto("https://weixin.sogou.com")
        await scraper.page.wait_for_load_state("networkidle")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(10)
        
        # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
        await scraper.stealth.simulate_human_behavior(scraper.page, duration=5)
        
        # æ£€æŸ¥é¡µé¢å†…å®¹
        content = await scraper.page.content()
        title = await scraper.page.title()
        
        print(f"   é¡µé¢æ ‡é¢˜: {title}")
        print(f"   å†…å®¹é•¿åº¦: {len(content)}")
        
        if "éªŒè¯ç " in content or "captcha" in content.lower():
            print(f"   âš ï¸ æœç‹—å¾®ä¿¡æœç´¢éœ€è¦éªŒè¯ç ")
        else:
            print(f"   âœ… æœç‹—å¾®ä¿¡æœç´¢è®¿é—®æˆåŠŸ")
            
            # å°è¯•æœç´¢
            search_box = await scraper.page.query_selector("input[name='query']")
            if search_box:
                await search_box.fill("Pythonç¼–ç¨‹")
                await search_box.press("Enter")
                await scraper.page.wait_for_load_state("networkidle")
                await asyncio.sleep(5)
                
                print(f"   âœ… æœç´¢åŠŸèƒ½å¯ç”¨")
            else:
                print(f"   âŒ æœªæ‰¾åˆ°æœç´¢æ¡†")
        
    except Exception as e:
        print(f"   âŒ ç›´æ¥è®¿é—®å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_wechat_advanced_bypass()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
