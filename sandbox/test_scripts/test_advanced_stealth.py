#!/usr/bin/env python3
"""
é«˜çº§åçˆ¬è™«ç­–ç•¥æµ‹è¯•

æµ‹è¯•æ”¹è¿›åçš„åçˆ¬è™«æŠ€æœ¯
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_advanced_stealth():
    """æµ‹è¯•é«˜çº§åçˆ¬è™«ç­–ç•¥"""
    print("ğŸ•µï¸ é«˜çº§åçˆ¬è™«ç­–ç•¥æµ‹è¯•")
    print("=" * 50)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®é«˜çº§éšèº«æµè§ˆå™¨
        print("\n1. è®¾ç½®é«˜çº§éšèº«æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "advanced_stealth_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        print("\n3. æµ‹è¯•é«˜çº§åçˆ¬è™«æœç´¢...")
        search_result = await scraper.search_wechat("Pythonç¼–ç¨‹", max_pages=1)
        
        print(f"\n4. æœç´¢ç»“æœ:")
        print(f"   çŠ¶æ€: {search_result['status']}")
        print(f"   æ¶ˆæ¯: {search_result['message']}")
        
        if search_result["status"] == "success":
            print(f"   âœ… æœç´¢æˆåŠŸ!")
            print(f"   æ€»ç»“æœæ•°: {search_result['total_results']}")
            print(f"   æœç´¢é¡µé¢æ•°: {search_result['pages_searched']}")
            print(f"   å”¯ä¸€é“¾æ¥æ•°: {search_result['unique_links']}")
            
            # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            results = search_result.get('results', [])
            if results:
                print(f"\n5. å‰3ä¸ªæœç´¢ç»“æœ:")
                for i, item in enumerate(results[:3], 1):
                    print(f"   {i}. {item['title']}")
                    print(f"      ä½œè€…: {item['author']}")
                    print(f"      å…¬ä¼—å·: {item['account_name']}")
                    print(f"      é“¾æ¥: {item['link'][:80]}...")
                    print()
                
                # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ
                if results:
                    first_result = results[0]
                    print(f"6. å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ...")
                    print(f"   æ ‡é¢˜: {first_result['title']}")
                    
                    download_result = await scraper.download_and_save_content(
                        first_result['link'], 
                        output_dir, 
                        first_result['title']
                    )
                    
                    print(f"\n7. ä¸‹è½½ç»“æœ:")
                    print(f"   çŠ¶æ€: {download_result['status']}")
                    print(f"   æ¶ˆæ¯: {download_result['message']}")
                    
                    if download_result["status"] == "success":
                        print(f"   âœ… ä¸‹è½½æˆåŠŸ!")
                        print(f"   ğŸ“ PDF: {download_result['files']['pdf']}")
                        print(f"   ğŸ“„ Markdown: {download_result['files']['markdown']}")
                        
                        # æ˜¾ç¤ºä¸‹è½½ç›®å½•ç»“æ„
                        print(f"\n8. ä¸‹è½½ç›®å½•ç»“æ„:")
                        print(f"   ğŸ“ {output_dir}")
                        if output_dir.exists():
                            for item in output_dir.rglob("*"):
                                if item.is_file():
                                    print(f"      ğŸ“„ {item.relative_to(output_dir)}")
                    else:
                        print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_result['message']}")
                        
                        # åˆ†æå¤±è´¥åŸå› 
                        if "éªŒè¯ç " in download_result.get('message', ''):
                            print(f"\nğŸ’¡ åˆ†æ:")
                            print(f"   - æœç‹—æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
                            print(f"   - éœ€è¦æ›´é«˜çº§çš„åçˆ¬è™«ç­–ç•¥")
                            print(f"   - å»ºè®®ä½¿ç”¨ä»£ç†æˆ–æ›´é•¿çš„ç­‰å¾…æ—¶é—´")
                        elif "é‡å®šå‘" in download_result.get('message', ''):
                            print(f"\nğŸ’¡ åˆ†æ:")
                            print(f"   - é‡å®šå‘é“¾æ¥å¤„ç†éœ€è¦æ”¹è¿›")
                            print(f"   - å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†éªŒè¯ç ")
            else:
                print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ")
        else:
            print(f"   âŒ æœç´¢å¤±è´¥: {search_result['message']}")
            
            # åˆ†æå¤±è´¥åŸå› 
            if "éªŒè¯ç " in search_result.get('message', ''):
                print(f"\nğŸ’¡ åˆ†æ:")
                print(f"   - æœç‹—æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
                print(f"   - éœ€è¦æ›´é«˜çº§çš„åçˆ¬è™«ç­–ç•¥")
                print(f"   - å»ºè®®ä½¿ç”¨ä»£ç†æˆ–æ›´é•¿çš„ç­‰å¾…æ—¶é—´")
            elif "bypass_attempted" in search_result:
                print(f"\nğŸ’¡ åˆ†æ:")
                print(f"   - å·²å°è¯•ç»•è¿‡éªŒè¯ç ")
                print(f"   - å¯èƒ½éœ€è¦äººå·¥å¹²é¢„")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n9. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ é«˜çº§åçˆ¬è™«ç­–ç•¥æµ‹è¯•å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_advanced_stealth()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
