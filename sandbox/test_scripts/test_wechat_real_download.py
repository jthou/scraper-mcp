#!/usr/bin/env python3
"""
å¾®ä¿¡å†…å®¹çœŸå®ä¸‹è½½æµ‹è¯•

ä½¿ç”¨çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œä¸‹è½½æµ‹è¯•
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_wechat_real_download():
    """æµ‹è¯•å¾®ä¿¡å†…å®¹çœŸå®ä¸‹è½½"""
    print("ğŸ“¥ å¾®ä¿¡å†…å®¹çœŸå®ä¸‹è½½æµ‹è¯•")
    print("=" * 40)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "wechat_real_download_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä½¿ç”¨ä¸€ä¸ªçœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•
        # è¿™æ˜¯ä¸€ä¸ªå…¬å¼€çš„å¾®ä¿¡æ–‡ç« é“¾æ¥ç¤ºä¾‹
        test_urls = [
            "https://mp.weixin.qq.com/s/example1",  # ç¤ºä¾‹é“¾æ¥1
            "https://mp.weixin.qq.com/s/example2",  # ç¤ºä¾‹é“¾æ¥2
        ]
        
        # å¦‚æœç”¨æˆ·æœ‰çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ›¿æ¢
        print("\n3. æ³¨æ„ï¼šç”±äºæœç‹—å¾®ä¿¡æœç´¢çš„åçˆ¬è™«æœºåˆ¶ï¼Œ")
        print("   å»ºè®®ä½¿ç”¨çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•")
        print("   æˆ–è€…æ‰‹åŠ¨ä»å¾®ä¿¡ä¸­å¤åˆ¶æ–‡ç« é“¾æ¥")
        
        # å…ˆæµ‹è¯•æœç‹—æœç´¢ï¼Œçœ‹çœ‹èƒ½å¦æ‰¾åˆ°çœŸå®é“¾æ¥
        print("\n4. å°è¯•æœç´¢å¾®ä¿¡å†…å®¹...")
        search_result = await scraper.search_wechat("Pythonç¼–ç¨‹", max_pages=1)
        
        if search_result["status"] == "success":
            print(f"   âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {search_result['total_results']} ä¸ªç»“æœ")
            
            # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ
            results = search_result.get('results', [])
            if results:
                first_result = results[0]
                print(f"\n5. å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ...")
                print(f"   æ ‡é¢˜: {first_result['title']}")
                print(f"   é“¾æ¥: {first_result['link']}")
                
                # ä¸‹è½½å•ä¸ªå†…å®¹
                download_result = await scraper.download_and_save_content(
                    first_result['link'], 
                    output_dir, 
                    first_result['title']
                )
                
                print(f"\n6. ä¸‹è½½ç»“æœ:")
                print(f"   çŠ¶æ€: {download_result['status']}")
                print(f"   æ¶ˆæ¯: {download_result['message']}")
                
                if download_result["status"] == "success":
                    print(f"   âœ… ä¸‹è½½æˆåŠŸ!")
                    print(f"   ğŸ“ PDF: {download_result['files']['pdf']}")
                    print(f"   ğŸ“„ Markdown: {download_result['files']['markdown']}")
                    
                    # æ˜¾ç¤ºä¸‹è½½ç›®å½•ç»“æ„
                    print(f"\n7. ä¸‹è½½ç›®å½•ç»“æ„:")
                    print(f"   ğŸ“ {output_dir}")
                    if output_dir.exists():
                        for item in output_dir.rglob("*"):
                            if item.is_file():
                                print(f"      ğŸ“„ {item.relative_to(output_dir)}")
                else:
                    print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_result['message']}")
                    
                    # å¦‚æœæ˜¯éªŒè¯ç é—®é¢˜ï¼Œæä¾›è§£å†³å»ºè®®
                    if "éªŒè¯ç " in download_result.get('message', ''):
                        print(f"\nğŸ’¡ è§£å†³å»ºè®®:")
                        print(f"   1. æœç‹—å¾®ä¿¡æœç´¢æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
                        print(f"   2. å»ºè®®ä½¿ç”¨çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥")
                        print(f"   3. æˆ–è€…æ‰‹åŠ¨å®ŒæˆéªŒè¯ç éªŒè¯")
            else:
                print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ")
        else:
            print(f"   âŒ æœç´¢å¤±è´¥: {search_result['message']}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n8. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ å¾®ä¿¡å†…å®¹çœŸå®ä¸‹è½½æµ‹è¯•å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_wechat_real_download()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
