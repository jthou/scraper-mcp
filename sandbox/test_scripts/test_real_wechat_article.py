#!/usr/bin/env python3
"""
çœŸå®å¾®ä¿¡æ–‡ç« æµ‹è¯•

ä½¿ç”¨çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•
"""

import asyncio
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_real_wechat_article():
    """æµ‹è¯•çœŸå®å¾®ä¿¡æ–‡ç« """
    print("ğŸ“° çœŸå®å¾®ä¿¡æ–‡ç« æµ‹è¯•")
    print("=" * 40)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "real_wechat_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä½¿ç”¨ä¸€äº›å…¬å¼€çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•
        test_articles = [
            {
                "title": "æµ‹è¯•æ–‡ç« 1",
                "url": "https://mp.weixin.qq.com/s/example1",
                "description": "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹é“¾æ¥"
            },
            {
                "title": "æµ‹è¯•æ–‡ç« 2", 
                "url": "https://mp.weixin.qq.com/s/example2",
                "description": "è¿™æ˜¯å¦ä¸€ä¸ªç¤ºä¾‹é“¾æ¥"
            }
        ]
        
        print(f"\n3. æ³¨æ„ï¼šç”±äºå¾®ä¿¡çš„ä¿æŠ¤æœºåˆ¶ï¼Œå»ºè®®ä½¿ç”¨çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥")
        print(f"   ä½ å¯ä»¥ä»å¾®ä¿¡ä¸­å¤åˆ¶æ–‡ç« é“¾æ¥æ¥æ›¿æ¢ä¸‹é¢çš„ç¤ºä¾‹é“¾æ¥")
        
        for i, article in enumerate(test_articles, 1):
            print(f"\n4.{i} æµ‹è¯•æ–‡ç« : {article['title']}")
            print(f"   URL: {article['url']}")
            print(f"   æè¿°: {article['description']}")
            
            try:
                # è®¿é—®æ–‡ç« 
                print(f"   ğŸŒ è®¿é—®æ–‡ç« ...")
                await scraper.page.goto(article['url'])
                await scraper.page.wait_for_load_state("networkidle")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(5)
                
                # è·å–é¡µé¢ä¿¡æ¯
                title = await scraper.page.title()
                current_url = scraper.page.url
                content = await scraper.page.content()
                
                print(f"   é¡µé¢æ ‡é¢˜: {title}")
                print(f"   å½“å‰URL: {current_url}")
                print(f"   å†…å®¹é•¿åº¦: {len(content)}")
                
                # æ£€æŸ¥é¡µé¢çŠ¶æ€
                if "è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€" in content:
                    print(f"   âš ï¸ é¡µé¢è¦æ±‚å¾®ä¿¡å®¢æˆ·ç«¯æ‰“å¼€")
                elif "éªŒè¯ç " in content or "captcha" in content.lower():
                    print(f"   âš ï¸ é¡µé¢åŒ…å«éªŒè¯ç ")
                elif "å¾®ä¿¡å…¬ä¼—å¹³å°" in title:
                    print(f"   âš ï¸ é¡µé¢è¢«é‡å®šå‘åˆ°å¾®ä¿¡å…¬ä¼—å¹³å°")
                else:
                    print(f"   âœ… é¡µé¢å¯èƒ½åŒ…å«çœŸå®å†…å®¹")
                
                # å°è¯•ç”ŸæˆPDF
                print(f"   ğŸ“„ å°è¯•ç”ŸæˆPDF...")
                pdf_result = await scraper.print_page_to_pdf(article['url'])
                
                if pdf_result["status"] == "success":
                    print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
                    print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_result['pdf_path']}")
                    
                    # æ£€æŸ¥PDFæ–‡ä»¶
                    pdf_path = Path(pdf_result['pdf_path'])
                    if pdf_path.exists():
                        file_size = pdf_path.stat().st_size
                        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                        
                        if file_size > 50000:
                            print(f"   âœ… PDFæ–‡ä»¶å¤§å°æ­£å¸¸ï¼Œå¯èƒ½åŒ…å«çœŸå®å†…å®¹")
                            
                            # å°è¯•è½¬æ¢ä¸ºMarkdown
                            print(f"   ğŸ“ å°è¯•è½¬æ¢ä¸ºMarkdown...")
                            markdown_result = await scraper.pdf_to_markdown(str(pdf_path))
                            
                            if markdown_result["status"] == "success":
                                print(f"   âœ… Markdownè½¬æ¢æˆåŠŸ")
                                print(f"   ğŸ“„ å†…å®¹é•¿åº¦: {len(markdown_result['markdown_content'])}")
                                
                                # ä¿å­˜æ–‡ä»¶
                                pdf_dir = output_dir / "pdfs"
                                markdown_dir = output_dir / "markdown"
                                pdf_dir.mkdir(exist_ok=True)
                                markdown_dir.mkdir(exist_ok=True)
                                
                                # å¤åˆ¶PDFæ–‡ä»¶
                                import shutil
                                target_pdf = pdf_dir / f"{article['title']}.pdf"
                                shutil.copy2(pdf_path, target_pdf)
                                
                                # ä¿å­˜Markdownæ–‡ä»¶
                                markdown_content = f"""# {article['title']}

**æ¥æº**: {article['url']}
**ä¿å­˜æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**æ¥æºå¹³å°**: å¾®ä¿¡å…¬ä¼—å¹³å°

---

{markdown_result['markdown_content']}
"""
                                
                                markdown_file = markdown_dir / f"{article['title']}.md"
                                with open(markdown_file, 'w', encoding='utf-8') as f:
                                    f.write(markdown_content)
                                
                                print(f"   âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ")
                                print(f"   ğŸ“ PDF: {target_pdf}")
                                print(f"   ğŸ“„ Markdown: {markdown_file}")
                            else:
                                print(f"   âŒ Markdownè½¬æ¢å¤±è´¥: {markdown_result['message']}")
                        else:
                            print(f"   âš ï¸ PDFæ–‡ä»¶è¾ƒå°ï¼Œå¯èƒ½æ˜¯éªŒè¯é¡µé¢")
                    else:
                        print(f"   âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨")
                else:
                    print(f"   âŒ PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}")
                
                # ç­‰å¾…ä¸€ä¸‹é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(3)
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print(f"\n5. ç”Ÿæˆçš„æ–‡ä»¶:")
        if output_dir.exists():
            for item in output_dir.rglob("*"):
                if item.is_file():
                    print(f"   ğŸ“„ {item.relative_to(output_dir)}")
        
        # æä¾›ä½¿ç”¨å»ºè®®
        print(f"\n6. ä½¿ç”¨å»ºè®®:")
        print(f"   ğŸ’¡ è¦æµ‹è¯•çœŸå®çš„å¾®ä¿¡æ–‡ç« ï¼Œè¯·:")
        print(f"   1. åœ¨å¾®ä¿¡ä¸­æ‰“å¼€ä¸€ç¯‡æ–‡ç« ")
        print(f"   2. ç‚¹å‡»å³ä¸Šè§’çš„ä¸‰ä¸ªç‚¹")
        print(f"   3. é€‰æ‹©'å¤åˆ¶é“¾æ¥'")
        print(f"   4. å°†é“¾æ¥æ›¿æ¢åˆ°ä¸Šé¢çš„test_articlesä¸­")
        print(f"   5. é‡æ–°è¿è¡Œæµ‹è¯•")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n7. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ çœŸå®å¾®ä¿¡æ–‡ç« æµ‹è¯•å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_real_wechat_article()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
