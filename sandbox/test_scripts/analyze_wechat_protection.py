#!/usr/bin/env python3
"""
å¾®ä¿¡åçˆ¬è™«æœºåˆ¶åˆ†æ

åˆ†æå¾®ä¿¡åœ¨PDFè½¬æ¢å’Œå†…å®¹æŠ“å–æ–¹é¢çš„ä¿æŠ¤æœºåˆ¶
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def analyze_wechat_protection():
    """åˆ†æå¾®ä¿¡çš„åçˆ¬è™«æœºåˆ¶"""
    print("ğŸ” å¾®ä¿¡åçˆ¬è™«æœºåˆ¶åˆ†æ")
    print("=" * 50)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # æµ‹è¯•ä¸åŒçš„ç½‘ç«™
        test_sites = [
            {
                "name": "ç™¾åº¦",
                "url": "https://www.baidu.com",
                "expected": "æ­£å¸¸ç½‘ç«™ï¼Œåº”è¯¥èƒ½æ­£å¸¸ç”ŸæˆPDF"
            },
            {
                "name": "çŸ¥ä¹",
                "url": "https://www.zhihu.com",
                "expected": "çŸ¥ä¹å¯èƒ½æœ‰åçˆ¬è™«ï¼Œä½†åº”è¯¥èƒ½ç”ŸæˆPDF"
            },
            {
                "name": "æœç‹—å¾®ä¿¡æœç´¢",
                "url": "https://weixin.sogou.com",
                "expected": "æœç‹—å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶"
            }
        ]
        
        for site in test_sites:
            print(f"\n2. æµ‹è¯• {site['name']}...")
            print(f"   URL: {site['url']}")
            print(f"   é¢„æœŸ: {site['expected']}")
            
            try:
                # è®¿é—®é¡µé¢
                await scraper.page.goto(site['url'])
                await scraper.page.wait_for_load_state("networkidle")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(3)
                
                # è·å–é¡µé¢ä¿¡æ¯
                title = await scraper.page.title()
                url = scraper.page.url
                
                print(f"   é¡µé¢æ ‡é¢˜: {title}")
                print(f"   å½“å‰URL: {url}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬è™«æœºåˆ¶
                content = await scraper.page.content()
                
                # æ£€æŸ¥å¸¸è§çš„åçˆ¬è™«å…³é”®è¯
                anti_crawl_keywords = [
                    "éªŒè¯ç ", "captcha", "å®‰å…¨éªŒè¯", "è¯·ä¾æ¬¡ç‚¹å‡»",
                    "æ£€æµ‹åˆ°å¼‚å¸¸è®¿é—®", "è®¿é—®è¿‡äºé¢‘ç¹", "éœ€è¦éªŒè¯",
                    "robot", "bot", "è‡ªåŠ¨åŒ–", "çˆ¬è™«"
                ]
                
                found_keywords = [kw for kw in anti_crawl_keywords if kw in content]
                if found_keywords:
                    print(f"   âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯: {found_keywords}")
                else:
                    print(f"   âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„åçˆ¬è™«æœºåˆ¶")
                
                # å°è¯•ç”ŸæˆPDF
                print(f"   å°è¯•ç”ŸæˆPDF...")
                pdf_result = await scraper.print_page_to_pdf(site['url'])
                
                if pdf_result["status"] == "success":
                    print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
                    print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_result['pdf_path']}")
                    
                    # æ£€æŸ¥PDFæ–‡ä»¶
                    pdf_path = Path(pdf_result['pdf_path'])
                    if pdf_path.exists():
                        file_size = pdf_path.stat().st_size
                        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                        
                        # å¦‚æœæ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ˜¯éªŒè¯ç é¡µé¢
                        if file_size < 50000:  # å°äº50KB
                            print(f"   âš ï¸ æ–‡ä»¶è¾ƒå°ï¼Œå¯èƒ½æ˜¯éªŒè¯ç é¡µé¢")
                else:
                    print(f"   âŒ PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}")
                
                print(f"   {'='*40}")
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        # ä¸“é—¨æµ‹è¯•å¾®ä¿¡æ–‡ç« é“¾æ¥
        print(f"\n3. æµ‹è¯•å¾®ä¿¡æ–‡ç« é“¾æ¥...")
        
        # ä½¿ç”¨ä¸€ä¸ªçœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•
        wechat_test_urls = [
            "https://mp.weixin.qq.com/s/example1",  # ç¤ºä¾‹é“¾æ¥
        ]
        
        for url in wechat_test_urls:
            print(f"   æµ‹è¯•URL: {url}")
            try:
                # è®¿é—®å¾®ä¿¡æ–‡ç« 
                await scraper.page.goto(url)
                await scraper.page.wait_for_load_state("networkidle")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(5)
                
                # è·å–é¡µé¢ä¿¡æ¯
                title = await scraper.page.title()
                current_url = scraper.page.url
                
                print(f"   é¡µé¢æ ‡é¢˜: {title}")
                print(f"   å½“å‰URL: {current_url}")
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
                if "éªŒè¯" in title or "captcha" in title.lower():
                    print(f"   âš ï¸ é¡µé¢è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢")
                
                # æ£€æŸ¥é¡µé¢å†…å®¹
                content = await scraper.page.content()
                
                # æ£€æŸ¥å¾®ä¿¡ç‰¹æœ‰çš„ä¿æŠ¤æœºåˆ¶
                wechat_protection_keywords = [
                    "è¯·åœ¨å¾®ä¿¡å®¢æˆ·ç«¯æ‰“å¼€", "è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€",
                    "å¾®ä¿¡å®‰å…¨éªŒè¯", "å¾®ä¿¡è®¿é—®éªŒè¯",
                    "è¯·åœ¨å¾®ä¿¡ä¸­æŸ¥çœ‹", "å¾®ä¿¡å®¢æˆ·ç«¯"
                ]
                
                found_protection = [kw for kw in wechat_protection_keywords if kw in content]
                if found_protection:
                    print(f"   âš ï¸ æ£€æµ‹åˆ°å¾®ä¿¡ä¿æŠ¤æœºåˆ¶: {found_protection}")
                
                # å°è¯•ç”ŸæˆPDF
                print(f"   å°è¯•ç”ŸæˆPDF...")
                pdf_result = await scraper.print_page_to_pdf(url)
                
                if pdf_result["status"] == "success":
                    print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
                    print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_result['pdf_path']}")
                    
                    # æ£€æŸ¥PDFæ–‡ä»¶
                    pdf_path = Path(pdf_result['pdf_path'])
                    if pdf_path.exists():
                        file_size = pdf_path.stat().st_size
                        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                else:
                    print(f"   âŒ PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}")
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        # åˆ†ææ€»ç»“
        print(f"\n4. åˆ†ææ€»ç»“:")
        print(f"   ğŸ” å¾®ä¿¡å¯èƒ½çš„åçˆ¬è™«æœºåˆ¶:")
        print(f"   - 1. é‡å®šå‘åˆ°éªŒè¯ç é¡µé¢")
        print(f"   - 2. æ£€æµ‹è‡ªåŠ¨åŒ–è®¿é—®ç‰¹å¾")
        print(f"   - 3. è¦æ±‚å¾®ä¿¡å®¢æˆ·ç«¯æ‰“å¼€")
        print(f"   - 4. åŠ¨æ€åŠ è½½å†…å®¹ï¼ŒPDFç”Ÿæˆæ—¶å†…å®¹ä¸ºç©º")
        print(f"   - 5. ä½¿ç”¨JavaScriptæ¸²æŸ“ï¼Œéœ€è¦ç­‰å¾…æ—¶é—´")
        print(f"   - 6. æ£€æµ‹æµè§ˆå™¨æŒ‡çº¹å’Œç”¨æˆ·ä»£ç†")
        
        print(f"\n   ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print(f"   - 1. ä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨ç¯å¢ƒ")
        print(f"   - 2. æ¨¡æ‹Ÿå¾®ä¿¡å®¢æˆ·ç«¯çš„è¯·æ±‚å¤´")
        print(f"   - 3. ä½¿ç”¨ä»£ç†IPè½®æ¢")
        print(f"   - 4. å¢åŠ æ›´é•¿çš„ç­‰å¾…æ—¶é—´")
        print(f"   - 5. ä½¿ç”¨æ— å¤´æµè§ˆå™¨ä½†æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º")
        print(f"   - 6. ç›´æ¥ä½¿ç”¨å¾®ä¿¡APIï¼ˆå¦‚æœæœ‰æƒé™ï¼‰")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n5. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ å¾®ä¿¡åçˆ¬è™«æœºåˆ¶åˆ†æå®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await analyze_wechat_protection()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
