#!/usr/bin/env python3
"""
PDFç”ŸæˆåŠŸèƒ½æµ‹è¯•

ä¸“é—¨æµ‹è¯•PDFç”ŸæˆåŠŸèƒ½ï¼Œä½¿ç”¨ç®€å•çš„ç½‘é¡µ
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.core.wechat_scraper import WeChatScraper


async def test_pdf_generation():
    """æµ‹è¯•PDFç”ŸæˆåŠŸèƒ½"""
    print("ğŸ“„ PDFç”ŸæˆåŠŸèƒ½æµ‹è¯•")
    print("=" * 40)
    
    scraper = WeChatScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        print("\n1. è®¾ç½®æµè§ˆå™¨...")
        setup_result = await scraper.setup_browser(headless=False, persistent=False)
        print(f"   ç»“æœ: {setup_result}")
        
        if setup_result["status"] != "success":
            print("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(__file__).parent / "pdf_test"
        output_dir.mkdir(exist_ok=True)
        print(f"\n2. è¾“å‡ºç›®å½•: {output_dir}")
        
        # æµ‹è¯•ç®€å•çš„ç½‘é¡µPDFç”Ÿæˆ
        test_urls = [
            "https://www.baidu.com",
            "https://www.zhihu.com",
            "https://www.github.com"
        ]
        
        for i, url in enumerate(test_urls, 1):
            print(f"\n3.{i} æµ‹è¯•PDFç”Ÿæˆ: {url}")
            
            try:
                # ç›´æ¥æµ‹è¯•PDFç”Ÿæˆ
                pdf_result = await scraper.print_page_to_pdf(url)
                print(f"   PDFç”Ÿæˆç»“æœ: {pdf_result}")
                
                if pdf_result["status"] == "success":
                    print(f"   âœ… PDFç”ŸæˆæˆåŠŸ")
                    print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_result['pdf_path']}")
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    pdf_path = Path(pdf_result['pdf_path'])
                    if pdf_path.exists():
                        file_size = pdf_path.stat().st_size
                        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                    else:
                        print(f"   âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨")
                else:
                    print(f"   âŒ PDFç”Ÿæˆå¤±è´¥: {pdf_result['message']}")
                
                # ç­‰å¾…ä¸€ä¸‹é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(3)
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„PDFæ–‡ä»¶
        print(f"\n4. ç”Ÿæˆçš„PDFæ–‡ä»¶:")
        pdf_dir = Path(__file__).parent.parent.parent / "data" / "pdfs"
        if pdf_dir.exists():
            pdf_files = list(pdf_dir.glob("wechat_*.pdf"))
            for pdf_file in pdf_files[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªæ–‡ä»¶
                print(f"   ğŸ“„ {pdf_file.name} ({pdf_file.stat().st_size} å­—èŠ‚)")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\n5. æ¸…ç†èµ„æº...")
        try:
            await scraper.cleanup()
            print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
    
    print("\nğŸ‰ PDFç”ŸæˆåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_pdf_generation()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
