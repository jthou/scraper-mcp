#!/usr/bin/env python3
"""ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·MCP Server"""
import asyncio
import sys
from pathlib import Path
from typing import Any, Dict, List

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import (
    CallToolRequest,
    CallToolResult,
    ListToolsRequest,
    ListToolsResult,
    Tool,
    TextContent
)
from src.core.web_scraper import WebScraper
from src.utils.logger import Logger


class ScraperMCPServer:
    """ç½‘é¡µå†…å®¹æŠ“å–MCP Server"""
    
    def __init__(self):
        self.logger = Logger()
        self.web_scraper = WebScraper()
        self.tools = [
            Tool(
                name="open_webpage",
                description="ä½¿ç”¨ç³»ç»ŸChromeæµè§ˆå™¨æ‰“å¼€æŒ‡å®šç½‘é¡µ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "è¦æ‰“å¼€çš„ç½‘é¡µURL"
                        },
                        "headless": {
                            "type": "boolean",
                            "description": "æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰",
                            "default": False
                        }
                    },
                    "required": ["url"]
                }
            ),
            Tool(
                name="get_page_info",
                description="è·å–ç½‘é¡µåŸºæœ¬ä¿¡æ¯",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ç½‘é¡µURL"
                        }
                    },
                    "required": ["url"]
                }
            ),
            Tool(
                name="login_zhihu",
                description="ç™»å½•çŸ¥ä¹ç½‘ç«™ï¼Œä¿æŒç™»å½•çŠ¶æ€",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "headless": {
                            "type": "boolean",
                            "description": "æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰",
                            "default": False
                        }
                    },
                    "required": []
                }
            ),
            Tool(
                name="read_zhihu_page",
                description="è¯»å–çŸ¥ä¹ç½‘é¡µå†…å®¹ï¼ˆéœ€è¦å·²ç™»å½•ï¼‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "è¦è¯»å–çš„çŸ¥ä¹é¡µé¢URL",
                            "default": "https://www.zhihu.com"
                        }
                    },
                    "required": []
                }
            ),
            Tool(
                name="search_zhihu",
                description="æœç´¢çŸ¥ä¹å†…å®¹ï¼Œè·å–ç¬¦åˆè¦æ±‚çš„æ‰€æœ‰é¡µé¢é“¾æ¥",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœç´¢å…³é”®è¯"
                        },
                        "max_pages": {
                            "type": "integer",
                            "description": "æœ€å¤§æœç´¢é¡µæ•°",
                            "default": 3
                        },
                        "min_relevance": {
                            "type": "number",
                            "description": "æœ€å°ç›¸å…³æ€§é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰",
                            "default": 0.5
                        }
                    },
                    "required": ["query"]
                }
            ),
            Tool(
                name="download_zhihu_content",
                description="ä¸‹è½½çŸ¥ä¹å†…å®¹å¹¶ä¿å­˜ä¸ºPDFå’ŒMarkdownæ–‡ä»¶",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "çŸ¥ä¹é¡µé¢URL"
                        },
                        "output_dir": {
                            "type": "string",
                            "description": "ä¿å­˜ç›®å½•è·¯å¾„"
                        },
                        "title": {
                            "type": "string",
                            "description": "è‡ªå®šä¹‰æ–‡ä»¶å(å¯é€‰)"
                        }
                    },
                    "required": ["url", "output_dir"]
                }
            ),
            Tool(
                name="batch_download_zhihu",
                description="æ‰¹é‡ä¸‹è½½çŸ¥ä¹æœç´¢ç»“æœå¹¶ä¿å­˜",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœç´¢å…³é”®è¯"
                        },
                        "output_dir": {
                            "type": "string",
                            "description": "ä¿å­˜ç›®å½•è·¯å¾„"
                        },
                        "max_pages": {
                            "type": "integer",
                            "description": "æœ€å¤§æœç´¢é¡µæ•°",
                            "default": 3
                        },
                        "min_relevance": {
                            "type": "number",
                            "description": "æœ€å°ç›¸å…³æ€§é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰",
                            "default": 0.5
                        }
                    },
                    "required": ["query", "output_dir"]
                }
            )
        ]
    
    async def handle_list_tools(self, request: ListToolsRequest) -> ListToolsResult:
        """å¤„ç†å·¥å…·åˆ—è¡¨è¯·æ±‚"""
        self.logger.info(f"æ”¶åˆ°å·¥å…·åˆ—è¡¨è¯·æ±‚: {request}")
        return ListToolsResult(tools=self.tools)
    
    async def handle_call_tool(self, request: CallToolRequest) -> CallToolResult:
        """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚"""
        self.logger.info(f"æ”¶åˆ°å·¥å…·è°ƒç”¨è¯·æ±‚: {request}")
        
        try:
            # ä»paramsä¸­è·å–å·¥å…·åç§°å’Œå‚æ•°
            tool_name = request.params.name
            arguments = request.params.arguments
            
            if tool_name == "open_webpage":
                url = arguments.get("url")
                headless = arguments.get("headless", False)
                
                if not url:
                    return CallToolResult(
                        content=[TextContent(type="text", text="é”™è¯¯ï¼šç¼ºå°‘URLå‚æ•°")],
                        isError=True
                    )
                
                result = await self.web_scraper.open_webpage(url, headless)
                
                if result["status"] == "success":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"âœ… æˆåŠŸæ‰“å¼€ç½‘é¡µ: {url}\næµè§ˆå™¨ç±»å‹: {result['browser_type']}\næ— å¤´æ¨¡å¼: {result['headless']}"
                        )]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ æ‰“å¼€ç½‘é¡µå¤±è´¥: {result['message']}")],
                        isError=True
                    )
            
            elif tool_name == "get_page_info":
                url = arguments.get("url")
                
                if not url:
                    return CallToolResult(
                        content=[TextContent(type="text", text="é”™è¯¯ï¼šç¼ºå°‘URLå‚æ•°")],
                        isError=True
                    )
                
                result = await self.web_scraper.get_page_info(url)
                return CallToolResult(
                    content=[TextContent(
                        type="text", 
                        text=f"ç½‘é¡µä¿¡æ¯: {result['message']}"
                    )]
                )
            
            elif tool_name == "login_zhihu":
                headless = arguments.get("headless", False)
                
                result = await self.web_scraper.login_zhihu(headless)
                
                if result["status"] == "success":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"âœ… {result['message']}\nç™»å½•çŠ¶æ€: {result['login_status']}\nç”¨æˆ·æ•°æ®ç›®å½•: {result['user_data_dir']}"
                        )]
                    )
                elif result["status"] == "waiting":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"â³ {result['message']}\nè¯·æ‰‹åŠ¨æ‰«ç ç™»å½•\nç”¨æˆ·æ•°æ®ç›®å½•: {result['user_data_dir']}"
                        )]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ {result['message']}")],
                        isError=True
                    )
            
            elif tool_name == "read_zhihu_page":
                url = arguments.get("url", "https://www.zhihu.com")
                
                result = await self.web_scraper.read_zhihu_page(url)
                
                if result["status"] == "success":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"âœ… {result['message']}\né¡µé¢æ ‡é¢˜: {result['title']}\nå†…å®¹é•¿åº¦: {result['content_length']} å­—ç¬¦\næ–‡æœ¬å†…å®¹: {result['text_content']}"
                        )]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ {result['message']}")],
                        isError=True
                    )
            
            elif tool_name == "search_zhihu":
                query = arguments.get("query")
                max_pages = arguments.get("max_pages", 3)
                min_relevance = arguments.get("min_relevance", 0.5)
                
                if not query:
                    return CallToolResult(
                        content=[TextContent(type="text", text="é”™è¯¯ï¼šç¼ºå°‘æœç´¢å…³é”®è¯å‚æ•°")],
                        isError=True
                    )
                
                result = await self.web_scraper.search_zhihu(query, max_pages, min_relevance)
                
                if result["status"] == "success":
                    # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
                    qualified_links = result.get("qualified_links", [])
                    results = result.get("results", [])
                    
                    # æ„å»ºè¾“å‡ºæ–‡æœ¬
                    output_text = f"âœ… {result['message']}\n"
                    output_text += f"æœç´¢å…³é”®è¯: {result['query']}\n"
                    output_text += f"æ€»ç»“æœæ•°: {result['total_results']}\n"
                    output_text += f"ç¬¦åˆè¦æ±‚çš„ç»“æœæ•°: {result['filtered_results']}\n\n"
                    
                    if qualified_links:
                        output_text += "ğŸ“‹ ç¬¦åˆè¦æ±‚çš„æ‰€æœ‰é¡µé¢é“¾æ¥:\n"
                        for i, link in enumerate(qualified_links, 1):
                            output_text += f"{i}. {link}\n"
                        
                        output_text += "\nğŸ“Š è¯¦ç»†ç»“æœä¿¡æ¯:\n"
                        for i, item in enumerate(results[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ªç»“æœ
                            output_text += f"\n{i}. {item['title']}\n"
                            output_text += f"   ä½œè€…: {item['author']}\n"
                            output_text += f"   ç‚¹èµ: {item['vote_count']}\n"
                            output_text += f"   ç›¸å…³æ€§: {item['relevance_score']:.2f}\n"
                            output_text += f"   é“¾æ¥: {item['url']}\n"
                            if item['summary']:
                                output_text += f"   æ‘˜è¦: {item['summary'][:100]}...\n"
                    else:
                        output_text += "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„ç»“æœ"
                    
                    return CallToolResult(
                        content=[TextContent(type="text", text=output_text)]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ {result['message']}")],
                        isError=True
                    )
            
            elif tool_name == "download_zhihu_content":
                url = arguments.get("url")
                output_dir = arguments.get("output_dir")
                title = arguments.get("title")
                
                if not url or not output_dir:
                    return CallToolResult(
                        content=[TextContent(type="text", text="é”™è¯¯ï¼šç¼ºå°‘URLæˆ–è¾“å‡ºç›®å½•å‚æ•°")],
                        isError=True
                    )
                
                from pathlib import Path
                output_path = Path(output_dir)
                
                result = await self.web_scraper.download_and_save_content(url, output_path, title)
                
                if result["status"] == "success":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"âœ… {result['message']}\næ–‡ä»¶å: {result['base_name']}\nä¿å­˜ç›®å½•: {result['files']['markdown']}\nPDF: {result['files']['pdf']}\nåŸå§‹æ ‡é¢˜: {result['original_title']}"
                        )]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ {result['message']}")],
                        isError=True
                    )
            
            elif tool_name == "batch_download_zhihu":
                query = arguments.get("query")
                output_dir = arguments.get("output_dir")
                max_pages = arguments.get("max_pages", 3)
                min_relevance = arguments.get("min_relevance", 0.5)
                
                if not query or not output_dir:
                    return CallToolResult(
                        content=[TextContent(type="text", text="é”™è¯¯ï¼šç¼ºå°‘æœç´¢å…³é”®è¯æˆ–è¾“å‡ºç›®å½•å‚æ•°")],
                        isError=True
                    )
                
                from pathlib import Path
                output_path = Path(output_dir)
                
                result = await self.web_scraper.batch_download_content(query, output_path, max_pages, min_relevance)
                
                if result["status"] == "success":
                    return CallToolResult(
                        content=[TextContent(
                            type="text", 
                            text=f"âœ… {result['message']}\næœç´¢å…³é”®è¯: {result['query']}\næ€»è®¡å‘ç°: {result['total_found']} ç¯‡\næˆåŠŸä¸‹è½½: {result['success_count']} ç¯‡\nå¤±è´¥: {result['failed_count']} ç¯‡\nä¿å­˜ç›®å½•: {result['output_dir']}\næ€»ç»“æ–‡ä»¶: {result['summary_file']}"
                        )]
                    )
                else:
                    return CallToolResult(
                        content=[TextContent(type="text", text=f"âŒ {result['message']}")],
                        isError=True
                    )
            
            else:
                return CallToolResult(
                    content=[TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {tool_name}")],
                    isError=True
                )
                
        except Exception as e:
            self.logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return CallToolResult(
                content=[TextContent(type="text", text=f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}")],
                isError=True
            )


async def main():
    """ä¸»å‡½æ•°"""
    logger = Logger()
    logger.info("å¯åŠ¨ç½‘é¡µå†…å®¹æŠ“å–MCP Server...")
    
    try:
        # åˆ›å»ºMCP Serverå®ä¾‹
        server = ScraperMCPServer()
        
        # åˆ›å»ºMCP Server
        mcp_server = Server("scraper-mcp")
        
        # æ³¨å†Œå·¥å…·åˆ—è¡¨å¤„ç†å™¨
        mcp_server.list_tools()(server.handle_list_tools)
        
        # æ³¨å†Œå·¥å…·è°ƒç”¨å¤„ç†å™¨
        mcp_server.call_tool()(server.handle_call_tool)
        
        logger.info("MCP Serverå·²å¯åŠ¨ï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
        logger.info("å¯ç”¨å·¥å…·:")
        for tool in server.tools:
            logger.info(f"  - {tool.name}: {tool.description}")
        
        # MCP Serverå·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥
        logger.info("MCP Serverå·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
        
        # å¯åŠ¨stdio serverï¼ŒæŒç»­è¿è¡Œ
        async with stdio_server() as (read_stream, write_stream):
            await mcp_server.run(
                read_stream,
                write_stream,
                mcp_server.create_initialization_options()
            )
            
    except KeyboardInterrupt:
        logger.info("MCP Serverè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"MCP Serverè¿è¡Œå¤±è´¥: {e}")
        return False
    
    logger.info("MCP Serverå·²åœæ­¢")
    return True


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
