"""GitHubç»Ÿä¸€å†…å®¹æŠ“å–å™¨ - æ•´åˆAPIå’ŒPagesæŠ“å–"""
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from urllib.parse import urlparse

from .config import GitHubConfig, default_config
from .utils import GitHubUtils
from .repo_scraper import GitHubRepoScraper
from .pages_scraper import GitHubPagesScraper


class GitHubContentScraper:
    """GitHubç»Ÿä¸€å†…å®¹æŠ“å–å™¨ - æ•´åˆä»“åº“APIå’ŒPagesæŠ“å–åŠŸèƒ½"""
    
    def __init__(self, config: GitHubConfig = None):
        """
        åˆå§‹åŒ–GitHubå†…å®¹æŠ“å–å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or default_config
        self.repo_scraper = GitHubRepoScraper(config)
        self.pages_scraper = GitHubPagesScraper(config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.config.output_base_dir / "GitHub"
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.repo_scraper.__aenter__()
        await self.pages_scraper.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.repo_scraper.__aexit__(exc_type, exc_val, exc_tb)
        await self.pages_scraper.__aexit__(exc_type, exc_val, exc_tb)
    
    async def scrape_comprehensive(self, owner: str, repo: str = None, 
                                 include_pages: bool = True,
                                 include_repo: bool = True,
                                 max_repo_files: int = None,
                                 max_pages: int = None) -> Dict[str, Any]:
        """
        ç»¼åˆæŠ“å–GitHubå†…å®¹ï¼ˆä»“åº“+Pagesï¼‰
        
        Args:
            owner: GitHubç”¨æˆ·åæˆ–ç»„ç»‡å
            repo: ä»“åº“åï¼Œå¦‚æœä¸ºNoneåˆ™æŠ“å–ç”¨æˆ·çš„ä¸»è¦å†…å®¹
            include_pages: æ˜¯å¦åŒ…å«GitHub PagesæŠ“å–
            include_repo: æ˜¯å¦åŒ…å«ä»“åº“å†…å®¹æŠ“å–
            max_repo_files: ä»“åº“æœ€å¤§æ–‡ä»¶æ•°
            max_pages: Pagesæœ€å¤§é¡µé¢æ•°
            
        Returns:
            ç»¼åˆæŠ“å–ç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹ç»¼åˆæŠ“å–GitHubå†…å®¹: {owner}/{repo if repo else 'all-content'}")
        
        results = {
            "status": "success",
            "owner": owner,
            "repo": repo,
            "scrape_type": "comprehensive",
            "started_at": datetime.now().isoformat(),
            "config_used": self.config.to_dict()
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        target_name = f"{owner}_{repo}" if repo else owner
        output_dir = self.output_dir / target_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        tasks = []
        
        try:
            # 1. æŠ“å–ä»“åº“å†…å®¹ï¼ˆå¦‚æœæŒ‡å®šäº†repoï¼‰
            if include_repo and repo:
                print("ğŸ“š å‡†å¤‡æŠ“å–ä»“åº“å†…å®¹...")
                tasks.append(("repository", self._scrape_repository_task(owner, repo, max_repo_files)))
            
            # 2. å‘ç°å¹¶æŠ“å–GitHub Pages
            if include_pages:
                print("ğŸ•·ï¸ å‡†å¤‡æŠ“å–GitHub Pages...")
                tasks.append(("pages", self._scrape_pages_task(owner, repo, max_pages)))
            
            # å¹¶è¡Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡
            if tasks:
                task_results = await asyncio.gather(
                    *[task[1] for task in tasks], 
                    return_exceptions=True
                )
                
                # æ•´ç†ç»“æœ
                for i, (task_name, result) in enumerate(zip([t[0] for t in tasks], task_results)):
                    if isinstance(result, Exception):
                        results[task_name] = {
                            "status": "error",
                            "error": str(result),
                            "completed_at": datetime.now().isoformat()
                        }
                        print(f"âŒ {task_name} æŠ“å–å¤±è´¥: {result}")
                    else:
                        results[task_name] = result
                        print(f"âœ… {task_name} æŠ“å–å®Œæˆ")
            
            results["completed_at"] = datetime.now().isoformat()
            
            # ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Š
            if self.config.save_metadata:
                await self._save_comprehensive_results(results, output_dir)
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            self._calculate_comprehensive_stats(results)
            
            return results
            
        except Exception as e:
            print(f"âŒ ç»¼åˆæŠ“å–å¤±è´¥: {e}")
            results.update({
                "status": "error",
                "error": str(e),
                "completed_at": datetime.now().isoformat()
            })
            return results
    
    async def _scrape_repository_task(self, owner: str, repo: str, max_files: int) -> Dict[str, Any]:
        """ä»“åº“æŠ“å–ä»»åŠ¡"""
        return await self.repo_scraper.scrape_repository_documentation(
            owner, repo, max_files or self.config.repo_max_files, True
        )
    
    async def _scrape_pages_task(self, owner: str, repo: str, max_pages: int) -> Dict[str, Any]:
        """PagesæŠ“å–ä»»åŠ¡"""
        # é¦–å…ˆå‘ç°Pagesç«™ç‚¹
        discovered_sites = await self.pages_scraper.discover_github_pages(owner, repo)
        
        if not discovered_sites:
            return {
                "status": "no_pages_found",
                "owner": owner,
                "repo": repo,
                "message": "No GitHub Pages sites discovered",
                "completed_at": datetime.now().isoformat()
            }
        
        # æŠ“å–å‘ç°çš„ç«™ç‚¹
        pages_results = []
        
        for site in discovered_sites:
            if site.get("status") == "active":
                site_url = site["url"]
                print(f"ğŸ•·ï¸ æŠ“å–Pagesç«™ç‚¹: {site_url}")
                
                site_result = await self.pages_scraper.scrape_documentation_site(
                    site_url, max_pages or self.config.pages_max_pages
                )
                
                # æ·»åŠ ç«™ç‚¹å‘ç°ä¿¡æ¯
                site_result["discovery_info"] = site
                pages_results.append(site_result)
        
        return {
            "status": "success",
            "discovered_sites": discovered_sites,
            "scraped_sites": pages_results,
            "sites_count": len(discovered_sites),
            "successful_scrapes": len([r for r in pages_results if r.get("status") == "success"]),
            "completed_at": datetime.now().isoformat()
        }
    
    def _calculate_comprehensive_stats(self, results: Dict[str, Any]):
        """è®¡ç®—ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_content_sources": 0,
            "total_files": 0,
            "total_pages": 0,
            "total_words": 0,
            "success_rate": 0.0,
            "content_types": set()
        }
        
        # ç»Ÿè®¡ä»“åº“å†…å®¹
        if "repository" in results and results["repository"].get("status") == "success":
            repo_data = results["repository"]
            stats["total_content_sources"] += 1
            stats["total_files"] += repo_data.get("scrape_summary", {}).get("extracted_files", 0)
            stats["content_types"].add("repository")
        
        # ç»Ÿè®¡Pageså†…å®¹
        if "pages" in results and results["pages"].get("status") == "success":
            pages_data = results["pages"]
            stats["total_content_sources"] += 1
            stats["content_types"].add("pages")
            
            for site_result in pages_data.get("scraped_sites", []):
                if site_result.get("status") == "success":
                    site_summary = site_result.get("scrape_summary", {})
                    stats["total_pages"] += site_summary.get("successful_pages", 0)
                    stats["total_words"] += site_summary.get("total_words", 0)
        
        # è®¡ç®—æˆåŠŸç‡
        total_sources = len([k for k in results.keys() if k in ["repository", "pages"]])
        successful_sources = len([k for k in ["repository", "pages"] 
                                if k in results and results[k].get("status") == "success"])
        
        if total_sources > 0:
            stats["success_rate"] = (successful_sources / total_sources) * 100
        
        stats["content_types"] = list(stats["content_types"])
        results["comprehensive_stats"] = stats
    
    async def discover_github_content(self, owner: str, repo: str = None) -> Dict[str, Any]:
        """
        å‘ç°GitHubå†…å®¹æºï¼ˆä¸è¿›è¡Œå®é™…æŠ“å–ï¼‰
        
        Args:
            owner: GitHubç”¨æˆ·åæˆ–ç»„ç»‡å
            repo: ä»“åº“å
            
        Returns:
            å‘ç°çš„å†…å®¹æºä¿¡æ¯
        """
        print(f"ğŸ” å‘ç°GitHubå†…å®¹æº: {owner}/{repo if repo else 'all'}")
        
        discovery_results = {
            "status": "success",
            "owner": owner,
            "repo": repo,
            "discovered_at": datetime.now().isoformat(),
            "sources": {}
        }
        
        try:
            # å¹¶è¡Œå‘ç°ä¸åŒç±»å‹çš„å†…å®¹
            tasks = []
            
            # å‘ç°ä»“åº“ä¿¡æ¯
            if repo:
                tasks.append(("repository_info", self.repo_scraper.get_repository_info(owner, repo)))
            
            # å‘ç°Pagesç«™ç‚¹
            tasks.append(("pages_sites", self.pages_scraper.discover_github_pages(owner, repo)))
            
            # æ‰§è¡Œå‘ç°ä»»åŠ¡
            results = await asyncio.gather(*[task[1] for task in tasks], return_exceptions=True)
            
            # æ•´ç†å‘ç°ç»“æœ
            for i, (task_name, result) in enumerate(zip([t[0] for t in tasks], results)):
                if isinstance(result, Exception):
                    discovery_results["sources"][task_name] = {
                        "status": "error",
                        "error": str(result)
                    }
                else:
                    discovery_results["sources"][task_name] = result
            
            return discovery_results
            
        except Exception as e:
            discovery_results.update({
                "status": "error", 
                "error": str(e)
            })
            return discovery_results
    
    async def scrape_by_url(self, url: str, 
                          scrape_type: str = "auto") -> Dict[str, Any]:
        """
        æ ¹æ®URLè‡ªåŠ¨è¯†åˆ«å¹¶æŠ“å–GitHubå†…å®¹
        
        Args:
            url: GitHub URL
            scrape_type: æŠ“å–ç±»å‹ ("auto", "repository", "pages")
            
        Returns:
            æŠ“å–ç»“æœ
        """
        print(f"ğŸ”— æ ¹æ®URLæŠ“å–GitHubå†…å®¹: {url}")
        
        # è§£æURL
        url_info = self._parse_github_url(url)
        if not url_info:
            return {
                "status": "error",
                "error": "Invalid GitHub URL",
                "url": url,
                "scraped_at": datetime.now().isoformat()
            }
        
        print(f"ğŸ¯ è¯†åˆ«åˆ°: {url_info['type']} - {url_info.get('owner')}/{url_info.get('repo', 'N/A')}")
        
        # æ ¹æ®URLç±»å‹é€‰æ‹©æŠ“å–æ–¹å¼
        if scrape_type == "auto":
            if url_info["type"] == "repository":
                scrape_type = "repository"
            elif url_info["type"] == "pages":
                scrape_type = "pages"
            else:
                scrape_type = "comprehensive"  # å¦‚æœæ— æ³•ç¡®å®šï¼Œä½¿ç”¨ç»¼åˆæ¨¡å¼
        
        try:
            if scrape_type == "repository":
                return await self.repo_scraper.scrape_repository_documentation(
                    url_info["owner"], url_info["repo"]
                )
            elif scrape_type == "pages":
                return await self.pages_scraper.scrape_documentation_site(url)
            else:  # comprehensive
                return await self.scrape_comprehensive(
                    url_info["owner"], url_info.get("repo")
                )
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "url": url,
                "scraped_at": datetime.now().isoformat()
            }
    
    def _parse_github_url(self, url: str) -> Optional[Dict[str, Any]]:
        """è§£æGitHub URL"""
        try:
            parsed = urlparse(url)
            
            if parsed.netloc not in ["github.com", "www.github.com"]:
                # æ£€æŸ¥æ˜¯å¦ä¸ºGitHub Pages
                if ".github.io" in parsed.netloc:
                    parts = parsed.netloc.split(".")
                    if len(parts) >= 3 and parts[-2] == "github" and parts[-1] == "io":
                        owner = parts[0]
                        # å°è¯•ä»è·¯å¾„ä¸­æå–repoå
                        path_parts = parsed.path.strip("/").split("/")
                        repo = path_parts[0] if path_parts and path_parts[0] else None
                        
                        return {
                            "type": "pages",
                            "owner": owner,
                            "repo": repo,
                            "url": url
                        }
                return None
            
            # è§£ægithub.com URL
            path_parts = parsed.path.strip("/").split("/")
            
            if len(path_parts) < 1:
                return None
            
            owner = path_parts[0]
            repo = path_parts[1] if len(path_parts) > 1 else None
            
            return {
                "type": "repository",
                "owner": owner,
                "repo": repo,
                "url": url
            }
            
        except Exception:
            return None
    
    async def _save_comprehensive_results(self, results: Dict[str, Any], output_dir: Path):
        """ä¿å­˜ç»¼åˆæŠ“å–ç»“æœ"""
        # ä¿å­˜ä¸»è¦å…ƒæ•°æ®
        metadata = {
            "scrape_info": {
                k: v for k, v in results.items() 
                if k not in ["repository", "pages"]
            }
        }
        
        metadata_file = output_dir / "comprehensive_metadata.json"
        GitHubUtils.save_json(metadata, metadata_file)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report(results, output_dir)
        
        print(f"ğŸ“ ç»¼åˆæŠ“å–ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _generate_comprehensive_report(self, results: Dict[str, Any], output_dir: Path):
        """ç”Ÿæˆç»¼åˆæŠ“å–æŠ¥å‘Š"""
        report_file = output_dir / "README.md"
        
        owner = results.get("owner", "unknown")
        repo = results.get("repo")
        stats = results.get("comprehensive_stats", {})
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# {owner}/{repo if repo else 'GitHubå†…å®¹'} ç»¼åˆæŠ“å–æŠ¥å‘Š\n\n")
            
            f.write("## æŠ“å–æ¦‚è§ˆ\n\n")
            f.write(f"**ç›®æ ‡**: {owner}/{repo if repo else 'All content'}\n")
            f.write(f"**æŠ“å–æ—¶é—´**: {results.get('started_at', 'N/A')}\n")
            f.write(f"**å®Œæˆæ—¶é—´**: {results.get('completed_at', 'N/A')}\n")
            f.write(f"**æˆåŠŸç‡**: {stats.get('success_rate', 0):.1f}%\n\n")
            
            f.write("## å†…å®¹ç»Ÿè®¡\n\n")
            f.write(f"- **å†…å®¹æºæ•°é‡**: {stats.get('total_content_sources', 0)}\n")
            f.write(f"- **ä»“åº“æ–‡ä»¶æ•°**: {stats.get('total_files', 0)}\n")
            f.write(f"- **Pagesé¡µé¢æ•°**: {stats.get('total_pages', 0)}\n")
            f.write(f"- **æ€»å­—æ•°**: {stats.get('total_words', 0):,}\n")
            f.write(f"- **å†…å®¹ç±»å‹**: {', '.join(stats.get('content_types', []))}\n\n")
            
            # ä»“åº“æŠ“å–ç»“æœ
            if "repository" in results:
                repo_result = results["repository"]
                f.write("## ä»“åº“å†…å®¹æŠ“å–\n\n")
                
                if repo_result.get("status") == "success":
                    repo_summary = repo_result.get("scrape_summary", {})
                    f.write(f"âœ… **çŠ¶æ€**: æˆåŠŸ\n")
                    f.write(f"- æ€»æ–‡ä»¶æ•°: {repo_summary.get('total_files_found', 0)}\n")
                    f.write(f"- æ–‡æ¡£æ–‡ä»¶æ•°: {repo_summary.get('documentation_files', 0)}\n")
                    f.write(f"- å·²æŠ“å–æ–‡ä»¶æ•°: {repo_summary.get('extracted_files', 0)}\n\n")
                else:
                    f.write(f"âŒ **çŠ¶æ€**: å¤±è´¥\n")
                    f.write(f"- é”™è¯¯: {repo_result.get('error', 'Unknown error')}\n\n")
            
            # PagesæŠ“å–ç»“æœ
            if "pages" in results:
                pages_result = results["pages"]
                f.write("## GitHub PagesæŠ“å–\n\n")
                
                if pages_result.get("status") == "success":
                    f.write(f"âœ… **çŠ¶æ€**: æˆåŠŸ\n")
                    f.write(f"- å‘ç°ç«™ç‚¹æ•°: {pages_result.get('sites_count', 0)}\n")
                    f.write(f"- æˆåŠŸæŠ“å–æ•°: {pages_result.get('successful_scrapes', 0)}\n\n")
                    
                    # åˆ—å‡ºæŠ“å–çš„ç«™ç‚¹
                    for site_result in pages_result.get("scraped_sites", []):
                        if site_result.get("status") == "success":
                            site_summary = site_result.get("scrape_summary", {})
                            discovery_info = site_result.get("discovery_info", {})
                            
                            f.write(f"### {discovery_info.get('url', 'Unknown site')}\n")
                            f.write(f"- ç”Ÿæˆå™¨: {discovery_info.get('generator', 'Unknown')}\n")
                            f.write(f"- æˆåŠŸé¡µé¢: {site_summary.get('successful_pages', 0)}\n")
                            f.write(f"- æ€»å­—æ•°: {site_summary.get('total_words', 0):,}\n\n")
                else:
                    f.write(f"âŒ **çŠ¶æ€**: å¤±è´¥\n")
                    f.write(f"- é”™è¯¯: {pages_result.get('error', 'Unknown error')}\n\n")
            
            f.write("---\n\n")
            f.write("*æŠ¥å‘Šç”± GitHub Content Scraper ç”Ÿæˆ*\n")


# ä¾¿æ·å‡½æ•°
async def scrape_github_content(target: str, 
                              scrape_type: str = "auto",
                              include_pages: bool = True,
                              include_repo: bool = True,
                              config: GitHubConfig = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæŠ“å–GitHubå†…å®¹
    
    Args:
        target: GitHub URL æˆ– "owner" æˆ– "owner/repo" æ ¼å¼
        scrape_type: æŠ“å–ç±»å‹ ("auto", "repository", "pages", "comprehensive") 
        include_pages: æ˜¯å¦åŒ…å«PagesæŠ“å–
        include_repo: æ˜¯å¦åŒ…å«ä»“åº“æŠ“å–
        config: é…ç½®å¯¹è±¡
        
    Returns:
        æŠ“å–ç»“æœ
    """
    async with GitHubContentScraper(config) as scraper:
        if target.startswith("http"):
            # URLæ ¼å¼
            return await scraper.scrape_by_url(target, scrape_type)
        else:
            # owner æˆ– owner/repo æ ¼å¼
            parts = target.split("/")
            owner = parts[0]
            repo = parts[1] if len(parts) > 1 else None
            
            if scrape_type == "comprehensive":
                return await scraper.scrape_comprehensive(
                    owner, repo, include_pages, include_repo
                )
            elif scrape_type == "repository" and repo:
                return await scraper.repo_scraper.scrape_repository_documentation(owner, repo)
            elif scrape_type == "pages":
                if repo:
                    sites = await scraper.pages_scraper.discover_github_pages(owner, repo)
                    if sites and sites[0].get("status") == "active":
                        return await scraper.pages_scraper.scrape_documentation_site(sites[0]["url"])
                return {"status": "no_pages_found", "message": "No GitHub Pages found"}
            else:
                # è‡ªåŠ¨æ¨¡å¼
                return await scraper.scrape_comprehensive(
                    owner, repo, include_pages, include_repo
                )


async def discover_github_content(target: str, config: GitHubConfig = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå‘ç°GitHubå†…å®¹æº
    
    Args:
        target: "owner" æˆ– "owner/repo" æ ¼å¼
        config: é…ç½®å¯¹è±¡
        
    Returns:
        å‘ç°ç»“æœ
    """
    async with GitHubContentScraper(config) as scraper:
        parts = target.split("/")
        owner = parts[0]
        repo = parts[1] if len(parts) > 1 else None
        
        return await scraper.discover_github_content(owner, repo)
