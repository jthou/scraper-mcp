"""é‡æ„åçš„GitHub PagesæŠ“å–å™¨ - åŸºäºæ–°çš„æ¨¡å—åŒ–ç»“æ„"""
import asyncio
import re
import json
import aiohttp
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Set
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import html2text

from .config import GitHubConfig, default_config
from .utils import GitHubUtils, AsyncRateLimiter, ContentExtractor


class GitHubPagesScraper:
    """GitHub Pagesæ–‡æ¡£ç«™ç‚¹ä¸“ç”¨æŠ“å–å™¨ - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self, config: GitHubConfig = None):
        """
        åˆå§‹åŒ–GitHub PagesæŠ“å–å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or default_config
        self.session: Optional[aiohttp.ClientSession] = None
        self.visited_urls: Set[str] = set()
        self.extracted_content: List[Dict[str, Any]] = []
        self.rate_limiter = AsyncRateLimiter(100, 3600)  # æ¯å°æ—¶100è¯·æ±‚
        
        # HTMLåˆ°Markdownè½¬æ¢å™¨é…ç½®
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = not self.config.extract_links
        self.h2t.ignore_images = not self.config.extract_images
        self.h2t.ignore_emphasis = False
        self.h2t.body_width = 0  # ä¸æ¢è¡Œ
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.config.output_base_dir / "Pages"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¸¸è§çš„æ–‡æ¡£ç”Ÿæˆå™¨ç‰¹å¾
        self.doc_generators = {
            "jekyll": {
                "indicators": ["jekyll", "_config.yml", ".jekyll-cache", "_site"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*jekyll'
            },
            "docusaurus": {
                "indicators": ["docusaurus", ".docusaurus"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*docusaurus'
            },
            "gatsby": {
                "indicators": ["gatsby", "___gatsby"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*gatsby'
            },
            "next": {
                "indicators": ["next.js", "_next"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*next'
            },
            "vuepress": {
                "indicators": ["vuepress"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*vuepress'
            },
            "gitbook": {
                "indicators": ["gitbook"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*gitbook'
            },
            "mkdocs": {
                "indicators": ["mkdocs"],
                "meta_pattern": r'generator["\s]*content["\s]*=["\s]*mkdocs'
            }
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            headers=self.config.request_headers,
            timeout=aiohttp.ClientTimeout(total=self.config.pages_timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def discover_github_pages(self, owner: str, repo: str = None) -> List[Dict[str, Any]]:
        """
        å‘ç°GitHub Pagesç«™ç‚¹
        
        Args:
            owner: GitHubç”¨æˆ·åæˆ–ç»„ç»‡å
            repo: ä»“åº“åï¼Œå¦‚æœä¸ºNoneåˆ™å‘ç°ç”¨æˆ·çš„ä¸»Pagesç«™ç‚¹
            
        Returns:
            å‘ç°çš„ç«™ç‚¹ä¿¡æ¯åˆ—è¡¨
        """
        print(f"ğŸ” å‘ç°GitHub Pages: {owner}/{repo if repo else 'main-site'}")
        
        if repo:
            sites = await self._discover_single_repo_pages(owner, repo)
        else:
            sites = await self._discover_user_pages(owner)
        
        print(f"ğŸ“‹ å‘ç° {len(sites)} ä¸ªç«™ç‚¹")
        return sites
    
    async def _discover_single_repo_pages(self, owner: str, repo: str) -> List[Dict[str, Any]]:
        """å‘ç°å•ä¸ªä»“åº“çš„Pagesç«™ç‚¹"""
        # ç”Ÿæˆå¯èƒ½çš„URLåˆ—è¡¨
        possible_urls = [
            f"https://{owner}.github.io/{repo}/",
            f"https://{owner}.github.io/" if repo == f"{owner}.github.io" else None,
            f"https://{repo}.netlify.app/",
            f"https://{repo}.vercel.app/",
            f"https://{repo}.surge.sh/"
        ]
        
        # è¿‡æ»¤Noneå€¼
        possible_urls = [url for url in possible_urls if url]
        
        # æ£€æŸ¥è‡ªå®šä¹‰åŸŸå
        try:
            cname_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/CNAME"
            async with self.session.get(cname_url) as response:
                if response.status == 200:
                    cname = (await response.text()).strip()
                    if cname:
                        possible_urls.append(f"https://{cname}/")
        except Exception:
            # å¦‚æœè·å–CNAMEå¤±è´¥ï¼Œç»§ç»­å…¶ä»–æ£€æŸ¥
            pass
        
        # å¹¶å‘éªŒè¯æ‰€æœ‰å¯èƒ½çš„URL
        tasks = [
            self._validate_pages_site(url, owner, repo) 
            for url in possible_urls
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_sites = []
        for result in results:
            if isinstance(result, dict) and result.get("status") == "active":
                valid_sites.append(result)
        
        return valid_sites
    
    async def _discover_user_pages(self, owner: str) -> List[Dict[str, Any]]:
        """å‘ç°ç”¨æˆ·/ç»„ç»‡çš„ä¸»Pagesç«™ç‚¹"""
        main_site_url = f"https://{owner}.github.io/"
        site_info = await self._validate_pages_site(main_site_url, owner, None)
        
        return [site_info] if site_info else []
    
    async def _validate_pages_site(self, url: str, owner: str, repo: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯Pagesç«™ç‚¹æ˜¯å¦å­˜åœ¨å¹¶è·å–åŸºæœ¬ä¿¡æ¯"""
        try:
            await self.rate_limiter.acquire()
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # æ£€æµ‹æ–‡æ¡£ç”Ÿæˆå™¨ç±»å‹
                    generator = self._detect_generator(content, dict(response.headers))
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    title = self._extract_title(content)
                    description = self._extract_description(content)
                    
                    # æ£€æŸ¥å†…å®¹è´¨é‡
                    text_content = ContentExtractor.extract_text_content(content)
                    
                    return {
                        "url": url,
                        "owner": owner,
                        "repo": repo,
                        "status": "active",
                        "generator": generator,
                        "title": title,
                        "description": description,
                        "content_length": len(text_content),
                        "discovered_at": datetime.now().isoformat(),
                        "response_headers": dict(response.headers)
                    }
                elif response.status == 404:
                    return {
                        "url": url,
                        "owner": owner,
                        "repo": repo,
                        "status": "not_found",
                        "discovered_at": datetime.now().isoformat()
                    }
        except Exception as e:
            print(f"âš ï¸ éªŒè¯ç«™ç‚¹å¤±è´¥ {url}: {e}")
            return {
                "url": url,
                "owner": owner,
                "repo": repo,
                "status": "error",
                "error": str(e),
                "discovered_at": datetime.now().isoformat()
            }
        
        return None
    
    def _detect_generator(self, content: str, headers: Dict[str, str]) -> str:
        """æ£€æµ‹æ–‡æ¡£ç”Ÿæˆå™¨ç±»å‹"""
        content_lower = content.lower()
        
        # é¦–å…ˆæ£€æŸ¥metaæ ‡ç­¾
        for generator, config in self.doc_generators.items():
            if re.search(config["meta_pattern"], content_lower, re.IGNORECASE):
                return generator.title()
        
        # ç„¶åæ£€æŸ¥å†…å®¹ç‰¹å¾
        for generator, config in self.doc_generators.items():
            for indicator in config["indicators"]:
                if indicator.lower() in content_lower:
                    return generator.title()
        
        # æ£€æŸ¥æœåŠ¡å™¨å¤´éƒ¨
        server = headers.get('server', '').lower()
        x_served_by = headers.get('x-served-by', '').lower()
        
        if 'github.com' in server or 'github-pages' in server:
            return "GitHub Pages"
        elif 'netlify' in server or 'netlify' in x_served_by:
            return "Netlify"
        elif 'vercel' in server or 'vercel' in x_served_by:
            return "Vercel"
        elif 'surge' in server:
            return "Surge"
        
        return "Unknown"
    
    def _extract_title(self, content: str) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # ä¼˜å…ˆæŸ¥æ‰¾titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            if title:
                return title
        
        # å¤‡é€‰ï¼šæŸ¥æ‰¾h1æ ‡ç­¾
        h1_tag = soup.find('h1')
        if h1_tag:
            h1_text = h1_tag.get_text().strip()
            if h1_text:
                return h1_text
        
        # å¤‡é€‰ï¼šæŸ¥æ‰¾og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        return "Untitled"
    
    def _extract_description(self, content: str) -> str:
        """æå–é¡µé¢æè¿°"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # æŸ¥æ‰¾meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            desc = meta_desc['content'].strip()
            if desc:
                return desc[:200] + "..." if len(desc) > 200 else desc
        
        # å¤‡é€‰ï¼šæŸ¥æ‰¾og:description
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            desc = og_desc['content'].strip()
            if desc:
                return desc[:200] + "..." if len(desc) > 200 else desc
        
        # å¤‡é€‰ï¼šæŸ¥æ‰¾ç¬¬ä¸€ä¸ªpæ ‡ç­¾
        p_tag = soup.find('p')
        if p_tag:
            text = p_tag.get_text().strip()
            if text:
                return text[:200] + "..." if len(text) > 200 else text
        
        return ""
    
    async def scrape_documentation_site(self, base_url: str, max_pages: int = None) -> Dict[str, Any]:
        """
        æŠ“å–å®Œæ•´çš„æ–‡æ¡£ç«™ç‚¹
        
        Args:
            base_url: ç«™ç‚¹åŸºç¡€URL
            max_pages: æœ€å¤§æŠ“å–é¡µé¢æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        max_pages = max_pages or self.config.pages_max_pages
        
        print(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–æ–‡æ¡£ç«™ç‚¹: {base_url}")
        print(f"ğŸ“Š æœ€å¤§é¡µé¢æ•°: {max_pages}")
        
        # é‡ç½®çŠ¶æ€
        self.visited_urls.clear()
        self.extracted_content.clear()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        domain = GitHubUtils.get_url_domain(base_url)
        site_dir = self.output_dir / domain
        site_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # ä½¿ç”¨Playwrightè¿›è¡Œæ·±åº¦æŠ“å–
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.config.pages_headless,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                
                page = await browser.new_page()
                
                # è®¾ç½®ç”¨æˆ·ä»£ç†å’Œå¤´éƒ¨
                await page.set_extra_http_headers(self.config.request_headers)
                
                try:
                    # é¦–å…ˆå‘ç°ç«™ç‚¹åœ°å›¾
                    sitemap_urls = await self._discover_sitemap(page, base_url)
                    
                    if sitemap_urls:
                        print(f"ğŸ“‹ å‘ç°ç«™ç‚¹åœ°å›¾ï¼ŒåŒ…å« {len(sitemap_urls)} ä¸ªé¡µé¢")
                        urls_to_crawl = sitemap_urls[:max_pages]
                    else:
                        print("ğŸ” æœªå‘ç°ç«™ç‚¹åœ°å›¾ï¼Œä½¿ç”¨æ™ºèƒ½çˆ¬å–")
                        urls_to_crawl = await self._intelligent_crawl(page, base_url, max_pages)
                    
                    print(f"ğŸ“ å¼€å§‹æŠ“å– {len(urls_to_crawl)} ä¸ªé¡µé¢")
                    
                    # æŠ“å–æ¯ä¸ªé¡µé¢çš„å†…å®¹
                    for i, url in enumerate(urls_to_crawl, 1):
                        if url not in self.visited_urls:
                            print(f"ğŸ“„ æŠ“å–é¡µé¢ {i}/{len(urls_to_crawl)}: {url}")
                            
                            await self._scrape_single_page(page, url)
                            self.visited_urls.add(url)
                            
                            # é¡µé¢é—´å»¶è¿Ÿ
                            if self.config.pages_delay > 0:
                                await asyncio.sleep(self.config.pages_delay)
                    
                    await browser.close()
                    
                    # ç»„ç»‡å’Œä¿å­˜å†…å®¹
                    result = await self._organize_scraped_content(base_url)
                    
                    if self.config.save_metadata:
                        await self._save_scraped_content(result, site_dir)
                    
                    return result
                    
                except Exception as e:
                    await browser.close()
                    raise e
                    
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "base_url": base_url,
                "scraped_at": datetime.now().isoformat()
            }
    
    async def _discover_sitemap(self, page, base_url: str) -> List[str]:
        """å‘ç°ç«™ç‚¹åœ°å›¾"""
        sitemap_urls = [
            urljoin(base_url, "sitemap.xml"),
            urljoin(base_url, "sitemap_index.xml"),
            urljoin(base_url, "robots.txt")
        ]
        
        all_urls = []
        
        for sitemap_url in sitemap_urls:
            try:
                response = await page.goto(sitemap_url, timeout=self.config.pages_timeout * 1000)
                if response and response.status == 200:
                    content = await page.content()
                    
                    if sitemap_url.endswith('.xml'):
                        # è§£æXML sitemap
                        urls = self._parse_xml_sitemap(content)
                        all_urls.extend(urls)
                    elif sitemap_url.endswith('robots.txt'):
                        # ä»robots.txtä¸­æå–sitemapå¼•ç”¨
                        sitemap_refs = self._extract_sitemaps_from_robots(content)
                        for sitemap_ref in sitemap_refs:
                            referenced_urls = await self._get_urls_from_sitemap(page, sitemap_ref)
                            all_urls.extend(referenced_urls)
                        
            except Exception as e:
                print(f"âš ï¸ è·å–sitemapå¤±è´¥ {sitemap_url}: {e}")
        
        # å»é‡å¹¶è¿‡æ»¤ç›¸å…³URL
        unique_urls = list(set(all_urls))
        filtered_urls = GitHubUtils.filter_urls(unique_urls, base_url)
        
        return filtered_urls
    
    def _parse_xml_sitemap(self, xml_content: str) -> List[str]:
        """è§£æXMLæ ¼å¼çš„sitemap"""
        soup = BeautifulSoup(xml_content, 'xml')
        urls = []
        
        # è§£æstandard sitemap
        for url_tag in soup.find_all('url'):
            loc_tag = url_tag.find('loc')
            if loc_tag:
                urls.append(loc_tag.get_text().strip())
        
        # è§£æsitemap index
        for sitemap_tag in soup.find_all('sitemap'):
            loc_tag = sitemap_tag.find('loc')
            if loc_tag:
                urls.append(loc_tag.get_text().strip())
        
        return urls
    
    def _extract_sitemaps_from_robots(self, robots_content: str) -> List[str]:
        """ä»robots.txtä¸­æå–sitemapå¼•ç”¨"""
        sitemaps = []
        for line in robots_content.splitlines():
            line = line.strip()
            if line.lower().startswith('sitemap:'):
                sitemap_url = line.split(':', 1)[1].strip()
                sitemaps.append(sitemap_url)
        return sitemaps
    
    async def _get_urls_from_sitemap(self, page, sitemap_url: str) -> List[str]:
        """ä»å¼•ç”¨çš„sitemapè·å–URLåˆ—è¡¨"""
        try:
            response = await page.goto(sitemap_url, timeout=self.config.pages_timeout * 1000)
            if response and response.status == 200:
                content = await page.content()
                return self._parse_xml_sitemap(content)
        except Exception as e:
            print(f"âš ï¸ è·å–å¼•ç”¨sitemapå¤±è´¥ {sitemap_url}: {e}")
        
        return []
    
    async def _intelligent_crawl(self, page, base_url: str, max_pages: int) -> List[str]:
        """æ™ºèƒ½çˆ¬å–ï¼ˆå½“æ²¡æœ‰sitemapæ—¶ï¼‰"""
        urls_to_crawl = [base_url]
        discovered_urls = set([base_url])
        
        await page.goto(base_url, timeout=self.config.pages_timeout * 1000)
        await page.wait_for_load_state("networkidle")
        
        # åˆ†æé¡µé¢ç»“æ„ï¼ŒæŸ¥æ‰¾å¯¼èˆªé“¾æ¥
        navigation_links = await self._extract_navigation_links(page, base_url)
        
        for link in navigation_links:
            if link not in discovered_urls and len(discovered_urls) < max_pages:
                discovered_urls.add(link)
                urls_to_crawl.append(link)
        
        return urls_to_crawl
    
    async def _extract_navigation_links(self, page, base_url: str) -> List[str]:
        """æå–å¯¼èˆªé“¾æ¥"""
        links = []
        
        # å¸¸è§çš„å¯¼èˆªé€‰æ‹©å™¨
        nav_selectors = [
            'nav a[href]',
            '.sidebar a[href]',
            '.navigation a[href]',
            '.menu a[href]',
            '.toc a[href]',
            '.table-of-contents a[href]',
            '[role="navigation"] a[href]',
            '.docs-navigation a[href]',
            '.site-nav a[href]'
        ]
        
        for selector in nav_selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    href = await element.get_attribute('href')
                    if href:
                        # è½¬æ¢ä¸ºç»å¯¹URL
                        if href.startswith('/'):
                            full_url = urljoin(base_url, href)
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç›¸å…³URL
                        if GitHubUtils.get_url_domain(full_url) == GitHubUtils.get_url_domain(base_url):
                            links.append(full_url)
                            
            except Exception as e:
                print(f"âš ï¸ æå–å¯¼èˆªé“¾æ¥å¤±è´¥ {selector}: {e}")
        
        return list(set(links))
    
    async def _scrape_single_page(self, page, url: str) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªé¡µé¢å†…å®¹"""
        try:
            response = await page.goto(url, 
                                     wait_until="networkidle", 
                                     timeout=self.config.pages_timeout * 1000)
            
            if not response or response.status != 200:
                error_data = {
                    "status": "error", 
                    "url": url, 
                    "error": f"HTTP {response.status if response else 'No response'}",
                    "extracted_at": datetime.now().isoformat()
                }
                self.extracted_content.append(error_data)
                return error_data
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_load_state("networkidle")
            
            # æå–é¡µé¢å†…å®¹
            content_data = await self._extract_page_content(page, url)
            self.extracted_content.append(content_data)
            
            return content_data
            
        except Exception as e:
            error_data = {
                "status": "error", 
                "url": url, 
                "error": str(e),
                "extracted_at": datetime.now().isoformat()
            }
            self.extracted_content.append(error_data)
            return error_data
    
    async def _extract_page_content(self, page, url: str) -> Dict[str, Any]:
        """æå–é¡µé¢çš„æ ¸å¿ƒå†…å®¹"""
        try:
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            title = await page.title()
            
            # æ™ºèƒ½é€‰æ‹©å†…å®¹åŒºåŸŸ
            content_html = await self._extract_main_content(page)
            
            # è½¬æ¢ä¸ºMarkdown
            if self.config.convert_to_markdown:
                markdown_content = self.h2t.handle(content_html)
            else:
                markdown_content = content_html
            
            # æå–å…ƒæ•°æ®
            metadata = await self._extract_page_metadata(page)
            
            # æå–é“¾æ¥å’Œå›¾ç‰‡
            page_url = page.url
            if self.config.extract_links:
                links = ContentExtractor.extract_links(content_html, page_url)
                metadata['links'] = links[:20]  # é™åˆ¶æ•°é‡
            
            if self.config.extract_images:
                images = ContentExtractor.extract_images(content_html, page_url)
                metadata['images'] = images[:10]  # é™åˆ¶æ•°é‡
            
            return {
                "status": "success",
                "url": url,
                "title": title,
                "content": markdown_content,
                "metadata": metadata,
                "extracted_at": datetime.now().isoformat(),
                "content_length": len(markdown_content),
                "word_count": len(markdown_content.split()) if markdown_content else 0
            }
            
        except Exception as e:
            return {
                "status": "error", 
                "url": url, 
                "error": str(e),
                "extracted_at": datetime.now().isoformat()
            }
    
    async def _extract_main_content(self, page) -> str:
        """æ™ºèƒ½æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # å¸¸è§çš„å†…å®¹é€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        content_selectors = [
            'main',
            'article',
            '.content',
            '.main-content',
            '.post-content',
            '.entry-content',
            '.page-content',
            '.documentation',
            '.docs-content',
            '.markdown-body',
            '#content',
            '.container .content',
            '.container main',
            'body'  # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        ]
        
        for selector in content_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    # ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ã€é¡µè„šç­‰æ— å…³å†…å®¹
                    await self._clean_content_element(page, element)
                    content_html = await element.inner_html()
                    
                    # æ£€æŸ¥å†…å®¹è´¨é‡
                    if self._is_quality_content(content_html):
                        return content_html
            except Exception:
                continue
        
        # å¦‚æœæ‰€æœ‰é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºå†…å®¹
        return ""
    
    async def _clean_content_element(self, page, element):
        """æ¸…ç†å†…å®¹å…ƒç´ ï¼Œç§»é™¤æ— å…³éƒ¨åˆ†"""
        # è¦ç§»é™¤çš„é€‰æ‹©å™¨
        remove_selectors = [
            'nav', '.navigation', '.navbar', '.nav',
            '.sidebar', '.side-nav', '.menu',
            'header', 'footer', '.header', '.footer',
            '.advertisement', '.ads', '.ad',
            '.social-share', '.social-sharing',
            '.comments', '.comment', '.disqus',
            'script', 'style', 'noscript',
            '.breadcrumb', '.breadcrumbs',
            '.edit-page', '.edit-link',
            '.prev-next', '.pagination'
        ]
        
        for selector in remove_selectors:
            try:
                elements = await element.query_selector_all(selector)
                for el in elements:
                    await el.evaluate('el => el.remove()')
            except Exception:
                continue
    
    def _is_quality_content(self, html_content: str) -> bool:
        """åˆ¤æ–­å†…å®¹è´¨é‡"""
        if not html_content or len(html_content.strip()) < 100:
            return False
        
        # è½¬æ¢ä¸ºæ–‡æœ¬æ£€æŸ¥
        text_content = ContentExtractor.extract_text_content(html_content)
        
        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(text_content) < 50:  # å¤ªçŸ­
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
        words = text_content.split()
        if len(words) < 10:  # è¯è¯­å¤ªå°‘
            return False
        
        return True
    
    async def _extract_page_metadata(self, page) -> Dict[str, Any]:
        """æå–é¡µé¢å…ƒæ•°æ®"""
        metadata = {}
        
        try:
            # æå–metaæ ‡ç­¾
            meta_tags = await page.query_selector_all('meta')
            for meta in meta_tags:
                name = await meta.get_attribute('name')
                property_attr = await meta.get_attribute('property')
                content = await meta.get_attribute('content')
                
                if (name or property_attr) and content:
                    key = name or property_attr
                    metadata[key] = content
            
            # æå–æ ‡é¢˜å±‚æ¬¡ç»“æ„
            headings = []
            for i in range(1, 7):  # h1-h6
                heading_elements = await page.query_selector_all(f'h{i}')
                for heading in heading_elements[:10]:  # é™åˆ¶æ•°é‡
                    text = await heading.inner_text()
                    if text.strip():
                        headings.append({
                            "level": i,
                            "text": text.strip()
                        })
            
            metadata['headings'] = headings
            
        except Exception as e:
            metadata['extraction_error'] = str(e)
        
        return metadata
    
    async def _organize_scraped_content(self, base_url: str) -> Dict[str, Any]:
        """ç»„ç»‡æŠ“å–çš„å†…å®¹"""
        successful_pages = [page for page in self.extracted_content if page.get("status") == "success"]
        failed_pages = [page for page in self.extracted_content if page.get("status") == "error"]
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_words = sum(page.get("word_count", 0) for page in successful_pages)
        total_pages = len(successful_pages)
        
        # æ„å»ºå†…å®¹å±‚æ¬¡ç»“æ„
        content_structure = self._build_content_structure(successful_pages, base_url)
        
        return {
            "status": "success",
            "base_url": base_url,
            "scrape_summary": {
                "total_pages_found": len(self.extracted_content),
                "successful_pages": total_pages,
                "failed_pages": len(failed_pages),
                "total_words": total_words,
                "scraped_at": datetime.now().isoformat(),
                "config_used": self.config.to_dict()
            },
            "content_structure": content_structure,
            "pages": successful_pages,
            "errors": failed_pages if self.config.save_errors else []
        }
    
    def _build_content_structure(self, pages: List[Dict], base_url: str) -> Dict[str, Any]:
        """æ„å»ºå†…å®¹å±‚æ¬¡ç»“æ„"""
        structure = {"pages": []}
        
        for page in pages:
            url = page["url"]
            relative_path = url.replace(base_url, "").strip("/")
            
            structure["pages"].append({
                "path": relative_path,
                "title": page["title"],
                "url": url,
                "word_count": page.get("word_count", 0),
                "content_length": page.get("content_length", 0)
            })
        
        # æŒ‰è·¯å¾„æ’åº
        structure["pages"].sort(key=lambda x: x["path"])
        
        return structure
    
    async def _save_scraped_content(self, result: Dict[str, Any], site_dir: Path):
        """ä¿å­˜æŠ“å–çš„å†…å®¹"""
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = site_dir / "metadata.json"
        GitHubUtils.save_json({
            "base_url": result["base_url"],
            "scrape_summary": result["scrape_summary"],
            "content_structure": result["content_structure"]
        }, metadata_file)
        
        # ä¿å­˜æ¯ä¸ªé¡µé¢çš„å†…å®¹
        pages_dir = site_dir / "pages"
        pages_dir.mkdir(exist_ok=True)
        
        for i, page in enumerate(result["pages"]):
            if page.get("status") == "success":
                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                safe_title = GitHubUtils.sanitize_filename(page["title"], 50)
                filename = f"{i+1:03d}_{safe_title}.md"
                
                page_file = pages_dir / filename
                
                # æ„å»ºé¡µé¢å†…å®¹
                page_content = f"# {page['title']}\n\n"
                page_content += f"**URL**: {page['url']}\n"
                page_content += f"**æŠ“å–æ—¶é—´**: {page['extracted_at']}\n"
                page_content += f"**å­—æ•°**: {page.get('word_count', 0)}\n\n"
                page_content += "---\n\n"
                page_content += page["content"]
                
                # ä¿å­˜é¡µé¢å†…å®¹
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(page_content)
        
        # ä¿å­˜é”™è¯¯æŠ¥å‘Š
        if result.get("errors") and self.config.save_errors:
            errors_file = site_dir / "errors.json"
            GitHubUtils.save_json(result["errors"], errors_file)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report(result, site_dir)
        
        print(f"ğŸ“ å†…å®¹å·²ä¿å­˜åˆ°: {site_dir}")
        
        return {"status": "success", "output_dir": str(site_dir)}
    
    def _generate_summary_report(self, result: Dict[str, Any], site_dir: Path):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        summary_file = site_dir / "README.md"
        
        base_url = result["base_url"]
        domain = GitHubUtils.get_url_domain(base_url)
        summary = result["scrape_summary"]
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# {domain} æ–‡æ¡£æŠ“å–æŠ¥å‘Š\n\n")
            f.write(f"**ç«™ç‚¹URL**: {base_url}\n")
            f.write(f"**æŠ“å–æ—¶é—´**: {summary['scraped_at']}\n")
            f.write(f"**æˆåŠŸé¡µé¢**: {summary['successful_pages']}\n")
            f.write(f"**å¤±è´¥é¡µé¢**: {summary['failed_pages']}\n")
            f.write(f"**æ€»å­—æ•°**: {summary['total_words']:,}\n\n")
            
            f.write("## é…ç½®ä¿¡æ¯\n\n")
            config = summary.get('config_used', {})
            f.write(f"- æœ€å¤§é¡µé¢æ•°: {config.get('pages_max_pages', 'N/A')}\n")
            f.write(f"- é¡µé¢å»¶è¿Ÿ: {config.get('pages_delay', 'N/A')}ç§’\n")
            f.write(f"- è½¬æ¢ä¸ºMarkdown: {config.get('convert_to_markdown', 'N/A')}\n")
            f.write(f"- æå–é“¾æ¥: {config.get('extract_links', 'N/A')}\n")
            f.write(f"- æå–å›¾ç‰‡: {config.get('extract_images', 'N/A')}\n\n")
            
            f.write("## é¡µé¢åˆ—è¡¨\n\n")
            for page_info in result["content_structure"]["pages"]:
                f.write(f"- [{page_info['title']}]({page_info['url']}) ({page_info['word_count']} words)\n")
            
            if result.get("errors"):
                f.write(f"\n## é”™è¯¯é¡µé¢\n\n")
                f.write(f"å…± {len(result['errors'])} ä¸ªé¡µé¢æŠ“å–å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ `errors.json`\n")


# ä¾¿æ·å‡½æ•°
async def scrape_github_pages(url: str, max_pages: int = 100, config: GitHubConfig = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæŠ“å–GitHub Pagesç«™ç‚¹
    
    Args:
        url: ç«™ç‚¹URL
        max_pages: æœ€å¤§æŠ“å–é¡µé¢æ•°
        config: é…ç½®å¯¹è±¡
        
    Returns:
        æŠ“å–ç»“æœ
    """
    async with GitHubPagesScraper(config) as scraper:
        return await scraper.scrape_documentation_site(url, max_pages)


async def discover_github_pages(owner: str, repo: str = None, config: GitHubConfig = None) -> List[Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå‘ç°GitHub Pagesç«™ç‚¹
    
    Args:
        owner: GitHubç”¨æˆ·åæˆ–ç»„ç»‡å
        repo: ä»“åº“å
        config: é…ç½®å¯¹è±¡
        
    Returns:
        å‘ç°çš„ç«™ç‚¹åˆ—è¡¨
    """
    async with GitHubPagesScraper(config) as scraper:
        return await scraper.discover_github_pages(owner, repo)
