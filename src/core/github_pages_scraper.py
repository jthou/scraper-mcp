"""GitHub Pages æ–‡æ¡£ç«™ç‚¹æŠ“å–å™¨"""
import asyncio
import re
import json
import aiohttp
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Set
from urllib.parse import urljoin, urlparse, parse_qs
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import html2text


class GitHubPagesScraper:
    """GitHub Pagesæ–‡æ¡£ç«™ç‚¹ä¸“ç”¨æŠ“å–å™¨"""
    
    def __init__(self, output_dir: Path = None):
        self.output_dir = output_dir or Path("K-Vault/GitHub-Pages")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.session = None
        self.visited_urls: Set[str] = set()
        self.extracted_content: List[Dict[str, Any]] = []
        
        # HTMLåˆ°Markdownè½¬æ¢å™¨é…ç½®
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.ignore_emphasis = False
        self.h2t.body_width = 0  # ä¸æ¢è¡Œ
        
        # å¸¸è§çš„æ–‡æ¡£ç”Ÿæˆå™¨ç‰¹å¾
        self.doc_generators = {
            "jekyll": ["_config.yml", ".jekyll-cache", "_site"],
            "docusaurus": ["docusaurus.config.js", ".docusaurus"],
            "gitbook": ["book.json", "_book"],
            "vuepress": [".vuepress", "vuepress.config.js"],
            "mkdocs": ["mkdocs.yml", "site"],
            "gatsby": ["gatsby-config.js", ".gatsby"],
            "next": ["next.config.js", ".next"],
            "nuxt": ["nuxt.config.js", ".nuxt"]
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def discover_github_pages(self, owner: str, repo: str = None) -> List[Dict[str, Any]]:
        """å‘ç°GitHub Pagesç«™ç‚¹"""
        discovered_sites = []
        
        if repo:
            # å•ä¸ªä»“åº“çš„Pages
            sites = await self._discover_single_repo_pages(owner, repo)
            discovered_sites.extend(sites)
        else:
            # ç”¨æˆ·/ç»„ç»‡çš„æ‰€æœ‰Pages
            sites = await self._discover_user_pages(owner)
            discovered_sites.extend(sites)
        
        return discovered_sites
    
    async def _discover_single_repo_pages(self, owner: str, repo: str) -> List[Dict[str, Any]]:
        """å‘ç°å•ä¸ªä»“åº“çš„Pagesç«™ç‚¹"""
        possible_urls = [
            f"https://{owner}.github.io/{repo}/",
            f"https://{owner}.github.io/",  # å¦‚æœrepoåæ˜¯username.github.io
            f"https://{repo}.netlify.app/",
            f"https://{repo}.vercel.app/",
            f"https://{repo}.surge.sh/"
        ]
        
        # æ£€æŸ¥è‡ªå®šä¹‰åŸŸå
        try:
            cname_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/CNAME"
            async with self.session.get(cname_url) as response:
                if response.status == 200:
                    cname = (await response.text()).strip()
                    possible_urls.append(f"https://{cname}/")
        except:
            pass
        
        valid_sites = []
        for url in possible_urls:
            site_info = await self._validate_pages_site(url, owner, repo)
            if site_info:
                valid_sites.append(site_info)
        
        return valid_sites
    
    async def _discover_user_pages(self, owner: str) -> List[Dict[str, Any]]:
        """å‘ç°ç”¨æˆ·/ç»„ç»‡çš„æ‰€æœ‰Pagesç«™ç‚¹"""
        # é€šè¿‡GitHub APIè·å–ç”¨æˆ·çš„ä»“åº“åˆ—è¡¨
        # è¿™é‡Œå…ˆå®ç°åŸºç¡€ç‰ˆæœ¬ï¼Œåç»­å¯ä»¥é›†æˆGitHub API
        main_site_url = f"https://{owner}.github.io/"
        site_info = await self._validate_pages_site(main_site_url, owner, None)
        
        return [site_info] if site_info else []
    
    async def _validate_pages_site(self, url: str, owner: str, repo: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯Pagesç«™ç‚¹æ˜¯å¦å­˜åœ¨å¹¶è·å–åŸºæœ¬ä¿¡æ¯"""
        try:
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # æ£€æµ‹æ–‡æ¡£ç”Ÿæˆå™¨ç±»å‹
                    generator = self._detect_generator(content, response.headers)
                    
                    return {
                        "url": url,
                        "owner": owner,
                        "repo": repo,
                        "status": "active",
                        "generator": generator,
                        "title": self._extract_title(content),
                        "description": self._extract_description(content),
                        "discovered_at": datetime.now().isoformat()
                    }
        except Exception as e:
            print(f"éªŒè¯ç«™ç‚¹å¤±è´¥ {url}: {e}")
        
        return None
    
    def _detect_generator(self, content: str, headers: Dict) -> str:
        """æ£€æµ‹æ–‡æ¡£ç”Ÿæˆå™¨ç±»å‹"""
        content_lower = content.lower()
        
        # é€šè¿‡metaæ ‡ç­¾æ£€æµ‹
        if 'generator" content="jekyll' in content_lower:
            return "Jekyll"
        elif 'docusaurus' in content_lower:
            return "Docusaurus"
        elif 'gitbook' in content_lower:
            return "GitBook"
        elif 'vuepress' in content_lower:
            return "VuePress"
        elif 'mkdocs' in content_lower:
            return "MkDocs"
        elif 'gatsby' in content_lower:
            return "Gatsby"
        elif 'next.js' in content_lower or 'nextjs' in content_lower:
            return "Next.js"
        elif 'nuxt' in content_lower:
            return "Nuxt.js"
        
        # é€šè¿‡æœåŠ¡å™¨å¤´éƒ¨æ£€æµ‹
        server = headers.get('server', '').lower()
        if 'github.com' in server:
            return "GitHub Pages"
        elif 'netlify' in server:
            return "Netlify"
        elif 'vercel' in server:
            return "Vercel"
        
        return "Unknown"
    
    def _extract_title(self, content: str) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        soup = BeautifulSoup(content, 'html.parser')
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾h1æ ‡ç­¾
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
        
        return "Untitled"
    
    def _extract_description(self, content: str) -> str:
        """æå–é¡µé¢æè¿°"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # æŸ¥æ‰¾meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾ç¬¬ä¸€ä¸ªpæ ‡ç­¾
        p_tag = soup.find('p')
        if p_tag:
            text = p_tag.get_text().strip()
            return text[:200] + "..." if len(text) > 200 else text
        
        return ""
    
    async def scrape_documentation_site(self, base_url: str, max_pages: int = 100) -> Dict[str, Any]:
        """æŠ“å–å®Œæ•´çš„æ–‡æ¡£ç«™ç‚¹"""
        print(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–æ–‡æ¡£ç«™ç‚¹: {base_url}")
        
        self.visited_urls.clear()
        self.extracted_content.clear()
        
        # ä½¿ç”¨Playwrightè¿›è¡Œæ·±åº¦æŠ“å–
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            # è®¾ç½®ç”¨æˆ·ä»£ç†
            await page.set_extra_http_headers({
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            })
            
            try:
                # é¦–å…ˆå‘ç°ç«™ç‚¹åœ°å›¾
                sitemap_urls = await self._discover_sitemap(page, base_url)
                
                if sitemap_urls:
                    print(f"ğŸ“‹ å‘ç°ç«™ç‚¹åœ°å›¾ï¼ŒåŒ…å« {len(sitemap_urls)} ä¸ªé¡µé¢")
                    urls_to_crawl = sitemap_urls[:max_pages]
                else:
                    print("ğŸ” æœªå‘ç°ç«™ç‚¹åœ°å›¾ï¼Œä½¿ç”¨æ™ºèƒ½çˆ¬å–")
                    urls_to_crawl = await self._intelligent_crawl(page, base_url, max_pages)
                
                # æŠ“å–æ¯ä¸ªé¡µé¢çš„å†…å®¹
                for i, url in enumerate(urls_to_crawl, 1):
                    if url not in self.visited_urls:
                        print(f"ğŸ“„ æŠ“å–é¡µé¢ {i}/{len(urls_to_crawl)}: {url}")
                        await self._scrape_single_page(page, url)
                        self.visited_urls.add(url)
                        
                        # é¿å…è¿‡å¿«è¯·æ±‚
                        await asyncio.sleep(0.5)
                
                await browser.close()
                
                # ç»„ç»‡å’Œä¿å­˜å†…å®¹
                result = await self._organize_scraped_content(base_url)
                await self._save_scraped_content(result)
                
                return result
                
            except Exception as e:
                await browser.close()
                print(f"âŒ æŠ“å–å¤±è´¥: {e}")
                return {"status": "error", "error": str(e)}
    
    async def _discover_sitemap(self, page, base_url: str) -> List[str]:
        """å‘ç°ç«™ç‚¹åœ°å›¾"""
        sitemap_urls = [
            urljoin(base_url, "sitemap.xml"),
            urljoin(base_url, "sitemap_index.xml"),
            urljoin(base_url, "robots.txt")
        ]
        
        all_urls = []
        
        for sitemap_url in sitemap_urls:
            try:
                response = await page.goto(sitemap_url)
                if response and response.status == 200:
                    content = await page.content()
                    
                    if sitemap_url.endswith('.xml'):
                        # è§£æXML sitemap
                        urls = self._parse_xml_sitemap(content)
                        all_urls.extend(urls)
                    elif sitemap_url.endswith('robots.txt'):
                        # ä»robots.txtä¸­æå–sitemap
                        lines = content.split('\n')
                        for line in lines:
                            if line.lower().startswith('sitemap:'):
                                sitemap_ref = line.split(':', 1)[1].strip()
                                # é€’å½’è·å–å¼•ç”¨çš„sitemap
                                referenced_urls = await self._get_urls_from_sitemap(page, sitemap_ref)
                                all_urls.extend(referenced_urls)
                        
            except Exception as e:
                print(f"è·å–sitemapå¤±è´¥ {sitemap_url}: {e}")
        
        # å»é‡å¹¶è¿‡æ»¤ç›¸å…³URL
        unique_urls = list(set(all_urls))
        filtered_urls = [url for url in unique_urls if self._is_relevant_url(url, base_url)]
        
        return filtered_urls
    
    def _parse_xml_sitemap(self, xml_content: str) -> List[str]:
        """è§£æXMLæ ¼å¼çš„sitemap"""
        soup = BeautifulSoup(xml_content, 'xml')
        urls = []
        
        # è§£æstandard sitemap
        for url_tag in soup.find_all('url'):
            loc_tag = url_tag.find('loc')
            if loc_tag:
                urls.append(loc_tag.get_text().strip())
        
        # è§£æsitemap index
        for sitemap_tag in soup.find_all('sitemap'):
            loc_tag = sitemap_tag.find('loc')
            if loc_tag:
                urls.append(loc_tag.get_text().strip())
        
        return urls
    
    async def _get_urls_from_sitemap(self, page, sitemap_url: str) -> List[str]:
        """ä»å¼•ç”¨çš„sitemapè·å–URLåˆ—è¡¨"""
        try:
            response = await page.goto(sitemap_url)
            if response and response.status == 200:
                content = await page.content()
                return self._parse_xml_sitemap(content)
        except Exception as e:
            print(f"è·å–å¼•ç”¨sitemapå¤±è´¥ {sitemap_url}: {e}")
        
        return []
    
    def _is_relevant_url(self, url: str, base_url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦ä¸æ–‡æ¡£ç›¸å…³"""
        base_domain = urlparse(base_url).netloc
        url_domain = urlparse(url).netloc
        
        # å¿…é¡»æ˜¯åŒä¸€åŸŸå
        if base_domain != url_domain:
            return False
        
        # æ’é™¤éæ–‡æ¡£URL
        excluded_patterns = [
            r'/api/v\d+/',  # API endpoints
            r'\.(css|js|png|jpg|jpeg|gif|svg|ico|woff|woff2|ttf)$',  # é™æ€èµ„æº
            r'/admin/',     # ç®¡ç†é¡µé¢
            r'/login',      # ç™»å½•é¡µé¢
            r'/logout',     # ç™»å‡ºé¡µé¢
            r'#',           # é”šç‚¹é“¾æ¥
        ]
        
        for pattern in excluded_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        return True
    
    async def _intelligent_crawl(self, page, base_url: str, max_pages: int) -> List[str]:
        """æ™ºèƒ½çˆ¬å–ï¼ˆå½“æ²¡æœ‰sitemapæ—¶ï¼‰"""
        urls_to_crawl = [base_url]
        discovered_urls = set([base_url])
        
        await page.goto(base_url)
        
        # åˆ†æé¡µé¢ç»“æ„ï¼ŒæŸ¥æ‰¾å¯¼èˆªé“¾æ¥
        navigation_links = await self._extract_navigation_links(page, base_url)
        
        for link in navigation_links:
            if link not in discovered_urls and len(discovered_urls) < max_pages:
                discovered_urls.add(link)
                urls_to_crawl.append(link)
        
        return urls_to_crawl
    
    async def _extract_navigation_links(self, page, base_url: str) -> List[str]:
        """æå–å¯¼èˆªé“¾æ¥"""
        links = []
        
        # å¸¸è§çš„å¯¼èˆªé€‰æ‹©å™¨
        nav_selectors = [
            'nav a',
            '.sidebar a',
            '.navigation a',
            '.menu a',
            '.toc a',
            '.table-of-contents a',
            '[role="navigation"] a'
        ]
        
        for selector in nav_selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    href = await element.get_attribute('href')
                    if href:
                        full_url = urljoin(base_url, href)
                        if self._is_relevant_url(full_url, base_url):
                            links.append(full_url)
            except Exception as e:
                print(f"æå–å¯¼èˆªé“¾æ¥å¤±è´¥ {selector}: {e}")
        
        return list(set(links))
    
    async def _scrape_single_page(self, page, url: str) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªé¡µé¢å†…å®¹"""
        try:
            response = await page.goto(url, wait_until="networkidle")
            if not response or response.status != 200:
                return {"status": "error", "url": url, "error": f"HTTP {response.status if response else 'No response'}"}
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_load_state("networkidle")
            
            # æå–é¡µé¢å†…å®¹
            content_data = await self._extract_page_content(page, url)
            self.extracted_content.append(content_data)
            
            return content_data
            
        except Exception as e:
            error_data = {"status": "error", "url": url, "error": str(e)}
            self.extracted_content.append(error_data)
            return error_data
    
    async def _extract_page_content(self, page, url: str) -> Dict[str, Any]:
        """æå–é¡µé¢çš„æ ¸å¿ƒå†…å®¹"""
        try:
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            title = await page.title()
            
            # æ™ºèƒ½é€‰æ‹©å†…å®¹åŒºåŸŸ
            content_html = await self._extract_main_content(page)
            
            # è½¬æ¢ä¸ºMarkdown
            markdown_content = self.h2t.handle(content_html)
            
            # æå–å…ƒæ•°æ®
            metadata = await self._extract_page_metadata(page)
            
            return {
                "status": "success",
                "url": url,
                "title": title,
                "content": markdown_content,
                "metadata": metadata,
                "extracted_at": datetime.now().isoformat(),
                "content_length": len(markdown_content),
                "word_count": len(markdown_content.split())
            }
            
        except Exception as e:
            return {
                "status": "error", 
                "url": url, 
                "error": str(e),
                "extracted_at": datetime.now().isoformat()
            }
    
    async def _extract_main_content(self, page) -> str:
        """æ™ºèƒ½æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # å¸¸è§çš„å†…å®¹é€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        content_selectors = [
            'main',
            'article',
            '.content',
            '.main-content',
            '.post-content',
            '.entry-content',
            '.page-content',
            '.documentation',
            '.docs-content',
            '#content',
            '.container .row .col',  # Bootstrapå¸ƒå±€
            'body'  # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        ]
        
        for selector in content_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    # ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ã€é¡µè„šç­‰æ— å…³å†…å®¹
                    await self._clean_content_element(page, element)
                    content_html = await element.inner_html()
                    
                    # æ£€æŸ¥å†…å®¹è´¨é‡
                    if self._is_quality_content(content_html):
                        return content_html
            except Exception:
                continue
        
        # å¦‚æœæ‰€æœ‰é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œè¿”å›bodyå†…å®¹
        try:
            body = await page.query_selector('body')
            if body:
                await self._clean_content_element(page, body)
                return await body.inner_html()
        except Exception:
            pass
        
        return ""
    
    async def _clean_content_element(self, page, element):
        """æ¸…ç†å†…å®¹å…ƒç´ ï¼Œç§»é™¤æ— å…³éƒ¨åˆ†"""
        # è¦ç§»é™¤çš„é€‰æ‹©å™¨
        remove_selectors = [
            'nav',
            '.navigation',
            '.navbar',
            '.sidebar',
            '.menu',
            'header',
            'footer',
            '.header',
            '.footer',
            '.advertisement',
            '.ads',
            '.social-share',
            '.comments',
            '.comment',
            'script',
            'style',
            '.breadcrumb'
        ]
        
        for selector in remove_selectors:
            try:
                elements = await element.query_selector_all(selector)
                for el in elements:
                    await el.evaluate('el => el.remove()')
            except Exception:
                continue
    
    def _is_quality_content(self, html_content: str) -> bool:
        """åˆ¤æ–­å†…å®¹è´¨é‡"""
        if not html_content or len(html_content.strip()) < 100:
            return False
        
        # è½¬æ¢ä¸ºæ–‡æœ¬æ£€æŸ¥
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text().strip()
        
        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(text) < 50:  # å¤ªçŸ­
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
        words = text.split()
        if len(words) < 10:  # è¯è¯­å¤ªå°‘
            return False
        
        return True
    
    async def _extract_page_metadata(self, page) -> Dict[str, Any]:
        """æå–é¡µé¢å…ƒæ•°æ®"""
        metadata = {}
        
        try:
            # æå–metaæ ‡ç­¾
            meta_tags = await page.query_selector_all('meta')
            for meta in meta_tags:
                name = await meta.get_attribute('name')
                property_attr = await meta.get_attribute('property')
                content = await meta.get_attribute('content')
                
                if name and content:
                    metadata[name] = content
                elif property_attr and content:
                    metadata[property_attr] = content
            
            # æå–æ ‡é¢˜å±‚æ¬¡ç»“æ„
            headings = []
            for i in range(1, 7):  # h1-h6
                heading_elements = await page.query_selector_all(f'h{i}')
                for heading in heading_elements:
                    text = await heading.inner_text()
                    if text.strip():
                        headings.append({
                            "level": i,
                            "text": text.strip()
                        })
            
            metadata['headings'] = headings
            
            # æå–é“¾æ¥ä¿¡æ¯
            links = []
            link_elements = await page.query_selector_all('a[href]')
            for link in link_elements[:20]:  # é™åˆ¶æ•°é‡
                href = await link.get_attribute('href')
                text = await link.inner_text()
                if href and text.strip():
                    links.append({
                        "href": href,
                        "text": text.strip()
                    })
            
            metadata['links'] = links
            
        except Exception as e:
            metadata['extraction_error'] = str(e)
        
        return metadata
    
    async def _organize_scraped_content(self, base_url: str) -> Dict[str, Any]:
        """ç»„ç»‡æŠ“å–çš„å†…å®¹"""
        successful_pages = [page for page in self.extracted_content if page.get("status") == "success"]
        failed_pages = [page for page in self.extracted_content if page.get("status") == "error"]
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_words = sum(page.get("word_count", 0) for page in successful_pages)
        total_pages = len(successful_pages)
        
        # æ„å»ºå†…å®¹å±‚æ¬¡ç»“æ„
        content_structure = self._build_content_structure(successful_pages, base_url)
        
        return {
            "base_url": base_url,
            "scrape_summary": {
                "total_pages_found": len(self.extracted_content),
                "successful_pages": total_pages,
                "failed_pages": len(failed_pages),
                "total_words": total_words,
                "scraped_at": datetime.now().isoformat()
            },
            "content_structure": content_structure,
            "pages": successful_pages,
            "errors": failed_pages
        }
    
    def _build_content_structure(self, pages: List[Dict], base_url: str) -> Dict[str, Any]:
        """æ„å»ºå†…å®¹å±‚æ¬¡ç»“æ„"""
        # ç®€å•çš„URL-basedç»“æ„
        structure = {"pages": []}
        
        for page in pages:
            url = page["url"]
            relative_path = url.replace(base_url, "").strip("/")
            
            structure["pages"].append({
                "path": relative_path,
                "title": page["title"],
                "url": url,
                "word_count": page.get("word_count", 0)
            })
        
        # æŒ‰è·¯å¾„æ’åº
        structure["pages"].sort(key=lambda x: x["path"])
        
        return structure
    
    async def _save_scraped_content(self, result: Dict[str, Any]):
        """ä¿å­˜æŠ“å–çš„å†…å®¹"""
        base_url = result["base_url"]
        domain = urlparse(base_url).netloc
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        site_dir = self.output_dir / domain
        site_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = site_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump({
                "base_url": result["base_url"],
                "scrape_summary": result["scrape_summary"],
                "content_structure": result["content_structure"]
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ¯ä¸ªé¡µé¢çš„å†…å®¹
        pages_dir = site_dir / "pages"
        pages_dir.mkdir(exist_ok=True)
        
        for i, page in enumerate(result["pages"]):
            if page.get("status") == "success":
                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                safe_title = re.sub(r'[^\w\s-]', '', page["title"])[:50]
                filename = f"{i+1:03d}_{safe_title}.md"
                
                page_file = pages_dir / filename
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {page['title']}\n\n")
                    f.write(f"**URL**: {page['url']}\n")
                    f.write(f"**æŠ“å–æ—¶é—´**: {page['extracted_at']}\n")
                    f.write(f"**å­—æ•°**: {page.get('word_count', 0)}\n\n")
                    f.write("---\n\n")
                    f.write(page["content"])
        
        # ä¿å­˜é”™è¯¯æŠ¥å‘Š
        if result.get("errors"):
            errors_file = site_dir / "errors.json"
            with open(errors_file, 'w', encoding='utf-8') as f:
                json.dump(result["errors"], f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary_file = site_dir / "README.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# {domain} æ–‡æ¡£æŠ“å–æŠ¥å‘Š\n\n")
            f.write(f"**ç«™ç‚¹URL**: {base_url}\n")
            f.write(f"**æŠ“å–æ—¶é—´**: {result['scrape_summary']['scraped_at']}\n")
            f.write(f"**æˆåŠŸé¡µé¢**: {result['scrape_summary']['successful_pages']}\n")
            f.write(f"**å¤±è´¥é¡µé¢**: {result['scrape_summary']['failed_pages']}\n")
            f.write(f"**æ€»å­—æ•°**: {result['scrape_summary']['total_words']:,}\n\n")
            
            f.write("## é¡µé¢åˆ—è¡¨\n\n")
            for page_info in result["content_structure"]["pages"]:
                f.write(f"- [{page_info['title']}]({page_info['url']}) ({page_info['word_count']} words)\n")
        
        print(f"ğŸ“ å†…å®¹å·²ä¿å­˜åˆ°: {site_dir}")
        
        return {"status": "success", "output_dir": str(site_dir)}


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
async def test_github_pages_scraper():
    """æµ‹è¯•GitHubPagesScraper"""
    async with GitHubPagesScraper() as scraper:
        # æµ‹è¯•å‘ç°GitHub Pages
        print("ğŸ” æµ‹è¯•å‘ç°GitHub Pages...")
        sites = await scraper.discover_github_pages("facebook", "react")
        print(f"å‘ç°çš„ç«™ç‚¹: {sites}")
        
        # æµ‹è¯•æŠ“å–æ–‡æ¡£ç«™ç‚¹
        if sites:
            print(f"ğŸ“š æµ‹è¯•æŠ“å–æ–‡æ¡£ç«™ç‚¹...")
            result = await scraper.scrape_documentation_site(sites[0]["url"], max_pages=5)
            print(f"æŠ“å–ç»“æœ: {result['scrape_summary']}")


if __name__ == "__main__":
    asyncio.run(test_github_pages_scraper())
