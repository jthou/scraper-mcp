#!/usr/bin/env python3
"""
ArXivæ–‡çŒ®æœç´¢ä¸ä¸‹è½½æ¨¡å—

åŸºäºç°æœ‰é¡¹ç›®æ¶æ„ï¼Œæä¾›ArXivå­¦æœ¯æ–‡çŒ®çš„æœç´¢ã€ä¸‹è½½å’Œç®¡ç†åŠŸèƒ½
æ”¯æŒï¼š
- ArXiv APIæœç´¢ï¼ˆæ— éœ€æµè§ˆå™¨ï¼‰
- PDFç›´æ¥ä¸‹è½½
- å…ƒæ•°æ®æå–å’Œç®¡ç†
- æ‰¹é‡ä¸‹è½½å’Œè¿›åº¦ç®¡ç†
- åˆ†ç±»ç»„ç»‡
"""

import asyncio
import aiohttp
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from urllib.parse import urlencode
import feedparser
import sys
import tempfile
import tarfile
import subprocess
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.logger import Logger

# è½¬æ¢å·¥å…·å¯¼å…¥
try:
    from marker.converters.pdf import PdfConverter
    MARKER_AVAILABLE = True
except ImportError:
    MARKER_AVAILABLE = False

try:
    import pymupdf4llm
    PYMUPDF4LLM_AVAILABLE = True
except ImportError:
    PYMUPDF4LLM_AVAILABLE = False

try:
    import pypandoc
    PYPANDOC_AVAILABLE = True
except ImportError:
    PYPANDOC_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

CONVERSION_AVAILABLE = MARKER_AVAILABLE or PYMUPDF4LLM_AVAILABLE or PYPANDOC_AVAILABLE or PDFPLUMBER_AVAILABLE


class ArxivSearcher:
    """ArXivæœç´¢å’Œä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: Path = None):
        self.logger = Logger("ArXivæ–‡çŒ®æœç´¢å™¨")
        self.output_dir = output_dir or Path("K-Vault/ArXiv")
        self.pdf_dir = self.output_dir / "pdfs"
        self.metadata_dir = self.output_dir / "metadata"
        self.markdown_dir = self.output_dir / "markdown"  # æ–°å¢markdownç›®å½•
        self.tex_dir = self.output_dir / "tex_sources"    # æ–°å¢texæºç ç›®å½•
        self.progress_file = self.output_dir / "progress.json"
        self.search_cache_file = self.output_dir / "search_cache.json"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        for dir_path in [self.pdf_dir, self.metadata_dir, self.markdown_dir, self.tex_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ArXiv APIé…ç½®
        self.api_base = "http://export.arxiv.org/api/query"
        self.delay_between_requests = 3  # ArXivè¦æ±‚è‡³å°‘3ç§’é—´éš”
        
        # è½¬æ¢é…ç½®
        self.enable_markdown = CONVERSION_AVAILABLE
        self.preferred_converter = "auto"  # auto, marker, pymupdf4llm, pypandoc
        
        # æ£€æŸ¥å¯ç”¨çš„è½¬æ¢å™¨
        self.available_converters = []
        if PYMUPDF4LLM_AVAILABLE:
            self.available_converters.append("pymupdf4llm")
        if PDFPLUMBER_AVAILABLE:
            self.available_converters.append("pdfplumber")
        if PYPANDOC_AVAILABLE:
            self.available_converters.append("pypandoc")
        # if MARKER_AVAILABLE:
        #     self.available_converters.append("marker")  # æš‚æ—¶ç¦ç”¨ï¼ŒAPIå¤æ‚
        
        if not CONVERSION_AVAILABLE:
            self.logger.warning("ğŸ“ Markdownè½¬æ¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£…è½¬æ¢å·¥å…·")
        else:
            self.logger.info(f"ğŸ“ å¯ç”¨è½¬æ¢å™¨: {', '.join(self.available_converters)}")
        
        # è¿›åº¦ç¼“å­˜
        self.progress_cache = self._load_progress()
        self.search_cache = self._load_search_cache()
    
    def _load_progress(self) -> Dict[str, Any]:
        """åŠ è½½ä¸‹è½½è¿›åº¦"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}
    
    def _save_progress(self):
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def _load_search_cache(self) -> Dict[str, Any]:
        """åŠ è½½æœç´¢ç¼“å­˜"""
        if self.search_cache_file.exists():
            try:
                with open(self.search_cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}
    
    def _save_search_cache(self):
        """ä¿å­˜æœç´¢ç¼“å­˜"""
        try:
            with open(self.search_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.search_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœç´¢ç¼“å­˜å¤±è´¥: {e}")
    
    def clean_filename(self, title: str, arxiv_id: str = None) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_title = re.sub(r'[<>:"/\\|?*\[\]]', '_', title)
        clean_title = re.sub(r'\s+', '_', clean_title)
        clean_title = clean_title.strip('_.')
        
        # é™åˆ¶é•¿åº¦ï¼Œä¸ºarxiv_idç•™ç©ºé—´
        max_len = 80 if arxiv_id else 100
        if len(clean_title) > max_len:
            clean_title = clean_title[:max_len-3] + "..."
        
        # å¦‚æœæœ‰arxiv_idï¼Œä½œä¸ºå‰ç¼€
        if arxiv_id:
            return f"{arxiv_id}_{clean_title}"
        return clean_title
    
    async def search_arxiv(
        self,
        query: str,
        max_results: int = 100,
        sort_by: str = "relevance",
        sort_order: str = "descending",
        categories: List[str] = None,
        start_date: str = None,
        end_date: str = None
    ) -> Dict[str, Any]:
        """
        æœç´¢ArXivæ–‡çŒ®
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
            sort_by: æ’åºæ–¹å¼ (relevance, lastUpdatedDate, submittedDate)
            sort_order: æ’åºé¡ºåº (ascending, descending)
            categories: åˆ†ç±»è¿‡æ»¤ (å¦‚ ['cs.AI', 'cs.LG'])
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        self.logger.info(f"ğŸ” æœç´¢ArXivæ–‡çŒ®: {query}")
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = f"all:{query}"
        
        # æ·»åŠ åˆ†ç±»è¿‡æ»¤
        if categories:
            cat_filter = " OR ".join([f"cat:{cat}" for cat in categories])
            search_query = f"({search_query}) AND ({cat_filter})"
        
        # æ·»åŠ æ—¥æœŸè¿‡æ»¤
        if start_date or end_date:
            date_filter = self._build_date_filter(start_date, end_date)
            if date_filter:
                search_query = f"({search_query}) AND {date_filter}"
        
        # APIå‚æ•°
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results,
            'sortBy': sort_by,
            'sortOrder': sort_order
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.api_base}?{urlencode(params)}"
                self.logger.info(f"ğŸ“¡ è¯·æ±‚URL: {url}")
                
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        return self._parse_arxiv_response(content, query)
                    else:
                        return {
                            "status": "error",
                            "message": f"APIè¯·æ±‚å¤±è´¥: HTTP {response.status}",
                            "query": query
                        }
        
        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"æœç´¢å¤±è´¥: {str(e)}",
                "query": query
            }
    
    def _build_date_filter(self, start_date: str, end_date: str) -> str:
        """æ„å»ºæ—¥æœŸè¿‡æ»¤å™¨"""
        filters = []
        
        if start_date:
            # ArXivæ—¥æœŸæ ¼å¼: YYYYMMDDHHMMSS
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            start_str = start_dt.strftime("%Y%m%d") + "000000"
            filters.append(f"submittedDate:[{start_str} TO *]")
        
        if end_date:
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            end_str = end_dt.strftime("%Y%m%d") + "235959"
            filters.append(f"submittedDate:[* TO {end_str}]")
        
        return " AND ".join(filters) if filters else ""
    
    def _parse_arxiv_response(self, xml_content: str, query: str) -> Dict[str, Any]:
        """è§£æArXiv APIå“åº”"""
        try:
            # ä½¿ç”¨feedparserè§£æAtom feed
            feed = feedparser.parse(xml_content)
            
            if not feed.entries:
                return {
                    "status": "success",
                    "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®",
                    "query": query,
                    "total_results": 0,
                    "results": []
                }
            
            results = []
            for entry in feed.entries:
                # æå–ArXiv ID
                arxiv_id = entry.id.split('/')[-1]
                
                # æå–ä½œè€…ä¿¡æ¯
                authors = []
                if hasattr(entry, 'authors'):
                    authors = [author.name for author in entry.authors]
                elif hasattr(entry, 'author'):
                    authors = [entry.author]
                
                # æå–åˆ†ç±»
                categories = []
                if hasattr(entry, 'arxiv_primary_category'):
                    categories.append(entry.arxiv_primary_category['term'])
                if hasattr(entry, 'tags'):
                    categories.extend([tag['term'] for tag in entry.tags if tag['scheme'].endswith('arxiv')])
                
                # æ„å»ºPDFé“¾æ¥
                pdf_url = None
                for link in entry.links:
                    if link.get('type') == 'application/pdf':
                        pdf_url = link.href
                        break
                
                if not pdf_url:
                    # æ„é€ æ ‡å‡†PDF URL
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
                paper_info = {
                    "arxiv_id": arxiv_id,
                    "title": entry.title,
                    "summary": entry.summary,
                    "authors": authors,
                    "categories": categories,
                    "published": entry.published if hasattr(entry, 'published') else None,
                    "updated": entry.updated if hasattr(entry, 'updated') else None,
                    "pdf_url": pdf_url,
                    "arxiv_url": entry.link,
                    "primary_category": categories[0] if categories else "unknown"
                }
                
                results.append(paper_info)
            
            self.logger.info(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡æ–‡çŒ®")
            
            # ç¼“å­˜æœç´¢ç»“æœ
            cache_key = f"{query}_{datetime.now().strftime('%Y%m%d')}"
            self.search_cache[cache_key] = {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "total_results": len(results),
                "results": results
            }
            self._save_search_cache()
            
            return {
                "status": "success",
                "message": f"æˆåŠŸæœç´¢åˆ° {len(results)} ç¯‡æ–‡çŒ®",
                "query": query,
                "total_results": len(results),
                "results": results
            }
        
        except Exception as e:
            self.logger.error(f"è§£æå“åº”å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"è§£æå“åº”å¤±è´¥: {str(e)}",
                "query": query
            }
    
    async def download_paper(self, paper_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸‹è½½å•ç¯‡æ–‡çŒ®"""
        arxiv_id = paper_info["arxiv_id"]
        title = paper_info["title"]
        pdf_url = paper_info["pdf_url"]
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if self._is_already_downloaded(arxiv_id):
            return {
                "status": "skipped",
                "message": "æ–‡çŒ®å·²å­˜åœ¨",
                "arxiv_id": arxiv_id,
                "title": title
            }
        
        self.logger.info(f"ğŸ“¥ ä¸‹è½½æ–‡çŒ®: {title}")
        
        try:
            # åˆ›å»ºæ¸…ç†åçš„æ–‡ä»¶å
            clean_title = self.clean_filename(title, arxiv_id)
            pdf_filename = f"{clean_title}.pdf"
            metadata_filename = f"{clean_title}.json"
            
            pdf_path = self.pdf_dir / pdf_filename
            metadata_path = self.metadata_dir / metadata_filename
            
            # ä¸‹è½½PDF
            async with aiohttp.ClientSession() as session:
                async with session.get(pdf_url) as response:
                    if response.status == 200:
                        pdf_content = await response.read()
                        
                        # éªŒè¯PDFå†…å®¹
                        if len(pdf_content) < 1024:  # å°äº1KBè®¤ä¸ºæ— æ•ˆ
                            return {
                                "status": "error",
                                "message": "PDFæ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½ä¸‹è½½å¤±è´¥",
                                "arxiv_id": arxiv_id
                            }
                        
                        # ä¿å­˜PDF
                        with open(pdf_path, 'wb') as f:
                            f.write(pdf_content)
                        
                        # ä¿å­˜å…ƒæ•°æ®
                        enhanced_metadata = {
                            **paper_info,
                            "download_timestamp": datetime.now().isoformat(),
                            "local_pdf_path": str(pdf_path),
                            "local_metadata_path": str(metadata_path),
                            "file_size": len(pdf_content)
                        }
                        
                        with open(metadata_path, 'w', encoding='utf-8') as f:
                            json.dump(enhanced_metadata, f, ensure_ascii=False, indent=2)
                        
                        # æ›´æ–°è¿›åº¦
                        self.progress_cache[arxiv_id] = {
                            "status": "success",
                            "title": title,
                            "download_time": datetime.now().isoformat(),
                            "pdf_path": str(pdf_path),
                            "metadata_path": str(metadata_path)
                        }
                        self._save_progress()
                        
                        self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {clean_title}")
                        
                        return {
                            "status": "success",
                            "message": "ä¸‹è½½æˆåŠŸ",
                            "arxiv_id": arxiv_id,
                            "title": title,
                            "pdf_path": str(pdf_path),
                            "metadata_path": str(metadata_path),
                            "file_size": len(pdf_content)
                        }
                    
                    else:
                        return {
                            "status": "error",
                            "message": f"ä¸‹è½½å¤±è´¥: HTTP {response.status}",
                            "arxiv_id": arxiv_id,
                            "pdf_url": pdf_url
                        }
        
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"ä¸‹è½½å¤±è´¥: {str(e)}",
                "arxiv_id": arxiv_id
            }
    
    def _is_already_downloaded(self, arxiv_id: str) -> bool:
        """æ£€æŸ¥æ–‡çŒ®æ˜¯å¦å·²ä¸‹è½½"""
        # æ£€æŸ¥è¿›åº¦ç¼“å­˜
        if arxiv_id in self.progress_cache:
            status = self.progress_cache[arxiv_id].get("status")
            if status == "success":
                # éªŒè¯æ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
                pdf_path = self.progress_cache[arxiv_id].get("pdf_path")
                if pdf_path and Path(pdf_path).exists():
                    return True
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆé€šè¿‡æ–‡ä»¶åæ¨¡å¼åŒ¹é…ï¼‰
        for pdf_file in self.pdf_dir.glob(f"{arxiv_id}_*.pdf"):
            if pdf_file.exists():
                return True
        
        return False
    
    async def batch_download(
        self,
        papers: List[Dict[str, Any]],
        max_concurrent: int = 3,
        delay_between: float = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡ä¸‹è½½æ–‡çŒ®"""
        delay = delay_between or self.delay_between_requests
        total = len(papers)
        self.logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½ {total} ç¯‡æ–‡çŒ®")
        
        success_count = 0
        error_count = 0
        skipped_count = 0
        errors = []
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def download_with_semaphore(paper, index):
            async with semaphore:
                try:
                    self.logger.info(f"ğŸ“¥ [{index+1}/{total}] å¤„ç†: {paper['title'][:50]}...")
                    result = await self.download_paper(paper)
                    
                    # è¯·æ±‚é—´éš”
                    if index < total - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦ç­‰å¾…
                        await asyncio.sleep(delay)
                    
                    return result
                
                except Exception as e:
                    self.logger.error(f"ä¸‹è½½å¼‚å¸¸: {e}")
                    return {
                        "status": "error",
                        "message": str(e),
                        "arxiv_id": paper.get("arxiv_id", "unknown")
                    }
        
        # å¹¶å‘ä¸‹è½½
        tasks = [download_with_semaphore(paper, i) for i, paper in enumerate(papers)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                error_count += 1
                errors.append(str(result))
            elif result["status"] == "success":
                success_count += 1
            elif result["status"] == "skipped":
                skipped_count += 1
            else:
                error_count += 1
                errors.append(result.get("message", "æœªçŸ¥é”™è¯¯"))
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_papers": total,
            "successful_downloads": success_count,
            "skipped_papers": skipped_count,
            "failed_downloads": error_count,
            "success_rate": (success_count / total * 100) if total > 0 else 0,
            "errors": errors[:10],  # åªä¿å­˜å‰10ä¸ªé”™è¯¯
            "output_directory": str(self.output_dir)
        }
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.output_dir / "download_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info("ğŸ“Š æ‰¹é‡ä¸‹è½½ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ æ€»æ–‡çŒ®æ•°: {total}")
        self.logger.info(f"  â€¢ æˆåŠŸä¸‹è½½: {success_count}")
        self.logger.info(f"  â€¢ å·²å­˜åœ¨è·³è¿‡: {skipped_count}")
        self.logger.info(f"  â€¢ ä¸‹è½½å¤±è´¥: {error_count}")
        self.logger.info(f"  â€¢ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        return {
            "status": "success" if error_count == 0 else "partial",
            "message": f"æ‰¹é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸç‡: {summary['success_rate']:.1f}%",
            "summary": summary
        }
    
    async def search_and_download(
        self,
        query: str,
        max_results: int = 50,
        categories: List[str] = None,
        start_date: str = None,
        end_date: str = None,
        auto_download: bool = True
    ) -> Dict[str, Any]:
        """æœç´¢å¹¶ä¸‹è½½æ–‡çŒ®ï¼ˆä¸€ç«™å¼æœåŠ¡ï¼‰"""
        self.logger.info(f"ğŸ¯ ä¸€ç«™å¼æœç´¢ä¸‹è½½: {query}")
        
        # 1. æœç´¢
        search_result = await self.search_arxiv(
            query=query,
            max_results=max_results,
            categories=categories,
            start_date=start_date,
            end_date=end_date
        )
        
        if search_result["status"] != "success":
            return search_result
        
        papers = search_result["results"]
        if not papers:
            return {
                "status": "success",
                "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®",
                "query": query
            }
        
        self.logger.info(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")
        
        # 2. ä¸‹è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if auto_download:
            download_result = await self.batch_download(papers)
            return {
                "status": "success",
                "message": f"æœç´¢å¹¶ä¸‹è½½å®Œæˆ",
                "query": query,
                "search_results": len(papers),
                "download_summary": download_result["summary"]
            }
        else:
            return {
                "status": "success",
                "message": f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®",
                "query": query,
                "results": papers
            }


    async def download_tex_source(self, arxiv_id: str) -> Dict[str, Any]:
        """ä¸‹è½½TeXæºç """
        if not self.enable_markdown:
            return {"status": "error", "message": "è½¬æ¢åŠŸèƒ½ä¸å¯ç”¨"}
        
        tex_url = f"https://arxiv.org/src/{arxiv_id}"
        tex_filename = f"{arxiv_id}_source.tar.gz"
        tex_path = self.tex_dir / tex_filename
        
        try:
            self.logger.info(f"ğŸ“¥ ä¸‹è½½TeXæºç : {arxiv_id}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(tex_url) as response:
                    if response.status == 200:
                        content = await response.read()
                        tex_path.write_bytes(content)
                        
                        # è§£å‹æºç 
                        extract_dir = self.tex_dir / f"{arxiv_id}_extracted"
                        extract_dir.mkdir(exist_ok=True)
                        
                        with tarfile.open(tex_path, 'r:gz') as tar:
                            tar.extractall(extract_dir)
                        
                        return {
                            "status": "success",
                            "tex_path": str(tex_path),
                            "extract_dir": str(extract_dir)
                        }
                    else:
                        return {"status": "error", "message": f"ä¸‹è½½å¤±è´¥: {response.status}"}
                        
        except Exception as e:
            self.logger.error(f"âŒ TeXæºç ä¸‹è½½å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}


    def convert_pdf_to_markdown(self, pdf_path: Path, converter: str = "auto") -> Dict[str, Any]:
        """å°†PDFè½¬æ¢ä¸ºMarkdownï¼ˆæ™ºèƒ½è½¬æ¢å™¨é€‰æ‹©ï¼‰"""
        if not self.enable_markdown:
            return {"status": "error", "message": "è½¬æ¢åŠŸèƒ½ä¸å¯ç”¨"}
        
        # æ™ºèƒ½é€‰æ‹©è½¬æ¢å™¨
        if converter == "auto":
            converter = self._choose_best_converter()
        
        self.logger.info(f"ğŸ“ PDFè½¬Markdown: {pdf_path.name} (ä½¿ç”¨{converter})")
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•è½¬æ¢å™¨
        converters_to_try = [converter] if converter != "auto" else self.available_converters
        
        for conv in converters_to_try:
            try:
                result = self._convert_with_specific_tool(pdf_path, conv)
                if result["status"] == "success":
                    result["converter_used"] = conv
                    return result
                else:
                    self.logger.warning(f"âš ï¸ {conv}è½¬æ¢å¤±è´¥: {result.get('message')}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ {conv}è½¬æ¢å‡ºé”™: {e}")
                continue
        
        return {"status": "error", "message": "æ‰€æœ‰è½¬æ¢å™¨éƒ½å¤±è´¥äº†"}


    def _choose_best_converter(self) -> str:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³è½¬æ¢å™¨"""
        # æŒ‰è´¨é‡ä¼˜å…ˆçº§æ’åº (å½“å‰æ¨èpymupdf4llmä½œä¸ºä¸»åŠ›)
        priority_order = ["pymupdf4llm", "pdfplumber", "pypandoc", "marker"]
        
        for converter in priority_order:
            if converter in self.available_converters:
                return converter
        
        return self.available_converters[0] if self.available_converters else "none"


    def _convert_with_specific_tool(self, pdf_path: Path, converter: str) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šå·¥å…·è½¬æ¢PDF"""
        
        if converter == "pymupdf4llm":
            return self._convert_with_pymupdf4llm(pdf_path)
        elif converter == "pdfplumber":
            return self._convert_with_pdfplumber(pdf_path)
        elif converter == "pypandoc":
            return self._convert_with_pypandoc(pdf_path)
        elif converter == "marker":
            return self._convert_with_marker(pdf_path)
        else:
            return {"status": "error", "message": f"æœªçŸ¥è½¬æ¢å™¨: {converter}"}


    def _convert_with_marker(self, pdf_path: Path) -> Dict[str, Any]:
        """ä½¿ç”¨Markerè½¬æ¢PDFï¼ˆæœ€é«˜è´¨é‡ï¼ŒåŸºäºAIï¼‰"""
        try:
            import tempfile
            import subprocess
            import json
            
            self.logger.info(f"ğŸ“ ä½¿ç”¨Markerè½¬æ¢: {pdf_path.name}")
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_output = Path(temp_dir) / "output"
                
                # ä½¿ç”¨markerå‘½ä»¤è¡Œå·¥å…·
                cmd = [
                    "python3", "-m", "marker.scripts.convert_single",
                    str(pdf_path),
                    str(temp_output),
                    "--config_file", "marker/settings/marker_local.yaml"
                ]
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                if result.returncode == 0:
                    # æŸ¥æ‰¾ç”Ÿæˆçš„markdownæ–‡ä»¶
                    markdown_files = list(temp_output.glob("*.md"))
                    if markdown_files:
                        markdown_content = markdown_files[0].read_text(encoding='utf-8')
                        
                        # ä¿å­˜åˆ°æ­£å¼ç›®å½•
                        markdown_filename = pdf_path.stem + "_marker.md"
                        markdown_path = self.markdown_dir / markdown_filename
                        markdown_path.write_text(markdown_content, encoding='utf-8')
                        
                        return {
                            "status": "success",
                            "markdown_path": str(markdown_path),
                            "content_length": len(markdown_content),
                            "converter": "marker"
                        }
                    else:
                        return {"status": "error", "message": "Markerè½¬æ¢æœªç”Ÿæˆmarkdownæ–‡ä»¶"}
                else:
                    error_msg = result.stderr or result.stdout or "æœªçŸ¥é”™è¯¯"
                    return {"status": "error", "message": f"Markerå‘½ä»¤è¡Œæ‰§è¡Œå¤±è´¥: {error_msg}"}
            
        except subprocess.TimeoutExpired:
            return {"status": "error", "message": "Markerè½¬æ¢è¶…æ—¶ï¼ˆ>5åˆ†é’Ÿï¼‰"}
        except Exception as e:
            return {"status": "error", "message": f"Markerè½¬æ¢å¤±è´¥: {e}"}


    def _convert_with_pymupdf4llm(self, pdf_path: Path) -> Dict[str, Any]:
        """ä½¿ç”¨PyMuPDF4LLMè½¬æ¢PDFï¼ˆæ¨èï¼Œå¿«é€Ÿä¸”è´¨é‡å¥½ï¼‰"""
        try:
            import pymupdf4llm
            
            markdown_content = pymupdf4llm.to_markdown(str(pdf_path))
            
            # ä¿å­˜markdownæ–‡ä»¶
            markdown_filename = pdf_path.stem + "_pymupdf.md"
            markdown_path = self.markdown_dir / markdown_filename
            
            markdown_path.write_text(markdown_content, encoding='utf-8')
            
            return {
                "status": "success",
                "markdown_path": str(markdown_path),
                "content_length": len(markdown_content)
            }
            
        except Exception as e:
            return {"status": "error", "message": f"PyMuPDF4LLMè½¬æ¢å¤±è´¥: {e}"}


    def _convert_with_pdfplumber(self, pdf_path: Path) -> Dict[str, Any]:
        """ä½¿ç”¨pdfplumberè½¬æ¢PDFï¼ˆè½»é‡çº§ï¼Œé€‚åˆç®€å•æ–‡æ¡£ï¼‰"""
        try:
            import pdfplumber
            
            markdown_content = []
            
            with pdfplumber.open(str(pdf_path)) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    # æå–æ–‡æœ¬
                    text = page.extract_text()
                    if text:
                        markdown_content.append(f"## Page {page_num}\n\n{text}\n\n")
                    
                    # æå–è¡¨æ ¼
                    tables = page.extract_tables()
                    for table_num, table in enumerate(tables, 1):
                        if table:
                            markdown_content.append(f"### Table {table_num} (Page {page_num})\n\n")
                            # è½¬æ¢ä¸ºmarkdownè¡¨æ ¼
                            if len(table) > 0:
                                # è¡¨å¤´
                                header = "| " + " | ".join(str(cell) if cell else "" for cell in table[0]) + " |\n"
                                separator = "|" + "---|" * len(table[0]) + "\n"
                                markdown_content.append(header + separator)
                                
                                # è¡¨æ ¼æ•°æ®
                                for row in table[1:]:
                                    row_md = "| " + " | ".join(str(cell) if cell else "" for cell in row) + " |\n"
                                    markdown_content.append(row_md)
                                markdown_content.append("\n")
            
            final_markdown = "".join(markdown_content)
            
            # ä¿å­˜markdownæ–‡ä»¶
            markdown_filename = pdf_path.stem + "_pdfplumber.md"
            markdown_path = self.markdown_dir / markdown_filename
            
            markdown_path.write_text(final_markdown, encoding='utf-8')
            
            return {
                "status": "success",
                "markdown_path": str(markdown_path),
                "content_length": len(final_markdown)
            }
            
        except Exception as e:
            return {"status": "error", "message": f"pdfplumberè½¬æ¢å¤±è´¥: {e}"}


    def _convert_with_pypandoc(self, pdf_path: Path) -> Dict[str, Any]:
        """ä½¿ç”¨Pypandocè½¬æ¢PDFï¼ˆåŸºç¡€è´¨é‡ï¼‰"""
        try:
            import pypandoc
            
            # æ³¨æ„ï¼špypandocå¤„ç†PDFæ•ˆæœä¸å¥½ï¼Œè¿™é‡Œåªæ˜¯ä½œä¸ºæœ€åå¤‡é€‰
            markdown_content = pypandoc.convert_file(
                str(pdf_path),
                'markdown',
                extra_args=['--wrap=none']
            )
            
            # ä¿å­˜markdownæ–‡ä»¶
            markdown_filename = pdf_path.stem + "_pandoc.md"
            markdown_path = self.markdown_dir / markdown_filename
            
            markdown_path.write_text(markdown_content, encoding='utf-8')
            
            return {
                "status": "success",
                "markdown_path": str(markdown_path),
                "content_length": len(markdown_content)
            }
            
        except Exception as e:
            return {"status": "error", "message": f"Pypandocè½¬æ¢å¤±è´¥: {e}"}


    def convert_tex_to_markdown(self, tex_dir: Path, main_tex_file: str = None) -> Dict[str, Any]:
        """å°†TeXæºç è½¬æ¢ä¸ºMarkdown"""
        if not self.enable_markdown:
            return {"status": "error", "message": "è½¬æ¢åŠŸèƒ½ä¸å¯ç”¨"}
        
        try:
            self.logger.info(f"ğŸ“ TeXè½¬Markdown: {tex_dir.name}")
            
            # å¯»æ‰¾ä¸»TeXæ–‡ä»¶
            if not main_tex_file:
                tex_files = list(tex_dir.glob("*.tex"))
                if not tex_files:
                    return {"status": "error", "message": "æœªæ‰¾åˆ°TeXæ–‡ä»¶"}
                
                # å°è¯•æ‰¾åˆ°ä¸»æ–‡ä»¶ï¼ˆé€šå¸¸åŒ…å«\\documentclassï¼‰
                main_tex_file = None
                for tex_file in tex_files:
                    content = tex_file.read_text(encoding='utf-8', errors='ignore')
                    if '\\documentclass' in content:
                        main_tex_file = tex_file.name
                        break
                
                if not main_tex_file:
                    main_tex_file = tex_files[0].name
            
            tex_file_path = tex_dir / main_tex_file
            
            # ä½¿ç”¨pandocè½¬æ¢ï¼ˆéœ€è¦åœ¨texç›®å½•ä¸­æ‰§è¡Œä»¥å¤„ç†å¼•ç”¨ï¼‰
            import pypandoc
            
            # åˆ‡æ¢åˆ°TeXç›®å½•
            original_cwd = os.getcwd()
            os.chdir(tex_dir)
            
            try:
                markdown_content = pypandoc.convert_file(
                    main_tex_file,
                    'markdown',
                    extra_args=[
                        '--wrap=none',
                        '--bibliography-title=References',
                        '--standalone'
                    ]
                )
                
                # ä¿å­˜markdownæ–‡ä»¶
                markdown_filename = tex_file_path.stem + ".md"
                markdown_path = self.markdown_dir / markdown_filename
                
                markdown_path.write_text(markdown_content, encoding='utf-8')
                
                return {
                    "status": "success",
                    "markdown_path": str(markdown_path),
                    "content_length": len(markdown_content),
                    "source_tex": main_tex_file
                }
                
            finally:
                os.chdir(original_cwd)
            
        except Exception as e:
            self.logger.error(f"âŒ TeXè½¬Markdownå¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}


    async def download_and_convert_to_markdown(
        self,
        paper: Dict[str, Any],
        convert_method: str = "both"  # "pdf", "tex", "both"
    ) -> Dict[str, Any]:
        """ä¸‹è½½å¹¶è½¬æ¢ä¸ºMarkdownï¼ˆä¸€ç«™å¼ï¼‰"""
        if not self.enable_markdown:
            return {"status": "error", "message": "è½¬æ¢åŠŸèƒ½ä¸å¯ç”¨"}
        
        arxiv_id = paper["arxiv_id"]
        title = paper["title"]
        
        self.logger.info(f"ğŸ“š ä¸‹è½½å¹¶è½¬æ¢: {title}")
        
        results = {"status": "success", "conversions": {}}
        
        # ä¸‹è½½PDFå’Œè½¬æ¢
        if convert_method in ["pdf", "both"]:
            # å…ˆä¸‹è½½PDF
            download_result = await self.download_paper(paper)
            
            if download_result["status"] == "success":
                pdf_path = Path(download_result["pdf_path"])
                
                # è½¬æ¢PDFä¸ºMarkdown
                pdf_conversion = self.convert_pdf_to_markdown(pdf_path)
                results["conversions"]["pdf_to_markdown"] = pdf_conversion
            else:
                results["conversions"]["pdf_to_markdown"] = {
                    "status": "error", 
                    "message": "PDFä¸‹è½½å¤±è´¥"
                }
        
        # ä¸‹è½½TeXæºç å’Œè½¬æ¢
        if convert_method in ["tex", "both"]:
            # ä¸‹è½½TeXæºç 
            tex_result = await self.download_tex_source(arxiv_id)
            
            if tex_result["status"] == "success":
                extract_dir = Path(tex_result["extract_dir"])
                
                # è½¬æ¢TeXä¸ºMarkdown
                tex_conversion = self.convert_tex_to_markdown(extract_dir)
                results["conversions"]["tex_to_markdown"] = tex_conversion
            else:
                results["conversions"]["tex_to_markdown"] = {
                    "status": "error",
                    "message": "TeXæºç ä¸‹è½½å¤±è´¥"
                }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æˆåŠŸçš„è½¬æ¢
        successful_conversions = [
            conv for conv in results["conversions"].values()
            if conv["status"] == "success"
        ]
        
        if not successful_conversions:
            results["status"] = "error"
            results["message"] = "æ‰€æœ‰è½¬æ¢æ–¹æ³•éƒ½å¤±è´¥äº†"
        
        return results


    async def batch_convert_to_markdown(
        self,
        papers: List[Dict[str, Any]],
        convert_method: str = "both"
    ) -> Dict[str, Any]:
        """æ‰¹é‡è½¬æ¢ä¸ºMarkdown"""
        if not self.enable_markdown:
            return {"status": "error", "message": "è½¬æ¢åŠŸèƒ½ä¸å¯ç”¨"}
        
        self.logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡Markdownè½¬æ¢ {len(papers)} ç¯‡æ–‡çŒ®")
        
        successful_conversions = 0
        failed_conversions = 0
        conversion_details = []
        
        for i, paper in enumerate(papers, 1):
            self.logger.info(f"ğŸ“ [{i}/{len(papers)}] è½¬æ¢: {paper['title'][:50]}...")
            
            try:
                result = await self.download_and_convert_to_markdown(paper, convert_method)
                
                conversion_details.append({
                    "arxiv_id": paper["arxiv_id"],
                    "title": paper["title"],
                    "result": result
                })
                
                if result["status"] == "success":
                    successful_conversions += 1
                else:
                    failed_conversions += 1
                    
            except Exception as e:
                self.logger.error(f"âŒ è½¬æ¢å‡ºé”™: {e}")
                failed_conversions += 1
                conversion_details.append({
                    "arxiv_id": paper["arxiv_id"],
                    "title": paper["title"],
                    "result": {"status": "error", "message": str(e)}
                })
            
            # æ·»åŠ å»¶è¿Ÿ
            await asyncio.sleep(self.delay_between_requests)
        
        # ç»Ÿè®¡ç»“æœ
        success_rate = (successful_conversions / len(papers)) * 100 if papers else 0
        
        summary = {
            "total_papers": len(papers),
            "successful_conversions": successful_conversions,
            "failed_conversions": failed_conversions,
            "success_rate": round(success_rate, 1)
        }
        
        self.logger.info(f"ğŸ“Š æ‰¹é‡è½¬æ¢ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ æ€»æ–‡çŒ®æ•°: {summary['total_papers']}")
        self.logger.info(f"  â€¢ æˆåŠŸè½¬æ¢: {summary['successful_conversions']}")
        self.logger.info(f"  â€¢ è½¬æ¢å¤±è´¥: {summary['failed_conversions']}")
        self.logger.info(f"  â€¢ æˆåŠŸç‡: {summary['success_rate']}%")
        
        return {
            "status": "success",
            "summary": summary,
            "details": conversion_details
        }


async def main():
    """æµ‹è¯•å‡½æ•°"""
    searcher = ArxivSearcher()
    
    # æµ‹è¯•æœç´¢
    result = await searcher.search_and_download(
        query="machine learning",
        max_results=5,
        categories=["cs.LG", "cs.AI"],
        auto_download=True
    )
    
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    asyncio.run(main())
