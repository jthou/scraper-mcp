# ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·åŒ…

ä¸€ä¸ªç‹¬ç«‹çš„Pythonå·¥å…·åŒ…ï¼Œæä¾›ç½‘é¡µå†…å®¹æŠ“å–ã€ä¸‹è½½ã€è½¬æ¢ç­‰åŠŸèƒ½ã€‚æ”¯æŒçŸ¥ä¹ã€å¾®ä¿¡ç­‰å¹³å°çš„å†…å®¹æŠ“å–ã€‚

## ç‰¹æ€§

- ğŸš€ **å¤šå¹³å°æ”¯æŒ**ï¼šæ”¯æŒçŸ¥ä¹ã€å¾®ä¿¡ç­‰å¹³å°
- ğŸ” **æ™ºèƒ½æœç´¢**ï¼šæ”¯æŒå…³é”®è¯æœç´¢å’Œç»“æœæå–
- ğŸ“¥ **å†…å®¹ä¸‹è½½**ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶è½¬æ¢ä¸ºPDFå’ŒMarkdownæ ¼å¼
- ğŸ­ **äººå·¥éªŒè¯æ”¯æŒ**ï¼šæ”¯æŒéœ€è¦äººå·¥éªŒè¯çš„å¹³å°ï¼ˆå¦‚å¾®ä¿¡ï¼‰
- âš¡ **ä¾¿æ·å‡½æ•°**ï¼šæä¾›å¿«é€Ÿä½¿ç”¨çš„ä¾¿æ·å‡½æ•°
- ğŸ–¥ï¸ **å‘½ä»¤è¡Œæ¥å£**ï¼šæä¾›å®Œæ•´çš„å‘½ä»¤è¡Œå·¥å…·
- ğŸ“š **ç¤ºä¾‹ä¸°å¯Œ**ï¼šæä¾›å¤šç§ä½¿ç”¨ç¤ºä¾‹

## å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd scraper-toolkit

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨
playwright install chromium
```

## å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨ä¾¿æ·å‡½æ•°

```python
import asyncio
from src.core.scraper_toolkit import quick_search, quick_download

async def main():
    # å¿«é€Ÿæœç´¢çŸ¥ä¹å†…å®¹
    result = await quick_search("zhihu", "Pythonç¼–ç¨‹", max_pages=2)
    print(result)
    
    # å¿«é€Ÿä¸‹è½½é¡µé¢
    result = await quick_download("zhihu", "https://www.zhihu.com/question/123456")
    print(result)

asyncio.run(main())
```

### 2. ä½¿ç”¨å·¥å…·åŒ…ç±»

```python
import asyncio
from pathlib import Path
from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

async def main():
    # åˆ›å»ºé…ç½®
    config = ScrapingConfig(
        platform=Platform.ZHIHU,
        headless=False,
        max_pages=3,
        output_dir=Path("data")
    )
    
    # åˆ›å»ºå·¥å…·åŒ…å®ä¾‹
    toolkit = ScraperToolkit(config)
    
    try:
        # æœç´¢å†…å®¹
        result = await toolkit.search(Platform.ZHIHU, "æœºå™¨å­¦ä¹ ", 3)
        print(f"æœç´¢ç»“æœ: {result}")
        
        # ä¸‹è½½å†…å®¹
        if result["status"] == "success" and result["results"]:
            first_result = result["results"][0]
            download_result = await toolkit.download_content(
                Platform.ZHIHU,
                first_result["url"],
                Path("data"),
                first_result["title"]
            )
            print(f"ä¸‹è½½ç»“æœ: {download_result}")
    
    finally:
        await toolkit.cleanup()

asyncio.run(main())
```

### 3. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·

```bash
# æœç´¢çŸ¥ä¹å†…å®¹
python -m src.cli search zhihu "Pythonç¼–ç¨‹" --max-pages 2

# ä¸‹è½½å•ä¸ªé¡µé¢
python -m src.cli download zhihu "https://www.zhihu.com/question/123456"

# æ‰¹é‡ä¸‹è½½
python -m src.cli batch zhihu "æœºå™¨å­¦ä¹ " --max-pages 3 --output data/ml

# å¿«é€Ÿæœç´¢
python -m src.cli quick-search zhihu "æ·±åº¦å­¦ä¹ "

# æŸ¥çœ‹å¹³å°ä¿¡æ¯
python -m src.cli info zhihu
```

## æ”¯æŒçš„å¹³å°

### çŸ¥ä¹ (zhihu)
- âœ… æœç´¢åŠŸèƒ½
- âœ… ç™»å½•åŠŸèƒ½
- âœ… é¡µé¢è¯»å–
- âœ… å†…å®¹ä¸‹è½½
- âŒ ä¸éœ€è¦äººå·¥éªŒè¯

### å¾®ä¿¡ (wechat)
- âœ… æœç´¢åŠŸèƒ½ï¼ˆé€šè¿‡æœç‹—æœç´¢ï¼‰
- âœ… é¡µé¢è¯»å–
- âœ… å†…å®¹ä¸‹è½½
- âš ï¸ éœ€è¦äººå·¥éªŒè¯ç éªŒè¯

## ä½¿ç”¨ç¤ºä¾‹

### çŸ¥ä¹å†…å®¹æœç´¢

```python
# examples/zhihu_search.py
import asyncio
from pathlib import Path
from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

async def search_zhihu():
    config = ScrapingConfig(
        platform=Platform.ZHIHU,
        headless=False,
        max_pages=2,
        output_dir=Path("data/zhihu")
    )
    
    toolkit = ScraperToolkit(config)
    
    try:
        # æœç´¢å†…å®¹
        result = await toolkit.search(Platform.ZHIHU, "Pythonç¼–ç¨‹", 2)
        print(f"æœç´¢ç»“æœ: {result}")
        
        # ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ
        if result["status"] == "success" and result["results"]:
            first_result = result["results"][0]
            download_result = await toolkit.download_content(
                Platform.ZHIHU,
                first_result["url"],
                Path("data/zhihu"),
                first_result["title"]
            )
            print(f"ä¸‹è½½ç»“æœ: {download_result}")
    
    finally:
        await toolkit.cleanup()

asyncio.run(search_zhihu())
```

### å¾®ä¿¡å†…å®¹æœç´¢

```python
# examples/wechat_search.py
import asyncio
from pathlib import Path
from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

async def search_wechat():
    config = ScrapingConfig(
        platform=Platform.WECHAT,
        headless=False,  # å¿…é¡»æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        max_pages=1,
        output_dir=Path("data/wechat"),
        wait_for_verification=True  # ç­‰å¾…äººå·¥éªŒè¯
    )
    
    toolkit = ScraperToolkit(config)
    
    try:
        # æœç´¢å†…å®¹ï¼ˆéœ€è¦äººå·¥éªŒè¯ï¼‰
        result = await toolkit.search(Platform.WECHAT, "äººå·¥æ™ºèƒ½", 1)
        print(f"æœç´¢ç»“æœ: {result}")
        
        # ä¸‹è½½ç¬¬ä¸€ä¸ªç»“æœ
        if result["status"] == "success" and result["results"]:
            first_result = result["results"][0]
            download_result = await toolkit.download_content(
                Platform.WECHAT,
                first_result["link"],
                Path("data/wechat"),
                first_result["title"]
            )
            print(f"ä¸‹è½½ç»“æœ: {download_result}")
    
    finally:
        await toolkit.cleanup()

asyncio.run(search_wechat())
```

### æ‰¹é‡ä¸‹è½½

```python
# examples/batch_download.py
import asyncio
from pathlib import Path
from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

async def batch_download():
    config = ScrapingConfig(
        platform=Platform.ZHIHU,
        headless=False,
        max_pages=3,
        output_dir=Path("data/batch")
    )
    
    toolkit = ScraperToolkit(config)
    
    try:
        # æ‰¹é‡ä¸‹è½½
        result = await toolkit.batch_download(
            Platform.ZHIHU,
            "æœºå™¨å­¦ä¹ ",
            Path("data/batch"),
            3
        )
        print(f"æ‰¹é‡ä¸‹è½½ç»“æœ: {result}")
    
    finally:
        await toolkit.cleanup()

asyncio.run(batch_download())
```

## å‘½ä»¤è¡Œå·¥å…·

### åŸºæœ¬ç”¨æ³•

```bash
# æŸ¥çœ‹å¸®åŠ©
python -m src.cli --help

# æŸ¥çœ‹å¹³å°ä¿¡æ¯
python -m src.cli info

# æŸ¥çœ‹ç‰¹å®šå¹³å°ä¿¡æ¯
python -m src.cli info zhihu
```

### æœç´¢å‘½ä»¤

```bash
# æœç´¢çŸ¥ä¹å†…å®¹
python -m src.cli search zhihu "Pythonç¼–ç¨‹" --max-pages 2

# æœç´¢å¾®ä¿¡å†…å®¹ï¼ˆéœ€è¦äººå·¥éªŒè¯ï¼‰
python -m src.cli search wechat "äººå·¥æ™ºèƒ½" --max-pages 1
```

### ä¸‹è½½å‘½ä»¤

```bash
# ä¸‹è½½çŸ¥ä¹é¡µé¢
python -m src.cli download zhihu "https://www.zhihu.com/question/123456"

# ä¸‹è½½å¾®ä¿¡é¡µé¢ï¼ˆéœ€è¦äººå·¥éªŒè¯ï¼‰
python -m src.cli download wechat "https://weixin.sogou.com/link?url=..."
```

### æ‰¹é‡ä¸‹è½½å‘½ä»¤

```bash
# æ‰¹é‡ä¸‹è½½çŸ¥ä¹å†…å®¹
python -m src.cli batch zhihu "æœºå™¨å­¦ä¹ " --max-pages 3 --output data/ml

# æ‰¹é‡ä¸‹è½½å¾®ä¿¡å†…å®¹ï¼ˆéœ€è¦äººå·¥éªŒè¯ï¼‰
python -m src.cli batch wechat "æ·±åº¦å­¦ä¹ " --max-pages 1 --output data/dl
```

### å¿«é€Ÿå‘½ä»¤

```bash
# å¿«é€Ÿæœç´¢
python -m src.cli quick-search zhihu "Pythonç¼–ç¨‹"

# å¿«é€Ÿä¸‹è½½
python -m src.cli quick-download zhihu "https://www.zhihu.com/question/123456"
```

## é…ç½®é€‰é¡¹

### ScrapingConfig å‚æ•°

- `platform`: å¹³å°ç±»å‹ï¼ˆPlatform.ZHIHU, Platform.WECHATï¼‰
- `headless`: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰
- `persistent`: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤Falseï¼‰
- `max_pages`: æœ€å¤§æœç´¢é¡µæ•°ï¼ˆé»˜è®¤3ï¼‰
- `output_dir`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤"data"ï¼‰
- `timeout`: è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤300ç§’ï¼‰
- `wait_for_verification`: æ˜¯å¦ç­‰å¾…äººå·¥éªŒè¯ï¼ˆé»˜è®¤Trueï¼‰

## æ³¨æ„äº‹é¡¹

### å¾®ä¿¡å¹³å°ç‰¹æ®Šè¯´æ˜

1. **éœ€è¦äººå·¥éªŒè¯**ï¼šå¾®ä¿¡æœç´¢é€šè¿‡æœç‹—æœç´¢ï¼Œéœ€è¦å®ŒæˆéªŒè¯ç éªŒè¯
2. **å¿…é¡»æ˜¾ç¤ºæµè§ˆå™¨**ï¼šä¸èƒ½ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼Œå› ä¸ºéœ€è¦ç”¨æˆ·äº¤äº’
3. **ç­‰å¾…æ—¶é—´**ï¼šç¨‹åºä¼šç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ï¼Œä¸ä¼šè¶…æ—¶
4. **éªŒè¯æç¤º**ï¼šç¨‹åºä¼šæ˜¾ç¤ºæ¸…æ™°çš„éªŒè¯æç¤ºä¿¡æ¯

### é€šç”¨æ³¨æ„äº‹é¡¹

1. **æµè§ˆå™¨è¦æ±‚**ï¼šéœ€è¦å®‰è£…Chromeæµè§ˆå™¨
2. **ç½‘ç»œè¦æ±‚**ï¼šéœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥
3. **åçˆ¬è™«**ï¼šæŸäº›å¹³å°å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œéœ€è¦äººå·¥å¤„ç†
4. **èµ„æºæ¸…ç†**ï¼šä½¿ç”¨å®Œæ¯•åè®°å¾—è°ƒç”¨`cleanup()`æ–¹æ³•æ¸…ç†èµ„æº

## é¡¹ç›®ç»“æ„

```
scraper-toolkit/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ scraper_toolkit.py    # ä¸»å·¥å…·åŒ…
â”‚   â”‚   â”œâ”€â”€ web_scraper.py        # çŸ¥ä¹æŠ“å–å™¨
â”‚   â”‚   â”œâ”€â”€ wechat_scraper.py     # å¾®ä¿¡æŠ“å–å™¨
â”‚   â”‚   â””â”€â”€ advanced_stealth.py   # é«˜çº§åçˆ¬è™«æŠ€æœ¯
â”‚   â”œâ”€â”€ cli.py                    # å‘½ä»¤è¡Œæ¥å£
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py             # æ—¥å¿—å·¥å…·
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ zhihu_search.py           # çŸ¥ä¹æœç´¢ç¤ºä¾‹
â”‚   â”œâ”€â”€ wechat_search.py          # å¾®ä¿¡æœç´¢ç¤ºä¾‹
â”‚   â”œâ”€â”€ batch_download.py         # æ‰¹é‡ä¸‹è½½ç¤ºä¾‹
â”‚   â””â”€â”€ quick_usage.py            # å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ data/                         # é»˜è®¤è¾“å‡ºç›®å½•
â”œâ”€â”€ requirements.txt              # ä¾èµ–æ–‡ä»¶
â””â”€â”€ README.md                     # è¯´æ˜æ–‡æ¡£
```

## å¼€å‘è¯´æ˜

### æ·»åŠ æ–°å¹³å°

1. åœ¨`Platform`æšä¸¾ä¸­æ·»åŠ æ–°å¹³å°
2. åœ¨`ScraperToolkit`ä¸­æ·»åŠ å¹³å°ç‰¹å®šçš„å¤„ç†æ–¹æ³•
3. åœ¨`get_platform_info`ä¸­æ·»åŠ å¹³å°ä¿¡æ¯
4. æ›´æ–°å‘½ä»¤è¡Œå·¥å…·æ”¯æŒæ–°å¹³å°

### æ‰©å±•åŠŸèƒ½

1. åœ¨`ScraperToolkit`ç±»ä¸­æ·»åŠ æ–°æ–¹æ³•
2. åœ¨å‘½ä»¤è¡Œå·¥å…·ä¸­æ·»åŠ å¯¹åº”çš„å‘½ä»¤
3. åœ¨examplesä¸­æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
4. æ›´æ–°æ–‡æ¡£è¯´æ˜

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒçŸ¥ä¹å’Œå¾®ä¿¡å¹³å°
- æä¾›å®Œæ•´çš„å‘½ä»¤è¡Œå·¥å…·
- æ”¯æŒäººå·¥éªŒè¯ç­‰å¾…
- æä¾›ä¸°å¯Œçš„ä½¿ç”¨ç¤ºä¾‹