#!/usr/bin/env python3
"""
Isaac Sim URLéªŒè¯å’Œå‘ç°å·¥å…·
æ‰¾å‡ºçœŸæ­£æœ‰æ•ˆçš„æ–‡æ¡£URL
"""

import asyncio
import aiohttp
import json
from pathlib import Path
from urllib.parse import urljoin, urlparse
import re

class IsaacURLDiscoverer:
    def __init__(self):
        self.valid_urls = []
        self.invalid_urls = []
        self.discovered_patterns = set()
        
    async def check_url_exists(self, session, url, timeout=10):
        """æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨"""
        try:
            async with session.head(url, timeout=timeout, allow_redirects=True) as response:
                return {
                    'url': url,
                    'status': response.status,
                    'valid': 200 <= response.status < 400,
                    'content_type': response.headers.get('content-type', ''),
                    'final_url': str(response.url)
                }
        except Exception as e:
            return {
                'url': url,
                'status': 'error',
                'valid': False,
                'error': str(e)
            }
    
    async def discover_isaac_documentation_structure(self):
        """å‘ç°Isaacæ–‡æ¡£çš„çœŸå®ç»“æ„"""
        print("ğŸ” å‘ç°Isaacæ–‡æ¡£çš„çœŸå®ç»“æ„...")
        
        # å·²çŸ¥çš„æœ‰æ•ˆåŸºç¡€URL
        base_urls_to_test = [
            "https://docs.omniverse.nvidia.com/isaacsim/",
            "https://docs.omniverse.nvidia.com/app_isaacsim/",
            "https://docs.omniverse.nvidia.com/py/isaacsim/",
            "https://isaac-sim.github.io/IsaacLab/",
            "https://isaac-sim.github.io/IsaacLab/main/",
            "https://nvidia-isaac-ros.github.io/",
            "https://docs.omniverse.nvidia.com/kit/docs/",
            "https://docs.omniverse.nvidia.com/usd/",
            "https://docs.omniverse.nvidia.com/materials-and-rendering/",
            "https://docs.omniverse.nvidia.com/extensions/"
        ]
        
        # å¯èƒ½çš„ç‰ˆæœ¬å·
        version_patterns = [
            "latest",
            "2023.1.1",
            "2024.1.0", 
            "4.0.0",
            "104.0",
            "105.0"
        ]
        
        # æ„å»ºè¦æµ‹è¯•çš„URLåˆ—è¡¨
        urls_to_test = []
        
        # æµ‹è¯•åŸºç¡€URL
        for base_url in base_urls_to_test:
            urls_to_test.append(base_url)
            
            # æµ‹è¯•å¸¦ç‰ˆæœ¬çš„URL
            for version in version_patterns:
                versioned_url = urljoin(base_url, version + "/")
                urls_to_test.append(versioned_url)
                
                # æµ‹è¯•å¸¸è§é¡µé¢
                common_pages = [
                    "index.html",
                    "installation.html", 
                    "getting_started.html",
                    "tutorial_intro.html",
                    "tutorials/index.html",
                    "api/index.html",
                    "guide/index.html"
                ]
                
                for page in common_pages:
                    page_url = urljoin(versioned_url, page)
                    urls_to_test.append(page_url)
        
        print(f"ğŸ§ª æµ‹è¯• {len(urls_to_test)} ä¸ªå¯èƒ½çš„URL...")
        
        # å¹¶å‘æ£€æŸ¥URL
        timeout = aiohttp.ClientTimeout(total=15)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = [self.check_url_exists(session, url) for url in urls_to_test]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†æç»“æœ
        for result in results:
            if isinstance(result, dict):
                if result['valid']:
                    self.valid_urls.append(result)
                    # æå–URLæ¨¡å¼
                    parsed = urlparse(result['url'])
                    pattern = f"{parsed.netloc}{parsed.path}"
                    self.discovered_patterns.add(pattern)
                else:
                    self.invalid_urls.append(result)
        
        print(f"âœ… å‘ç° {len(self.valid_urls)} ä¸ªæœ‰æ•ˆURL")
        print(f"âŒ æ— æ•ˆURL: {len(self.invalid_urls)} ä¸ª")
        
        return self.valid_urls
    
    async def analyze_documentation_sites(self):
        """åˆ†æä¸»è¦æ–‡æ¡£ç«™ç‚¹çš„ç»“æ„"""
        print("\nğŸŒ åˆ†æä¸»è¦æ–‡æ¡£ç«™ç‚¹...")
        
        # ä¸»è¦ç«™ç‚¹åˆ†æ
        main_sites = [
            {
                "name": "Omniverse Isaac Sim",
                "base": "https://docs.omniverse.nvidia.com/",
                "patterns": ["isaacsim", "app_isaacsim", "py/isaacsim"]
            },
            {
                "name": "Isaac Lab", 
                "base": "https://isaac-sim.github.io/",
                "patterns": ["IsaacLab", "IsaacLab/main"]
            },
            {
                "name": "Isaac ROS",
                "base": "https://nvidia-isaac-ros.github.io/",
                "patterns": ["", "getting_started", "repositories_and_packages"]
            },
            {
                "name": "Omniverse Kit",
                "base": "https://docs.omniverse.nvidia.com/kit/docs/",
                "patterns": ["kit-manual", "omni_physics", "carbonite"]
            }
        ]
        
        site_analysis = {}
        
        for site in main_sites:
            print(f"  ğŸ” åˆ†æ {site['name']}...")
            site_valid_urls = []
            
            for pattern in site['patterns']:
                test_url = urljoin(site['base'], pattern + "/")
                
                timeout = aiohttp.ClientTimeout(total=10)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    result = await self.check_url_exists(session, test_url)
                    if result['valid']:
                        site_valid_urls.append(result)
                        print(f"    âœ… {test_url}")
                    else:
                        print(f"    âŒ {test_url} ({result.get('status', 'error')})")
            
            site_analysis[site['name']] = site_valid_urls
        
        return site_analysis
    
    def generate_corrected_urls(self):
        """åŸºäºå‘ç°çš„æœ‰æ•ˆURLç”Ÿæˆæ›´æ­£çš„ä¸‹è½½åˆ—è¡¨"""
        print("\nğŸ¯ ç”Ÿæˆæ›´æ­£çš„ä¸‹è½½URLåˆ—è¡¨...")
        
        # åŸºäºæœ‰æ•ˆURLç”Ÿæˆæ‰©å±•åˆ—è¡¨
        corrected_urls = []
        
        for url_info in self.valid_urls:
            base_url = url_info['url']
            
            # å¦‚æœæ˜¯ç›®å½•é¡µé¢ï¼Œå°è¯•æ·»åŠ å¸¸è§å­é¡µé¢
            if base_url.endswith('/'):
                common_subpages = [
                    "index.html",
                    "installation.html",
                    "getting_started.html", 
                    "tutorial_intro.html",
                    "tutorials/index.html",
                    "api/index.html",
                    "reference/index.html",
                    "guide/index.html",
                    "examples/index.html"
                ]
                
                for subpage in common_subpages:
                    full_url = urljoin(base_url, subpage)
                    corrected_urls.append({
                        "url": full_url,
                        "source": "generated_from_valid_base",
                        "base": base_url
                    })
            else:
                corrected_urls.append({
                    "url": base_url,
                    "source": "directly_validated",
                    "base": base_url
                })
        
        return corrected_urls
    
    async def main_analysis(self):
        """ä¸»åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹Isaacæ–‡æ¡£URLæ·±åº¦åˆ†æ...")
        
        # 1. å‘ç°æ–‡æ¡£ç»“æ„
        valid_urls = await self.discover_isaac_documentation_structure()
        
        # 2. åˆ†æä¸»è¦ç«™ç‚¹
        site_analysis = await self.analyze_documentation_sites()
        
        # 3. ç”Ÿæˆæ›´æ­£çš„URLåˆ—è¡¨
        corrected_urls = self.generate_corrected_urls()
        
        # 4. æ€»ç»“å‘ç°
        print(f"\nğŸ“Š åˆ†ææ€»ç»“:")
        print(f"  ğŸ” æµ‹è¯•çš„URLæ€»æ•°: {len(self.valid_urls) + len(self.invalid_urls)}")
        print(f"  âœ… æœ‰æ•ˆURL: {len(self.valid_urls)}")
        print(f"  âŒ æ— æ•ˆURL: {len(self.invalid_urls)}")
        print(f"  ğŸ¯ ç”Ÿæˆçš„ä¸‹è½½å€™é€‰: {len(corrected_urls)}")
        
        # 5. æŒ‰ç«™ç‚¹åˆ†ç»„æ˜¾ç¤ºæœ‰æ•ˆURL
        print(f"\nğŸŒ æŒ‰ç«™ç‚¹åˆ†ç»„çš„æœ‰æ•ˆURL:")
        site_groups = {}
        for url_info in self.valid_urls:
            domain = urlparse(url_info['url']).netloc
            if domain not in site_groups:
                site_groups[domain] = []
            site_groups[domain].append(url_info['url'])
        
        for domain, urls in site_groups.items():
            print(f"  ğŸ“ {domain}: {len(urls)} ä¸ªæœ‰æ•ˆURL")
            for url in urls[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"    - {url}")
            if len(urls) > 5:
                print(f"    ... è¿˜æœ‰ {len(urls) - 5} ä¸ª")
        
        # 6. ä¿å­˜ç»“æœ
        report = {
            "åˆ†ææ—¶é—´": "2025-09-09",
            "æœ‰æ•ˆURLæ€»æ•°": len(self.valid_urls),
            "æ— æ•ˆURLæ€»æ•°": len(self.invalid_urls),
            "æœ‰æ•ˆURLåˆ—è¡¨": [url['url'] for url in self.valid_urls],
            "ç«™ç‚¹åˆ†æ": {domain: urls for domain, urls in site_groups.items()},
            "æ¨èä¸‹è½½URL": [item['url'] for item in corrected_urls[:50]]  # å‰50ä¸ª
        }
        
        report_file = Path("isaac_url_analysis_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
        # 7. ç”Ÿæˆå»ºè®®
        print(f"\nğŸ’¡ ä¸‹è½½å»ºè®®:")
        print(f"  1. ä¼˜å…ˆä¸‹è½½å·²éªŒè¯çš„ {len(self.valid_urls)} ä¸ªæœ‰æ•ˆURL")
        print(f"  2. Isaac Simå®˜æ–¹æ–‡æ¡£å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„URLç»“æ„")
        print(f"  3. é‡ç‚¹å…³æ³¨ isaac-sim.github.io å’Œ nvidia-isaac-ros.github.io")
        print(f"  4. é¿å…ä½¿ç”¨ /latest/ è·¯å¾„ï¼Œå¾ˆå¤šè¿”å›404")
        
        return corrected_urls

async def main():
    discoverer = IsaacURLDiscoverer()
    corrected_urls = await discoverer.main_analysis()
    
    print(f"\nğŸ¯ å¯ä»¥ç”¨è¿™äº›éªŒè¯è¿‡çš„URLåˆ›å»ºæ–°çš„ä¸‹è½½å™¨!")

if __name__ == "__main__":
    asyncio.run(main())
