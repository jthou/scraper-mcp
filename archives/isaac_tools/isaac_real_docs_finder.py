#!/usr/bin/env python3
"""
æ‰¾åˆ°Isaac SimçœŸæ­£çš„æ–‡æ¡£URL
é€šè¿‡çˆ¬å–å®é™…å¯è®¿é—®çš„é¡µé¢æ¥å‘ç°æ­£ç¡®çš„é“¾æ¥
"""

import asyncio
import aiohttp
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import re

async def discover_real_isaac_docs():
    """å‘ç°çœŸæ­£å¯è®¿é—®çš„Isaacæ–‡æ¡£"""
    
    print("ğŸ” å¯»æ‰¾Isaac SimçœŸæ­£çš„æ–‡æ¡£ç»“æ„...")
    
    # å°è¯•ä»å·²çŸ¥å¯è®¿é—®çš„é¡µé¢å¼€å§‹
    entry_points = [
        "https://docs.omniverse.nvidia.com/",
        "https://docs.omniverse.nvidia.com/isaacsim/",
        "https://isaac-sim.github.io/IsaacLab/",
        "https://developer.nvidia.com/isaac-sim"
    ]
    
    valid_links = set()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        
        for entry_point in entry_points:
            try:
                print(f"ğŸŒ æ£€æŸ¥å…¥å£: {entry_point}")
                response = await page.goto(entry_point, timeout=15000)
                
                if response and response.status < 400:
                    print(f"âœ… å¯è®¿é—®: {entry_point}")
                    
                    # è·å–é¡µé¢å†…å®¹
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    links = soup.find_all('a', href=True)
                    
                    for link in links:
                        href = link['href']
                        full_url = urljoin(entry_point, href)
                        
                        # è¿‡æ»¤Isaacç›¸å…³çš„é“¾æ¥
                        if any(keyword in full_url.lower() for keyword in [
                            'isaac', 'simulation', 'robot', 'omniverse', 
                            'install', 'tutorial', 'guide', 'api', 'doc'
                        ]):
                            valid_links.add(full_url)
                            
                    print(f"  ğŸ“‹ å‘ç° {len([l for l in valid_links if entry_point in l])} ä¸ªç›¸å…³é“¾æ¥")
                
                else:
                    print(f"âŒ æ— æ³•è®¿é—®: {entry_point}")
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯ {entry_point}: {e}")
        
        await browser.close()
    
    # è¿‡æ»¤å’Œæ¸…ç†é“¾æ¥
    isaac_links = []
    for link in valid_links:
        # æ’é™¤ä¸ç›¸å…³çš„é“¾æ¥
        if any(exclude in link for exclude in [
            'javascript:', 'mailto:', '#', 'github.com/NVIDIA', 
            '.zip', '.tar.gz', 'download'
        ]):
            continue
            
        # åªä¿ç•™æ–‡æ¡£ç±»é“¾æ¥
        if any(keyword in link.lower() for keyword in [
            'doc', 'tutorial', 'guide', 'api', 'install', 'setup'
        ]):
            isaac_links.append(link)
    
    print(f"\nğŸ“Š æ€»å…±å‘ç° {len(isaac_links)} ä¸ªæ½œåœ¨çš„Isaacæ–‡æ¡£é“¾æ¥")
    
    # æŒ‰åŸŸååˆ†ç»„
    by_domain = {}
    for link in isaac_links:
        domain = urlparse(link).netloc
        if domain not in by_domain:
            by_domain[domain] = []
        by_domain[domain].append(link)
    
    print(f"\nğŸŒ æŒ‰åŸŸååˆ†ç»„:")
    for domain, links in by_domain.items():
        print(f"  ğŸ“ {domain}: {len(links)} ä¸ªé“¾æ¥")
        for link in links[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"    - {link}")
        if len(links) > 3:
            print(f"    ... è¿˜æœ‰ {len(links) - 3} ä¸ª")
    
    # ä¿å­˜å‘ç°çš„é“¾æ¥
    discovery_report = {
        "å‘ç°æ—¶é—´": "2025-09-09",
        "æ€»é“¾æ¥æ•°": len(isaac_links),
        "æŒ‰åŸŸååˆ†ç»„": by_domain,
        "æ‰€æœ‰é“¾æ¥": isaac_links
    }
    
    with open("isaac_real_docs_discovery.json", 'w', encoding='utf-8') as f:
        json.dump(discovery_report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å‘ç°æŠ¥å‘Šä¿å­˜è‡³: isaac_real_docs_discovery.json")
    
    return isaac_links

async def verify_discovered_links(isaac_links):
    """éªŒè¯å‘ç°çš„é“¾æ¥æ˜¯å¦çœŸçš„å¯è®¿é—®"""
    
    print(f"\nğŸ§ª éªŒè¯ {len(isaac_links)} ä¸ªå‘ç°çš„é“¾æ¥...")
    
    async def check_link(session, url):
        try:
            async with session.head(url, timeout=10, allow_redirects=True) as response:
                return {
                    "url": url,
                    "status": response.status,
                    "valid": 200 <= response.status < 400
                }
        except:
            return {"url": url, "status": "error", "valid": False}
    
    timeout = aiohttp.ClientTimeout(total=15)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        tasks = [check_link(session, url) for url in isaac_links[:50]]  # åªéªŒè¯å‰50ä¸ª
        results = await asyncio.gather(*tasks)
    
    valid_urls = [r for r in results if r["valid"]]
    invalid_urls = [r for r in results if not r["valid"]]
    
    print(f"âœ… æœ‰æ•ˆé“¾æ¥: {len(valid_urls)}")
    print(f"âŒ æ— æ•ˆé“¾æ¥: {len(invalid_urls)}")
    
    # æ˜¾ç¤ºæœ‰æ•ˆé“¾æ¥
    print(f"\nğŸ“‹ æœ‰æ•ˆçš„Isaacæ–‡æ¡£é“¾æ¥:")
    for result in valid_urls:
        print(f"  âœ… {result['url']}")
    
    return [r["url"] for r in valid_urls]

async def main():
    # 1. å‘ç°çœŸå®çš„Isaacæ–‡æ¡£é“¾æ¥
    isaac_links = await discover_real_isaac_docs()
    
    # 2. éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
    if isaac_links:
        valid_urls = await verify_discovered_links(isaac_links)
        
        print(f"\nğŸ¯ å»ºè®®:")
        print(f"  1. ä½¿ç”¨è¿™ {len(valid_urls)} ä¸ªéªŒè¯è¿‡çš„URL")
        print(f"  2. Isaac Simå¯èƒ½å·²ç»è¿ç§»äº†æ–‡æ¡£ç»“æ„")
        print(f"  3. é‡ç‚¹å…³æ³¨Isaac Labå’ŒIsaac ROSæ–‡æ¡£")
        
        return valid_urls
    else:
        print("âŒ æœªå‘ç°æœ‰æ•ˆçš„Isaacæ–‡æ¡£é“¾æ¥")
        return []

if __name__ == "__main__":
    asyncio.run(main())
