#!/usr/bin/env python3
"""
NVIDIA RTX 5080 æ˜¾å¡é—®é¢˜å’Œé¿å‘æŒ‡å—æœç´¢è„šæœ¬

æœç´¢çŸ¥ä¹ä¸Šå…³äº NVIDIA RTX 5080 æ˜¾å¡çš„ç›¸å…³é—®é¢˜å’Œä½¿ç”¨æ³¨æ„äº‹é¡¹
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, Any, List

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path('data/rtx5080')
LINKS_JSON = OUTPUT_DIR / 'links.json'
STATE_JSON = OUTPUT_DIR / 'state.json'


async def wait_until_zhihu_logged_in(toolkit: ScraperToolkit):
    """ç­‰å¾…çŸ¥ä¹ç™»å½•å®Œæˆ"""
    print('ç­‰å¾…ä½ åœ¨æµè§ˆå™¨ä¸­å®ŒæˆçŸ¥ä¹ç™»å½•ï¼ˆæ— é™ç­‰å¾…ï¼‰ï¼Œå®Œæˆåæˆ‘æ‰ä¼šç»§ç»­...')
    
    # æ‰“å¼€çŸ¥ä¹é¦–é¡µ
    page = toolkit.web_scraper.zhihu_page
    await page.goto('https://www.zhihu.com')
    await page.wait_for_load_state('networkidle')
    
    # å¾ªç¯æ£€æŸ¥ç™»å½•çŠ¶æ€
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™»å½•é¡µé¢
            current_url = page.url
            if "login" in current_url.lower() or "signin" in current_url.lower():
                print('æ£€æµ‹åˆ°ç™»å½•é¡µé¢ï¼Œè¯·å®Œæˆç™»å½•...')
                await page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•çŠ¶æ€ï¼‰
            login_button = await page.query_selector('button:has-text("ç™»å½•"), .SignFlow-tab, [data-testid*="login"]')
            if login_button:
                print('æ£€æµ‹åˆ°ç™»å½•æŒ‰é’®ï¼Œè¯·å®Œæˆç™»å½•...')
                await page.wait_for_timeout(3000)
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒæˆ–ç”¨æˆ·åï¼ˆå·²ç™»å½•çŠ¶æ€ï¼‰
            user_avatar = await page.query_selector('.AppHeader-userInfo, .UserAvatar, [data-testid*="user"]')
            if user_avatar:
                print('æ£€æµ‹åˆ°ç”¨æˆ·ä¿¡æ¯ï¼Œç™»å½•æˆåŠŸï¼')
                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢æ¡†ï¼ˆç™»å½•åçš„é¡µé¢ç‰¹å¾ï¼‰
            search_box = await page.query_selector('.SearchBar-input, input[placeholder*="æœç´¢"]')
            if search_box:
                print('æ£€æµ‹åˆ°æœç´¢åŠŸèƒ½ï¼Œç™»å½•æˆåŠŸï¼')
                break
                
            print('ç­‰å¾…ç™»å½•å®Œæˆ...')
            await page.wait_for_timeout(2000)  # ç­‰å¾…2ç§’åå†æ¬¡æ£€æŸ¥
            
        except Exception as e:
            print(f'æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}')
            await page.wait_for_timeout(2000)
    
    print('çŸ¥ä¹ç™»å½•æ£€æµ‹å®Œæˆï¼Œå¼€å§‹æœç´¢...')


async def collect_all_links(toolkit: ScraperToolkit) -> List[str]:
    """æ”¶é›†æ‰€æœ‰å…³äºRTX 5080çš„é“¾æ¥"""
    print('2) æ”¶é›†å…³äºNVIDIA RTX 5080çš„ç›¸å…³é“¾æ¥...')
    all_links: List[str] = []
    
    # æœç´¢å…³é”®è¯
    queries = [
        "NVIDIA RTX 5080 é—®é¢˜",
        "RTX 5080 ä½¿ç”¨æŠ€å·§",
        "RTX 5080 é¿å‘",
        "RTX 5080 æ•…éšœ",
        "RTX 5080 å…¼å®¹æ€§",
        "RTX 5080 æ€§èƒ½é—®é¢˜",
        "RTX 5080 æ¸¸æˆé—®é¢˜"
    ]
    
    for i, query in enumerate(queries, 1):
        print(f'  [{i}/{len(queries)}] æœç´¢: {query}')
        try:
            res = await toolkit.search(Platform.ZHIHU, query, max_pages=3)  # æ¯ä¸ªå…³é”®è¯æœç´¢3é¡µ
            if res.get('status') == 'success':
                links = res.get('all_links') or [item.get('url') for item in (res.get('results') or []) if item.get('url')]
                links = [l for l in links if l]
                print(f'    å–å¾— {len(links)} æ¡é“¾æ¥')
                all_links.extend(links)
            else:
                print(f'    æœç´¢å¤±è´¥: {res.get("message", "æœªçŸ¥é”™è¯¯")}')
        except Exception as e:
            print(f'    æœç´¢å¼‚å¸¸: {e}')
        
        # æœç´¢é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(2)
    
    # å»é‡
    unique_links = list(dict.fromkeys(all_links))
    print(f'  æ”¶é›†åˆè®¡ {len(all_links)} æ¡ï¼Œå»é‡å {len(unique_links)} æ¡')
    return unique_links


def load_json(path: Path, default):
    """åŠ è½½JSONæ–‡ä»¶"""
    if path.exists():
        try:
            return json.loads(path.read_text(encoding='utf-8'))
        except Exception:
            return default
    return default


def save_json(path: Path, data: Any):
    """ä¿å­˜JSONæ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')


def load_downloaded_urls(output_dir: Path) -> set:
    """ä»å·²æœ‰çš„ä¸‹è½½æ–‡ä»¶ä¸­æå–URL"""
    downloaded = set()
    if output_dir.exists():
        for json_file in output_dir.rglob("*.json"):
            try:
                data = json.loads(json_file.read_text(encoding='utf-8'))
                if 'url' in data:
                    downloaded.add(data['url'])
            except Exception:
                pass
    return downloaded


async def run_zhihu_rtx5080_search():
    """è¿è¡ŒçŸ¥ä¹RTX 5080ç›¸å…³å†…å®¹æœç´¢å’Œä¸‹è½½"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    config = ScrapingConfig(
        platform=Platform.ZHIHU,
        headless=False,
        max_pages=3,
        output_dir=OUTPUT_DIR,
        wait_for_verification=True,
    )
    toolkit = ScraperToolkit(config)
    
    try:
        print('1) è®¾ç½®æµè§ˆå™¨å¹¶ç™»å½•çŸ¥ä¹...')
        # ä½¿ç”¨ScraperToolkitçš„setup_browseræ–¹æ³•æ¥æ­£ç¡®åˆå§‹åŒ–æµè§ˆå™¨
        setup_result = await toolkit.setup_browser(Platform.ZHIHU, headless=False)
        if setup_result["status"] != "success":
            print(f"âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥: {setup_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return
        print("âœ… æµè§ˆå™¨è®¾ç½®æˆåŠŸ")
        
        print('2) ç­‰å¾…çŸ¥ä¹ç™»å½•å®Œæˆ...')
        await wait_until_zhihu_logged_in(toolkit)
        
        # è¯»å–æˆ–ç”Ÿæˆé“¾æ¥æ¸…å•
        if LINKS_JSON.exists():
            print('3) å‘ç° links.jsonï¼Œç›´æ¥è¯»å–...')
            links: List[str] = load_json(LINKS_JSON, [])
        else:
            links = await collect_all_links(toolkit)
            save_json(LINKS_JSON, links)
            print(f'  å·²ä¿å­˜é“¾æ¥æ¸…å•ï¼š{LINKS_JSON}')
        
        # è¯»å–æˆ–åˆå§‹åŒ–çŠ¶æ€
        state = load_json(STATE_JSON, {
            'completed': [],
            'failed': [],
            'index': 0,
            'total': len(links),
        })
        print(f"4) è¯»å–è¿›åº¦ï¼šindex={state.get('index',0)}/{state.get('total',len(links))}, completed={len(state.get('completed',[]))}, failed={len(state.get('failed',[]))}")
        
        # è¯»å–å·²ä¸‹è½½URLé›†åˆ
        downloaded_urls = load_downloaded_urls(OUTPUT_DIR)
        if downloaded_urls:
            print(f'   å‘ç°å·²ä¸‹è½½ {len(downloaded_urls)} ç¯‡ï¼Œå°†è‡ªåŠ¨è·³è¿‡åŒ¹é…é“¾æ¥')
        
        # ä¸²è¡Œä¸‹è½½
        print('5) ä¸²è¡Œä¸‹è½½RTX 5080ç›¸å…³æ–‡ç« ...')
        for i in range(state.get('index', 0), len(links)):
            link = links[i]
            
            # è·³è¿‡å·²ä¸‹è½½çš„é“¾æ¥
            if link in downloaded_urls or link in state.get('completed', []):
                print(f'  [{i+1}/{len(links)}] è·³è¿‡ï¼ˆå·²ä¸‹è½½ï¼‰: {link}')
                continue
            
            print(f'  [{i+1}/{len(links)}] ä¸‹è½½: {link}')
            try:
                r = await toolkit.download_content(Platform.ZHIHU, link, OUTPUT_DIR)
                if r.get('status') == 'success':
                    files = r.get('files', {})
                    print('     âœ… æˆåŠŸ')
                    print(f"       PDF: {files.get('pdf')}")
                    print(f"       MD : {files.get('markdown')}")
                    state.setdefault('completed', []).append(link)
                else:
                    print(f"     âŒ å¤±è´¥: {r.get('message')}")
                    state.setdefault('failed', []).append({'link': link, 'message': r.get('message')})
                
                # æ›´æ–°ç´¢å¼•å¹¶ä¿å­˜çŠ¶æ€
                state['index'] = i + 1
                save_json(STATE_JSON, state)
                
                # ä¸‹è½½é—´éš”
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f'     ğŸ’¥ å¼‚å¸¸: {e}')
                state.setdefault('failed', []).append({'link': link, 'error': str(e)})
                state['index'] = i + 1
                save_json(STATE_JSON, state)
        
        print('\n6) å®Œæˆç»Ÿè®¡ï¼š')
        print(f"  æˆåŠŸ: {len(state.get('completed', []))}")
        print(f"  å¤±è´¥: {len(state.get('failed', []))}")
        print(f"  æ€»æ•°: {len(links)}")
        print(f"  é“¾æ¥æ¸…å•: {LINKS_JSON}")
        print(f"  è¿›åº¦æ–‡ä»¶: {STATE_JSON}")
        print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
    finally:
        await toolkit.cleanup()


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await run_zhihu_rtx5080_search()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())