#!/usr/bin/env python3
"""
å¾®ä¿¡ Isaac Sim å…¨é‡æŠ“å–æµæ°´çº¿ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰

åŠŸèƒ½ï¼š
- å¤šå…³é”®è¯å…¨é‡æœç´¢ â†’ åˆå¹¶å»é‡ â†’ ä¸²è¡Œé€ä¸ªä¸‹è½½
- æ–­ç‚¹ç»­è·‘ï¼šlinks.json ä¿å­˜é“¾æ¥ã€state.json ä¿å­˜è¿›åº¦ï¼ˆcompleted/failed/indexï¼‰
- ä¸‹è½½è¿‡ç¨‹ä¸­å¦‚é‡éªŒè¯ç ï¼Œç­‰å¾…äººå·¥éªŒè¯å®Œæˆåç»§ç»­

è¿è¡Œï¼š
  python examples/wechat_isaac_pipeline.py
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, Any, List

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform


QUERIES: List[str] = [
    'Isaac Sim',
    'NVIDIA Isaac Sim',
    'è‹±ä¼Ÿè¾¾ Isaac Sim',
    'Isaac ä»¿çœŸ',
    'è‹±ä¼Ÿè¾¾ ä»¿çœŸ å¹³å° Isaac',
]

OUTPUT_DIR = Path('data/wechat_isaac_all')
LINKS_JSON = OUTPUT_DIR / 'links.json'
STATE_JSON = OUTPUT_DIR / 'state.json'


def load_json(path: Path, default):
    if path.exists():
        try:
            return json.loads(path.read_text(encoding='utf-8'))
        except Exception:
            return default
    return default


def save_json(path: Path, data: Any):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')


async def collect_all_links(toolkit: ScraperToolkit) -> List[str]:
    print('2) æ”¶é›†é“¾æ¥ï¼ˆå…¨é‡ï¼Œå¤šå…³é”®è¯ï¼‰...')
    all_links: List[str] = []
    for q in QUERIES:
        print(f'  - æœç´¢: {q}')
        res = await toolkit.search(Platform.WECHAT, q, max_pages=0)
        links = res.get('all_links') or [item.get('link') for item in (res.get('results') or []) if item.get('link')]
        links = [l for l in links if l]
        print(f'    å–å¾— {len(links)} æ¡é“¾æ¥')
        all_links.extend(links)
    unique_links = list(dict.fromkeys(all_links))
    print(f'  æ”¶é›†åˆè®¡ {len(all_links)} æ¡ï¼Œå»é‡å {len(unique_links)} æ¡')
    return unique_links


async def run_pipeline():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    config = ScrapingConfig(
        platform=Platform.WECHAT,
        headless=False,
        max_pages=0,  # å…¨éƒ¨é¡µé¢
        output_dir=OUTPUT_DIR,
        wait_for_verification=True,
    )
    toolkit = ScraperToolkit(config)

    try:
        print('1) è®¾ç½®æµè§ˆå™¨...')
        await toolkit.setup_browser(Platform.WECHAT)

        # è¯»å–æˆ–ç”Ÿæˆé“¾æ¥æ¸…å•
        if LINKS_JSON.exists():
            print('2) å‘ç° links.jsonï¼Œç›´æ¥è¯»å–...')
            links: List[str] = load_json(LINKS_JSON, [])
        else:
            links = await collect_all_links(toolkit)
            save_json(LINKS_JSON, links)
            print(f'  å·²ä¿å­˜é“¾æ¥æ¸…å•ï¼š{LINKS_JSON}')

        # è¯»å–æˆ–åˆå§‹åŒ–çŠ¶æ€
        state = load_json(STATE_JSON, {
            'completed': [],
            'failed': [],
            'index': 0,
            'total': len(links),
        })
        print(f"3) è¯»å–è¿›åº¦ï¼šindex={state.get('index',0)}/{state.get('total',len(links))}, completed={len(state.get('completed',[]))}, failed={len(state.get('failed',[]))}")

        # ä¸²è¡Œä¸‹è½½ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰
        print('4) ä¸²è¡Œä¸‹è½½ï¼Œé‡åˆ°éªŒè¯ç å°†ç­‰å¾…äººå·¥éªŒè¯...')
        for i in range(state.get('index', 0), len(links)):
            link = links[i]
            if link in state.get('completed', []):
                print(f'  [{i+1}/{len(links)}] è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰: {link}')
                continue

            print(f'  [{i+1}/{len(links)}] ä¸‹è½½: {link}')
            try:
                r = await toolkit.download_content(Platform.WECHAT, link, OUTPUT_DIR)
                if r.get('status') == 'success':
                    files = r.get('files', {})
                    print('     âœ… æˆåŠŸ')
                    print(f"       PDF: {files.get('pdf')}")
                    print(f"       MD : {files.get('markdown')}")
                    state.setdefault('completed', []).append(link)
                else:
                    print(f"     âŒ å¤±è´¥: {r.get('message')}")
                    state.setdefault('failed', []).append({'link': link, 'message': r.get('message')})
                # æ›´æ–°ç´¢å¼•å¹¶ä¿å­˜çŠ¶æ€
                state['index'] = i + 1
                save_json(STATE_JSON, state)
            except Exception as e:
                print(f'     ğŸ’¥ å¼‚å¸¸: {e}')
                state.setdefault('failed', []).append({'link': link, 'error': str(e)})
                state['index'] = i + 1
                save_json(STATE_JSON, state)

        print('\n5) å®Œæˆç»Ÿè®¡ï¼š')
        print(f"  æˆåŠŸ: {len(state.get('completed', []))}")
        print(f"  å¤±è´¥: {len(state.get('failed', []))}")
        print(f"  æ€»æ•°: {len(links)}")
        print(f"  é“¾æ¥æ¸…å•: {LINKS_JSON}")
        print(f"  è¿›åº¦æ–‡ä»¶: {STATE_JSON}")

    finally:
        await toolkit.cleanup()


if __name__ == '__main__':
    asyncio.run(run_pipeline())


