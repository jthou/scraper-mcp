#!/usr/bin/env python3
"""
Isaac Sim æ— é™åˆ¶å®Œæ•´ä¸‹è½½å™¨
ä¸‹è½½æ‰€æœ‰å¯èƒ½æ‰¾åˆ°çš„Isaac Simç›¸å…³æ–‡æ¡£ï¼Œä¸è®¾é™åˆ¶
"""

import asyncio
import json
import aiohttp
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import time
import hashlib
import re
from datetime import datetime

class IsaacUnlimitedDownloader:
    def __init__(self, output_dir="isaac_unlimited_downloads"):
        self.output_dir = Path(output_dir)
        self.pdfs_dir = self.output_dir / "pdfs"
        self.logs_dir = self.output_dir / "logs"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        for dir_path in [self.pdfs_dir, self.logs_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # çŠ¶æ€è¿½è¸ª
        self.discovered_urls = set()
        self.visited_urls = set()
        self.downloaded_urls = set()
        self.failed_urls = set()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'å‘ç°': 0,
            'è®¿é—®': 0,
            'æˆåŠŸ': 0,
            'å¤±è´¥': 0,
            'è·³è¿‡': 0
        }
        
        # æ‰€æœ‰å¯èƒ½çš„Isaacç›¸å…³åŸŸåå’Œè·¯å¾„
        self.isaac_domains = [
            'isaac-sim.github.io',
            'docs.omniverse.nvidia.com',
            'docs.nvidia.com',
            'developer.nvidia.com'
        ]
        
        # ç§å­URL - æ‰©å±•ç‰ˆæœ¬
        self.seed_urls = [
            # Isaac Lab
            "https://isaac-sim.github.io/IsaacLab/",
            "https://isaac-sim.github.io/IsaacLab/main/",
            "https://isaac-sim.github.io/IsaacLab/main/source/api/index.html",
            "https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html",
            "https://isaac-sim.github.io/IsaacLab/main/source/setup/index.html",
            "https://isaac-sim.github.io/IsaacLab/main/source/features/index.html",
            
            # Omniverse Isaac Sim
            "https://docs.omniverse.nvidia.com/isaacsim/latest/",
            "https://docs.omniverse.nvidia.com/isaacsim/latest/installation/",
            "https://docs.omniverse.nvidia.com/isaacsim/latest/tutorial_intro.html",
            "https://docs.omniverse.nvidia.com/isaacsim/latest/features/",
            "https://docs.omniverse.nvidia.com/isaacsim/latest/api/",
            
            # NVIDIA Developer
            "https://developer.nvidia.com/isaac-sim",
            "https://docs.nvidia.com/isaac/",
        ]
        
        # å¹¶å‘æ§åˆ¶ - æ›´ç§¯æçš„è®¾ç½®
        self.discovery_semaphore = asyncio.Semaphore(10)  # å‘ç°é“¾æ¥çš„å¹¶å‘æ•°
        self.download_semaphore = asyncio.Semaphore(3)    # ä¸‹è½½çš„å¹¶å‘æ•°
        
        self.start_time = time.time()
        
    def is_isaac_related_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸Isaacç›¸å…³"""
        parsed = urlparse(url)
        
        # å¿…é¡»åœ¨ç›¸å…³åŸŸåå†…
        if parsed.netloc not in self.isaac_domains:
            return False
        
        # Isaacç›¸å…³å…³é”®è¯
        isaac_keywords = [
            'isaac', 'omniverse', 'simulation', 'robot', 'physics',
            'isaacsim', 'isaaclab', 'gym', 'rl', 'reinforcement'
        ]
        
        url_lower = url.lower()
        if not any(keyword in url_lower for keyword in isaac_keywords):
            return False
        
        # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹
        exclude_patterns = [
            r'\.(css|js|png|jpg|jpeg|gif|svg|ico|woff|ttf|eot)(\?|$)',
            r'\.(zip|tar|gz|pdf|mp4|mov|avi)(\?|$)',
            r'/_static/',
            r'/_sources/',
            r'/genindex',
            r'/search',
            r'#',
            'mailto:',
            'javascript:',
            'tel:',
            'ftp:',
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, url):
                return False
        
        return True
    
    async def discover_links_from_url(self, session, url):
        """ä»URLå‘ç°æ–°é“¾æ¥"""
        async with self.discovery_semaphore:
            if url in self.visited_urls:
                return []
            
            try:
                print(f"ğŸ” æ¢ç´¢: {url}")
                async with session.get(url, timeout=30) as response:
                    if response.status != 200:
                        return []
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    discovered = []
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        absolute_url = urljoin(url, href)
                        
                        if (self.is_isaac_related_url(absolute_url) and 
                            absolute_url not in self.discovered_urls):
                            discovered.append(absolute_url)
                            self.discovered_urls.add(absolute_url)
                    
                    # æŸ¥æ‰¾å¯èƒ½çš„APIæˆ–æ–‡æ¡£ç»“æ„
                    for link in soup.find_all(['link', 'script'], src=True):
                        src = link.get('src', '')
                        if src:
                            absolute_url = urljoin(url, src)
                            if (self.is_isaac_related_url(absolute_url) and 
                                absolute_url not in self.discovered_urls):
                                discovered.append(absolute_url)
                                self.discovered_urls.add(absolute_url)
                    
                    self.visited_urls.add(url)
                    self.stats['è®¿é—®'] += 1
                    self.stats['å‘ç°'] += len(discovered)
                    
                    if discovered:
                        print(f"ğŸ“ å‘ç° {len(discovered)} ä¸ªæ–°é“¾æ¥")
                    
                    return discovered
                    
            except Exception as e:
                print(f"âŒ æ¢ç´¢å¤±è´¥ {url}: {e}")
                return []
    
    async def download_page_to_pdf(self, browser, url):
        """å°†é¡µé¢ä¸‹è½½ä¸ºPDF"""
        async with self.download_semaphore:
            if url in self.downloaded_urls:
                self.stats['è·³è¿‡'] += 1
                return {"status": "è·³è¿‡", "url": url}
            
            try:
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                parsed = urlparse(url)
                
                # ä»URLè·¯å¾„ç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å
                path_parts = [p for p in parsed.path.split('/') if p and p != 'index.html']
                if path_parts:
                    name_part = '_'.join(path_parts[-3:])[:80]  # å–æœ€å3ä¸ªè·¯å¾„éƒ¨åˆ†
                else:
                    name_part = parsed.netloc.replace('.', '_')
                
                # æ¸…ç†æ–‡ä»¶å
                safe_name = re.sub(r'[^\w\-_]', '_', name_part)
                filename = f"isaac_{safe_name}_{url_hash}.pdf"
                pdf_path = self.pdfs_dir / filename
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if pdf_path.exists():
                    self.downloaded_urls.add(url)
                    self.stats['è·³è¿‡'] += 1
                    return {"status": "è·³è¿‡", "url": url, "path": pdf_path}
                
                print(f"ğŸ“¥ ä¸‹è½½: {url}")
                
                # åˆ›å»ºæ–°é¡µé¢
                page = await browser.new_page()
                
                # è®¾ç½®æ›´å¥½çš„ç”¨æˆ·ä»£ç†å’Œå¤´ä¿¡æ¯
                await page.set_extra_http_headers({
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                })
                
                # è®¾ç½®è§†å£å¤§å°
                await page.set_viewport_size({"width": 1920, "height": 1080})
                
                # å¯¼èˆªåˆ°é¡µé¢
                response = await page.goto(url, timeout=90000, wait_until='domcontentloaded')
                
                if not response or response.status >= 400:
                    raise Exception(f"HTTPé”™è¯¯: {response.status}")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await page.wait_for_timeout(5000)
                
                # å°è¯•ç­‰å¾…ä¸»è¦å†…å®¹
                try:
                    await page.wait_for_selector('body', timeout=10000)
                    # ç­‰å¾…å¯èƒ½çš„åŠ¨æ€å†…å®¹
                    await page.wait_for_timeout(3000)
                except:
                    pass
                
                # ç”Ÿæˆé«˜è´¨é‡PDF
                await page.pdf(
                    path=str(pdf_path),
                    format='A4',
                    print_background=True,
                    prefer_css_page_size=False,
                    margin={
                        'top': '20px',
                        'right': '20px', 
                        'bottom': '20px',
                        'left': '20px'
                    },
                    display_header_footer=False
                )
                
                await page.close()
                
                # éªŒè¯PDFæ–‡ä»¶
                if not pdf_path.exists() or pdf_path.stat().st_size < 2000:
                    if pdf_path.exists():
                        pdf_path.unlink()
                    raise Exception("PDFæ–‡ä»¶å¤ªå°æˆ–ç”Ÿæˆå¤±è´¥")
                
                self.downloaded_urls.add(url)
                self.stats['æˆåŠŸ'] += 1
                
                print(f"âœ… æˆåŠŸä¸‹è½½: {filename} ({pdf_path.stat().st_size / 1024:.1f} KB)")
                
                return {
                    "status": "æˆåŠŸ",
                    "url": url,
                    "filename": filename,
                    "path": str(pdf_path),
                    "size": pdf_path.stat().st_size,
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                if 'pdf_path' in locals() and pdf_path.exists():
                    pdf_path.unlink()
                
                self.failed_urls.add(url)
                self.stats['å¤±è´¥'] += 1
                
                print(f"âŒ ä¸‹è½½å¤±è´¥: {url} - {str(e)[:100]}")
                
                return {
                    "status": "å¤±è´¥",
                    "url": url,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
    
    async def unlimited_crawl_and_download(self):
        """æ— é™åˆ¶çˆ¬å–å’Œä¸‹è½½"""
        print(f"ğŸš€ å¼€å§‹æ— é™åˆ¶Isaacæ–‡æ¡£ä¸‹è½½!")
        print(f"ğŸ“‹ ç§å­URL: {len(self.seed_urls)} ä¸ª")
        
        # åˆå§‹åŒ–é˜Ÿåˆ—
        discovery_queue = list(self.seed_urls)
        download_queue = list(self.seed_urls)
        self.discovered_urls.update(self.seed_urls)
        
        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-extensions'
                ]
            )
            
            # å¯åŠ¨HTTPä¼šè¯
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                
                try:
                    round_number = 1
                    
                    while discovery_queue or download_queue:
                        print(f"\nğŸ”„ ç¬¬ {round_number} è½®å¤„ç†")
                        print(f"ğŸ“Š çŠ¶æ€: å‘ç° {len(self.discovered_urls)}, è®¿é—® {len(self.visited_urls)}, ä¸‹è½½ {len(self.downloaded_urls)}")
                        
                        # é˜¶æ®µ1: å¹¶å‘å‘ç°æ–°é“¾æ¥
                        if discovery_queue:
                            print(f"ğŸ” å‘ç°é˜¶æ®µ: å¤„ç† {len(discovery_queue)} ä¸ªURL")
                            current_discovery = discovery_queue[:20]  # æ¯è½®å¤„ç†20ä¸ª
                            discovery_queue = discovery_queue[20:]
                            
                            discovery_tasks = [
                                self.discover_links_from_url(session, url)
                                for url in current_discovery
                            ]
                            
                            discovery_results = await asyncio.gather(*discovery_tasks, return_exceptions=True)
                            
                            # æ”¶é›†æ–°å‘ç°çš„é“¾æ¥
                            for result in discovery_results:
                                if isinstance(result, list):
                                    discovery_queue.extend(result)
                                    download_queue.extend(result)
                        
                        # é˜¶æ®µ2: å¹¶å‘ä¸‹è½½é¡µé¢
                        if download_queue:
                            print(f"ğŸ“¥ ä¸‹è½½é˜¶æ®µ: å¤„ç† {min(10, len(download_queue))} ä¸ªURL")
                            current_downloads = download_queue[:10]  # æ¯è½®ä¸‹è½½10ä¸ª
                            download_queue = download_queue[10:]
                            
                            download_tasks = [
                                self.download_page_to_pdf(browser, url)
                                for url in current_downloads
                            ]
                            
                            download_results = await asyncio.gather(*download_tasks, return_exceptions=True)
                            
                            # æ˜¾ç¤ºä¸‹è½½ç»“æœ
                            for result in download_results:
                                if isinstance(result, dict) and result.get('status') == 'æˆåŠŸ':
                                    print(f"ğŸ“„ å·²ä¸‹è½½: {result['filename']}")
                        
                        # æ¯10è½®æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                        if round_number % 10 == 0:
                            self.print_detailed_stats()
                        
                        round_number += 1
                        
                        # çŸ­æš‚ä¼‘æ¯
                        await asyncio.sleep(2)
                        
                        # å®‰å…¨æ£€æŸ¥ï¼šé¿å…æ— é™å¾ªç¯
                        if round_number > 1000:
                            print("âš ï¸  è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶ï¼Œåœæ­¢å¤„ç†")
                            break
                
                finally:
                    await browser.close()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report()
    
    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        total_size = 0
        total_files = 0
        
        if self.pdfs_dir.exists():
            for pdf in self.pdfs_dir.glob("*.pdf"):
                total_size += pdf.stat().st_size
                total_files += 1
        
        elapsed = time.time() - self.start_time
        
        print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ (è¿è¡Œæ—¶é—´: {elapsed/60:.1f} åˆ†é’Ÿ):")
        print(f"  ğŸ”— å‘ç°URL: {len(self.discovered_urls)} ä¸ª")
        print(f"  ğŸ‘ï¸  è®¿é—®URL: {len(self.visited_urls)} ä¸ª")
        print(f"  âœ… æˆåŠŸä¸‹è½½: {self.stats['æˆåŠŸ']} ä¸ª")
        print(f"  âŒ ä¸‹è½½å¤±è´¥: {self.stats['å¤±è´¥']} ä¸ª")
        print(f"  â­ï¸  è·³è¿‡é‡å¤: {self.stats['è·³è¿‡']} ä¸ª")
        print(f"  ğŸ“ PDFæ–‡ä»¶: {total_files} ä¸ª")
        print(f"  ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f} MB")
        print(f"  ğŸ“ˆ ä¸‹è½½é€Ÿåº¦: {self.stats['æˆåŠŸ'] / max(elapsed/60, 1):.1f} æ–‡ä»¶/åˆ†é’Ÿ")
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        total_size = 0
        total_files = 0
        
        if self.pdfs_dir.exists():
            for pdf in self.pdfs_dir.glob("*.pdf"):
                total_size += pdf.stat().st_size
                total_files += 1
        
        elapsed = time.time() - self.start_time
        
        report = {
            "æ— é™åˆ¶ä¸‹è½½æ€»ç»“": {
                "è¿è¡Œæ—¶é—´åˆ†é’Ÿ": f"{elapsed/60:.1f}",
                "å‘ç°URLæ€»æ•°": len(self.discovered_urls),
                "è®¿é—®URLæ€»æ•°": len(self.visited_urls),
                "æˆåŠŸä¸‹è½½": self.stats['æˆåŠŸ'],
                "ä¸‹è½½å¤±è´¥": self.stats['å¤±è´¥'],
                "è·³è¿‡é‡å¤": self.stats['è·³è¿‡'],
                "PDFæ–‡ä»¶æ•°": total_files,
                "æ€»å¤§å°MB": f"{total_size / 1024 / 1024:.1f}",
                "ä¸‹è½½é€Ÿåº¦": f"{self.stats['æˆåŠŸ'] / max(elapsed/60, 1):.1f} æ–‡ä»¶/åˆ†é’Ÿ"
            },
            "æ–‡ä»¶ä½ç½®": {
                "PDFç›®å½•": str(self.pdfs_dir),
                "æ—¥å¿—ç›®å½•": str(self.logs_dir)
            },
            "å®Œæˆæ—¶é—´": datetime.now().isoformat(),
            "ç§å­URLs": self.seed_urls,
            "ç»Ÿè®¡è¯¦æƒ…": self.stats
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "unlimited_download_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜URLåˆ—è¡¨
        urls_file = self.output_dir / "all_discovered_urls.json"
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump({
                "discovered": list(self.discovered_urls),
                "visited": list(self.visited_urls),
                "downloaded": list(self.downloaded_urls),
                "failed": list(self.failed_urls)
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ æ— é™åˆ¶ä¸‹è½½å®Œæˆ!")
        self.print_detailed_stats()
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"ğŸ”— URLåˆ—è¡¨: {urls_file}")

async def main():
    """ä¸»å‡½æ•°"""
    downloader = IsaacUnlimitedDownloader()
    await downloader.unlimited_crawl_and_download()

if __name__ == "__main__":
    asyncio.run(main())
