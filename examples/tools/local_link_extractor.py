#!/usr/bin/env python3
"""
ä»å·²ä¸‹è½½çš„Isaac Labå†…å®¹ä¸­æå–æ‰€æœ‰é“¾æ¥
"""

import os
import re
import json
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

class LocalLinkExtractor:
    def __init__(self):
        self.isaac_downloads_dir = Path("isaac_sim_downloads")
        self.output_dir = Path("isaac_complete_links")
        self.output_dir.mkdir(exist_ok=True)
        
        self.all_links = set()
        self.page_info = []
    
    def extract_links_from_html_file(self, html_file):
        """ä»å•ä¸ªHTMLæ–‡ä»¶æå–é“¾æ¥"""
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # è·å–é¡µé¢æ ‡é¢˜
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            
            # æå–æ‰€æœ‰é“¾æ¥
            links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                text = a_tag.get_text(strip=True)
                
                # è½¬æ¢ä¸ºç»å¯¹URL
                if href.startswith('/'):
                    if 'isaac-sim.github.io' in str(html_file):
                        base_url = "https://isaac-sim.github.io"
                    elif 'leggedrobotics.github.io' in str(html_file):
                        base_url = "https://leggedrobotics.github.io"
                    else:
                        continue
                    absolute_url = base_url + href
                elif href.startswith('http'):
                    absolute_url = href
                else:
                    # ç›¸å¯¹é“¾æ¥
                    if 'isaac-sim.github.io' in str(html_file):
                        base_url = "https://isaac-sim.github.io/IsaacLab/"
                    elif 'leggedrobotics.github.io' in str(html_file):
                        base_url = "https://leggedrobotics.github.io/legged_gym/"
                    else:
                        continue
                    absolute_url = urljoin(base_url, href)
                
                # è¿‡æ»¤æœ‰æ•ˆçš„Isaacç›¸å…³é“¾æ¥
                if self.is_isaac_related_link(absolute_url):
                    links.append({
                        'url': absolute_url,
                        'text': text[:100],
                        'source_file': str(html_file)
                    })
                    self.all_links.add(absolute_url)
            
            page_info = {
                'file': str(html_file),
                'title': title,
                'links_count': len(links),
                'links': links
            }
            
            return page_info
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {html_file}: {str(e)}")
            return None
    
    def is_isaac_related_link(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºIsaacç›¸å…³çš„æœ‰æ•ˆé“¾æ¥"""
        parsed = urlparse(url)
        
        # å¿…é¡»æ˜¯ç›®æ ‡åŸŸå
        valid_domains = ['isaac-sim.github.io', 'leggedrobotics.github.io', 'zhengyiluo.github.io']
        if not any(domain in parsed.netloc for domain in valid_domains):
            return False
        
        # å¿…é¡»åŒ…å«Isaacç›¸å…³è·¯å¾„
        isaac_paths = ['/IsaacLab/', '/legged_gym/', '/PHC/']
        if not any(path in parsed.path for path in isaac_paths):
            return False
        
        # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹
        excluded_extensions = ['.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.zip', '.tar.gz']
        if any(parsed.path.lower().endswith(ext) for ext in excluded_extensions):
            return False
        
        # æ’é™¤é”šç‚¹é“¾æ¥
        if '#' in url and len(parsed.fragment) > 0:
            return False
        
        return True
    
    def scan_downloaded_content(self):
        """æ‰«æå·²ä¸‹è½½çš„å†…å®¹æå–é“¾æ¥"""
        print("ğŸ” æ‰«æå·²ä¸‹è½½çš„Isaac Simå†…å®¹...")
        
        html_files = []
        
        # æœç´¢æ‰€æœ‰å¯èƒ½çš„Isaacç›¸å…³HTMLæ–‡ä»¶
        search_patterns = [
            "./isaac_sim_downloads/**/isaac-sim.github.io/**/*.html",
            "./isaac_sim_downloads/**/leggedrobotics.github.io/**/*.html", 
            "./isaac_sim_downloads/**/zhengyiluo.github.io/**/*.html",
            "./**/isaac-sim.github.io/**/*.html",
            "./**/leggedrobotics.github.io/**/*.html",
            "./isaac-sim.github.io/**/*.html",
            "./leggedrobotics.github.io/**/*.html"
        ]
        
        for pattern in search_patterns:
            found_files = list(Path(".").glob(pattern))
            html_files.extend(found_files)
            if found_files:
                print(f"ğŸ“ åœ¨ {pattern} æ‰¾åˆ° {len(found_files)} ä¸ªæ–‡ä»¶")
        
        # å»é‡
        html_files = list(set(html_files))
        print(f"ğŸ“„ æ€»è®¡æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªHTMLæ–‡ä»¶
        for html_file in html_files:
            print(f"ğŸ”— æå–é“¾æ¥: {html_file.name}")
            page_info = self.extract_links_from_html_file(html_file)
            if page_info:
                self.page_info.append(page_info)
        
        return len(self.all_links)
    
    def categorize_links(self):
        """å¯¹é“¾æ¥è¿›è¡Œåˆ†ç±»"""
        categorized = {
            'installation': [],
            'tutorials': [],
            'api': [],
            'guides': [],
            'features': [],
            'examples': [],
            'other': []
        }
        
        for link in self.all_links:
            link_lower = link.lower()
            
            if any(keyword in link_lower for keyword in ['install', 'setup', 'quickstart']):
                categorized['installation'].append(link)
            elif any(keyword in link_lower for keyword in ['tutorial', 'walkthrough']):
                categorized['tutorials'].append(link)
            elif any(keyword in link_lower for keyword in ['api', 'reference']):
                categorized['api'].append(link)
            elif any(keyword in link_lower for keyword in ['guide', 'how-to']):
                categorized['guides'].append(link)
            elif any(keyword in link_lower for keyword in ['feature', 'overview']):
                categorized['features'].append(link)
            elif any(keyword in link_lower for keyword in ['example', 'demo']):
                categorized['examples'].append(link)
            else:
                categorized['other'].append(link)
        
        return categorized
    
    def generate_download_queue(self):
        """ç”Ÿæˆä¼˜å…ˆçº§ä¸‹è½½é˜Ÿåˆ—"""
        categorized = self.categorize_links()
        
        # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
        priority_order = ['installation', 'tutorials', 'guides', 'features', 'api', 'examples', 'other']
        
        download_queue = []
        
        for category in priority_order:
            links = categorized[category]
            for i, link in enumerate(links):
                download_queue.append({
                    'url': link,
                    'category': category,
                    'priority': len(priority_order) - priority_order.index(category),
                    'filename': f"{category}_{i+1:03d}"
                })
        
        return download_queue
    
    def save_results(self):
        """ä¿å­˜æå–ç»“æœ"""
        # ä¿å­˜æ‰€æœ‰é“¾æ¥
        all_links_file = self.output_dir / "all_isaac_links.json"
        with open(all_links_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.all_links), f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆ†ç±»é“¾æ¥
        categorized = self.categorize_links()
        categorized_file = self.output_dir / "categorized_links.json"
        with open(categorized_file, 'w', encoding='utf-8') as f:
            json.dump(categorized, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸‹è½½é˜Ÿåˆ—
        download_queue = self.generate_download_queue()
        queue_file = self.output_dir / "download_queue.json"
        with open(queue_file, 'w', encoding='utf-8') as f:
            json.dump(download_queue, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(categorized, download_queue)
        
        return {
            'all_links': len(self.all_links),
            'categorized': categorized,
            'download_queue': len(download_queue)
        }
    
    def generate_report(self, categorized, download_queue):
        """ç”Ÿæˆé“¾æ¥æå–æŠ¥å‘Š"""
        report_file = self.output_dir / "link_extraction_report.md"
        
        report_content = f"""# Isaac Sim é“¾æ¥æå–æŠ¥å‘Š

## æå–æ¦‚å†µ
- **æå–æ—¶é—´**: {Path().cwd().name}
- **å¤„ç†æ–‡ä»¶æ•°**: {len(self.page_info)}
- **æ€»é“¾æ¥æ•°**: {len(self.all_links)}
- **åˆ†ç±»æ•°**: {len(categorized)}

## é“¾æ¥åˆ†ç±»ç»Ÿè®¡

"""
        
        for category, links in categorized.items():
            if links:
                report_content += f"### {category.upper()} ({len(links)}ä¸ª)\n"
                for i, link in enumerate(links[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report_content += f"{i}. {link}\n"
                if len(links) > 5:
                    report_content += f"... è¿˜æœ‰ {len(links) - 5} ä¸ªé“¾æ¥\n"
                report_content += "\n"
        
        report_content += f"""
## ä¸‹è½½é˜Ÿåˆ—

å·²ç”Ÿæˆä¼˜å…ˆçº§ä¸‹è½½é˜Ÿåˆ—ï¼Œå…± {len(download_queue)} ä¸ªé¡µé¢ï¼š

### ä¼˜å…ˆçº§è¯´æ˜
1. **Installation** - å®‰è£…å’Œè®¾ç½®ç›¸å…³ (æœ€é«˜ä¼˜å…ˆçº§)
2. **Tutorials** - æ•™ç¨‹å’Œæ¼”ç»ƒ
3. **Guides** - æŒ‡å—å’ŒHow-toæ–‡æ¡£  
4. **Features** - åŠŸèƒ½å’Œæ¦‚è§ˆ
5. **API** - APIå‚è€ƒæ–‡æ¡£
6. **Examples** - ç¤ºä¾‹å’Œæ¼”ç¤º
7. **Other** - å…¶ä»–æ–‡æ¡£

## è¾“å‡ºæ–‡ä»¶
- `all_isaac_links.json` - æ‰€æœ‰æå–çš„é“¾æ¥
- `categorized_links.json` - åˆ†ç±»åçš„é“¾æ¥
- `download_queue.json` - ä¼˜å…ˆçº§ä¸‹è½½é˜Ÿåˆ—

## ä¸‹ä¸€æ­¥
è¿è¡Œæ‰¹é‡ä¸‹è½½å™¨ï¼š
```bash
python isaac_batch_downloader.py
```
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“Š æå–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    extractor = LocalLinkExtractor()
    
    # æ‰«æå·²ä¸‹è½½å†…å®¹
    total_links = extractor.scan_downloaded_content()
    
    if total_links == 0:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œè¯·å…ˆè¿è¡Œwgetä¸‹è½½æˆ–æ£€æŸ¥ä¸‹è½½ç›®å½•")
        return
    
    # ä¿å­˜ç»“æœ
    results = extractor.save_results()
    
    print(f"\nğŸ‰ é“¾æ¥æå–å®Œæˆï¼")
    print(f"ğŸ”— æ€»è®¡æå– {results['all_links']} ä¸ªæœ‰æ•ˆé“¾æ¥")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {extractor.output_dir.absolute()}")
    
    # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
    print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for category, links in results['categorized'].items():
        if links:
            print(f"  {category}: {len(links)} ä¸ªé“¾æ¥")

if __name__ == "__main__":
    main()
