#!/usr/bin/env python3
"""
çŸ¥ä¹AIç¼–ç¨‹æ–‡ç« æ‰¹é‡ä¸‹è½½è„šæœ¬
ä»¿ç…§å¾®ä¿¡ç™»å½•ç­–ç•¥ï¼šæ‰“å¼€ç™»å½•é¡µé¢ï¼Œé—´éš”æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œç™»å½•æˆåŠŸåè¿›è¡Œæ‰¹é‡ä¸‹è½½
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, Any, List

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.core.scraper_toolkit import ScraperToolkit, ScrapingConfig, Platform

# AIç¼–ç¨‹ç›¸å…³æœç´¢å…³é”®è¯
AI_CODING_QUERIES = [
    "AIç¼–ç¨‹",
    "äººå·¥æ™ºèƒ½ç¼–ç¨‹", 
    "AIå¼€å‘",
    "æ™ºèƒ½ç¼–ç¨‹",
    "AIç¼–ç¨‹åŠ©æ‰‹",
    "Claude Code",
    "ChatGPTç¼–ç¨‹",
    "GitHub Copilot",
    "AIä»£ç ç”Ÿæˆ",
    "è‡ªåŠ¨ç¼–ç¨‹",
    "Python AIç¼–ç¨‹",
    "JavaScript AI",
    "AIå·¥å…·å¼€å‘",
    "æœºå™¨å­¦ä¹ ç¼–ç¨‹",
    "æ·±åº¦å­¦ä¹ ç¼–ç¨‹",
    "AIè°ƒè¯•",
    "AIé‡æ„",
    "AIæµ‹è¯•",
    "AIç¼–ç¨‹å·¥å…·",
    "AIå¼€å‘ç¯å¢ƒ",
    "AIç¼–ç¨‹èŒƒå¼",
    "æ™ºèƒ½ä»£ç åˆ†æ",
    "AIä»£ç å®¡æŸ¥",
    "AIç¼–ç¨‹æœ€ä½³å®è·µ",
    "AIè¾…åŠ©å¼€å‘"
]

OUTPUT_DIR = Path('K-Vault/AI-Coding/zhihu')
LINKS_JSON = OUTPUT_DIR / 'links.json'
STATE_JSON = OUTPUT_DIR / 'state.json'


async def wait_until_zhihu_logged_in(toolkit: ScraperToolkit):
    """ç­‰å¾…çŸ¥ä¹ç™»å½•å®Œæˆï¼Œä»¿ç…§å¾®ä¿¡ç™»å½•ç­–ç•¥"""
    print('ç­‰å¾…ä½ åœ¨æµè§ˆå™¨ä¸­å®ŒæˆçŸ¥ä¹ç™»å½•ï¼ˆæ— é™ç­‰å¾…ï¼‰ï¼Œå®Œæˆåæˆ‘æ‰ä¼šç»§ç»­...')
    
    # æ‰“å¼€çŸ¥ä¹é¦–é¡µ
    page = toolkit.web_scraper.zhihu_page
    await page.goto('https://www.zhihu.com')
    await page.wait_for_load_state('networkidle')
    
    # å¾ªç¯æ£€æŸ¥ç™»å½•çŠ¶æ€
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™»å½•é¡µé¢
            current_url = page.url
            if "login" in current_url.lower() or "signin" in current_url.lower():
                print('æ£€æµ‹åˆ°ç™»å½•é¡µé¢ï¼Œè¯·å®Œæˆç™»å½•...')
                await page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•çŠ¶æ€ï¼‰
            login_button = await page.query_selector('button:has-text("ç™»å½•"), .SignFlow-tab, [data-testid*="login"]')
            if login_button:
                print('æ£€æµ‹åˆ°ç™»å½•æŒ‰é’®ï¼Œè¯·å®Œæˆç™»å½•...')
                await page.wait_for_timeout(3000)
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒæˆ–ç”¨æˆ·åï¼ˆå·²ç™»å½•çŠ¶æ€ï¼‰
            user_avatar = await page.query_selector('.AppHeader-userInfo, .UserAvatar, [data-testid*="user"]')
            if user_avatar:
                print('æ£€æµ‹åˆ°ç”¨æˆ·ä¿¡æ¯ï¼Œç™»å½•æˆåŠŸï¼')
                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢æ¡†ï¼ˆç™»å½•åçš„é¡µé¢ç‰¹å¾ï¼‰
            search_box = await page.query_selector('.SearchBar-input, input[placeholder*="æœç´¢"]')
            if search_box:
                print('æ£€æµ‹åˆ°æœç´¢åŠŸèƒ½ï¼Œç™»å½•æˆåŠŸï¼')
                break
                
            print('ç­‰å¾…ç™»å½•å®Œæˆ...')
            await page.wait_for_timeout(2000)  # ç­‰å¾…2ç§’åå†æ¬¡æ£€æŸ¥
            
        except Exception as e:
            print(f'æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}')
            await page.wait_for_timeout(2000)
    
    print('çŸ¥ä¹ç™»å½•æ£€æµ‹å®Œæˆï¼Œå¼€å§‹æœç´¢...')


def load_downloaded_urls(out_dir: Path):
    """ä» file_mapping.json è¯»å–å·²ä¸‹è½½URLé›†åˆï¼Œç”¨äºè·³è¿‡"""
    mapping_file = out_dir / 'file_mapping.json'
    urls = set()
    if mapping_file.exists():
        try:
            data = json.loads(mapping_file.read_text(encoding='utf-8'))
            for _, v in (data or {}).items():
                u = v.get('url')
                if u:
                    urls.add(u)
        except Exception:
            pass
    return urls


async def collect_all_links(toolkit: ScraperToolkit) -> List[str]:
    """æ”¶é›†æ‰€æœ‰AIç¼–ç¨‹ç›¸å…³é“¾æ¥"""
    print('2) æ”¶é›†AIç¼–ç¨‹ç›¸å…³é“¾æ¥ï¼ˆå¤šå…³é”®è¯æœç´¢ï¼‰...')
    all_links: List[str] = []
    
    for i, query in enumerate(AI_CODING_QUERIES, 1):
        print(f'  [{i}/{len(AI_CODING_QUERIES)}] æœç´¢: {query}')
        try:
            res = await toolkit.search(Platform.ZHIHU, query, max_pages=3)  # æ¯ä¸ªå…³é”®è¯æœç´¢3é¡µ
            if res.get('status') == 'success':
                links = res.get('all_links') or [item.get('url') for item in (res.get('results') or []) if item.get('url')]
                links = [l for l in links if l]
                print(f'    å–å¾— {len(links)} æ¡é“¾æ¥')
                all_links.extend(links)
            else:
                print(f'    æœç´¢å¤±è´¥: {res.get("message", "æœªçŸ¥é”™è¯¯")}')
        except Exception as e:
            print(f'    æœç´¢å¼‚å¸¸: {e}')
        
        # æœç´¢é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(2)
    
    # å»é‡
    unique_links = list(dict.fromkeys(all_links))
    print(f'  æ”¶é›†åˆè®¡ {len(all_links)} æ¡ï¼Œå»é‡å {len(unique_links)} æ¡')
    return unique_links


def load_json(path: Path, default):
    """åŠ è½½JSONæ–‡ä»¶"""
    if path.exists():
        try:
            return json.loads(path.read_text(encoding='utf-8'))
        except Exception:
            return default
    return default


def save_json(path: Path, data: Any):
    """ä¿å­˜JSONæ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')


async def run_zhihu_ai_coding_download():
    """è¿è¡ŒçŸ¥ä¹AIç¼–ç¨‹æ–‡ç« æ‰¹é‡ä¸‹è½½"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    config = ScrapingConfig(
        platform=Platform.ZHIHU,
        headless=False,
        max_pages=3,
        output_dir=OUTPUT_DIR,
        wait_for_verification=True,
    )
    toolkit = ScraperToolkit(config)
    
    try:
        print('1) è®¾ç½®æµè§ˆå™¨...')
        await toolkit.setup_browser(Platform.ZHIHU)
        
        print('2) ç­‰å¾…çŸ¥ä¹ç™»å½•...')
        await wait_until_zhihu_logged_in(toolkit)
        
        # ç¡®ä¿ç™»å½•çŠ¶æ€æ­£ç¡®è®¾ç½®
        print('3) ç¡®è®¤ç™»å½•çŠ¶æ€...')
        login_result = await toolkit.web_scraper.login_zhihu()
        if login_result["status"] != "success":
            print(f"âŒ ç™»å½•ç¡®è®¤å¤±è´¥: {login_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return
        print("âœ… ç™»å½•çŠ¶æ€ç¡®è®¤æˆåŠŸ")
        
        # è¯»å–æˆ–ç”Ÿæˆé“¾æ¥æ¸…å•
        if LINKS_JSON.exists():
            print('4) å‘ç° links.jsonï¼Œç›´æ¥è¯»å–...')
            links: List[str] = load_json(LINKS_JSON, [])
        else:
            links = await collect_all_links(toolkit)
            save_json(LINKS_JSON, links)
            print(f'  å·²ä¿å­˜é“¾æ¥æ¸…å•ï¼š{LINKS_JSON}')
        
        # è¯»å–æˆ–åˆå§‹åŒ–çŠ¶æ€
        state = load_json(STATE_JSON, {
            'completed': [],
            'failed': [],
            'index': 0,
            'total': len(links),
        })
        print(f"5) è¯»å–è¿›åº¦ï¼šindex={state.get('index',0)}/{state.get('total',len(links))}, completed={len(state.get('completed',[]))}, failed={len(state.get('failed',[]))}")
        
        # è¯»å–å·²ä¸‹è½½URLé›†åˆ
        downloaded_urls = load_downloaded_urls(OUTPUT_DIR)
        if downloaded_urls:
            print(f'   å‘ç°å·²ä¸‹è½½ {len(downloaded_urls)} ç¯‡ï¼Œå°†è‡ªåŠ¨è·³è¿‡åŒ¹é…é“¾æ¥')
        
        # ä¸²è¡Œä¸‹è½½
        print('6) ä¸²è¡Œä¸‹è½½AIç¼–ç¨‹æ–‡ç« ...')
        for i in range(state.get('index', 0), len(links)):
            link = links[i]
            
            # è·³è¿‡å·²ä¸‹è½½çš„é“¾æ¥
            if link in downloaded_urls or link in state.get('completed', []):
                print(f'  [{i+1}/{len(links)}] è·³è¿‡ï¼ˆå·²ä¸‹è½½ï¼‰: {link}')
                continue
            
            print(f'  [{i+1}/{len(links)}] ä¸‹è½½: {link}')
            try:
                r = await toolkit.download_content(Platform.ZHIHU, link, OUTPUT_DIR)
                if r.get('status') == 'success':
                    files = r.get('files', {})
                    print('     âœ… æˆåŠŸ')
                    print(f"       PDF: {files.get('pdf')}")
                    print(f"       MD : {files.get('markdown')}")
                    state.setdefault('completed', []).append(link)
                else:
                    print(f"     âŒ å¤±è´¥: {r.get('message')}")
                    state.setdefault('failed', []).append({'link': link, 'message': r.get('message')})
                
                # æ›´æ–°ç´¢å¼•å¹¶ä¿å­˜çŠ¶æ€
                state['index'] = i + 1
                save_json(STATE_JSON, state)
                
                # ä¸‹è½½é—´éš”
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f'     ğŸ’¥ å¼‚å¸¸: {e}')
                state.setdefault('failed', []).append({'link': link, 'error': str(e)})
                state['index'] = i + 1
                save_json(STATE_JSON, state)
        
        print('\n7) å®Œæˆç»Ÿè®¡ï¼š')
        print(f"  æˆåŠŸ: {len(state.get('completed', []))}")
        print(f"  å¤±è´¥: {len(state.get('failed', []))}")
        print(f"  æ€»æ•°: {len(links)}")
        print(f"  é“¾æ¥æ¸…å•: {LINKS_JSON}")
        print(f"  è¿›åº¦æ–‡ä»¶: {STATE_JSON}")
        print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
    finally:
        await toolkit.cleanup()


if __name__ == '__main__':
    print("ğŸš€ çŸ¥ä¹AIç¼–ç¨‹æ–‡ç« æ‰¹é‡ä¸‹è½½")
    print("=" * 50)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ” æœç´¢å…³é”®è¯: {len(AI_CODING_QUERIES)} ä¸ª")
    print("â° é¢„è®¡æ—¶é—´: 30-60åˆ†é’Ÿ")
    print("=" * 50)
    
    try:
        asyncio.run(run_zhihu_ai_coding_download())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
