#!/usr/bin/env python3
"""
æµ‹è¯•MCP Serverçš„æ‰€æœ‰å·¥å…·åŠŸèƒ½
"""
import asyncio
import sys
import json
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from core.web_scraper import WebScraper
from utils.logger import Logger


async def test_mcp_tools():
    """æµ‹è¯•æ‰€æœ‰MCPå·¥å…·åŠŸèƒ½"""
    logger = Logger("MCPå·¥å…·æµ‹è¯•")
    
    # åˆ›å»ºWebScraperå®ä¾‹
    scraper = WebScraper()
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•MCP Serverçš„æ‰€æœ‰å·¥å…·...")
    
    # æµ‹è¯•1: ç™»å½•çŸ¥ä¹
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•1: ç™»å½•çŸ¥ä¹")
    print("="*50)
    
    login_result = await scraper.login_zhihu(headless=False)
    if login_result["status"] == "success":
        print("âœ… ç™»å½•æˆåŠŸï¼")
    else:
        print(f"âŒ ç™»å½•å¤±è´¥: {login_result['message']}")
        return False
    
    # æµ‹è¯•2: æœç´¢çŸ¥ä¹å†…å®¹
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•2: æœç´¢çŸ¥ä¹å†…å®¹")
    print("="*50)
    
    search_result = await scraper.search_zhihu(
        query="Pythonç¼–ç¨‹",
        max_pages=1,
        min_relevance=0.5
    )
    
    if search_result["status"] == "success":
        print("âœ… æœç´¢æˆåŠŸï¼")
        print(f"ğŸ“Š æ€»ç»“æœæ•°: {search_result['total_results']}")
        print(f"ğŸ¯ ç¬¦åˆè¦æ±‚çš„ç»“æœæ•°: {search_result['filtered_results']}")
        
        qualified_links = search_result.get("qualified_links", [])
        if qualified_links:
            print(f"\nğŸ“‹ ç¬¦åˆè¦æ±‚çš„æ‰€æœ‰é¡µé¢é“¾æ¥ ({len(qualified_links)}ä¸ª):")
            for i, link in enumerate(qualified_links[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i}. {link}")
    else:
        print(f"âŒ æœç´¢å¤±è´¥: {search_result['message']}")
        return False
    
    # æµ‹è¯•3: è¯»å–çŸ¥ä¹é¡µé¢å†…å®¹
    if qualified_links:
        print("\n" + "="*50)
        print("ğŸ“– æµ‹è¯•3: è¯»å–çŸ¥ä¹é¡µé¢å†…å®¹")
        print("="*50)
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªé“¾æ¥è¿›è¡Œæµ‹è¯•
        test_url = qualified_links[0]
        print(f"ğŸ”— æµ‹è¯•URL: {test_url}")
        
        read_result = await scraper.read_zhihu_page(test_url)
        
        if read_result["status"] == "success":
            print("âœ… é¡µé¢è¯»å–æˆåŠŸï¼")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {read_result['title']}")
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {read_result['text_length']} å­—ç¬¦")
            print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {read_result['text_content'][:200]}...")
        else:
            print(f"âŒ é¡µé¢è¯»å–å¤±è´¥: {read_result['message']}")
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰MCPå·¥å…·æµ‹è¯•å®Œæˆï¼")
    print("="*50)
    
    return True


async def main():
    """ä¸»å‡½æ•°"""
    try:
        success = await test_mcp_tools()
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MCP Serverå·¥å…·åŠŸèƒ½æ­£å¸¸ï¼")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
